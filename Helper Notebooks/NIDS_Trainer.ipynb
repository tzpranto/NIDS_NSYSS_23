{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uk8TDHYpcyO4",
        "outputId": "b7f23218-32a7-41b6-cf16-375c1e4a6d20"
      },
      "source": [
        "! git clone -b master https://github.com/tzpranto/NIDS_NSYSS_23"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NIDS-Project'...\n",
            "remote: Enumerating objects: 865, done.\u001b[K\n",
            "remote: Counting objects: 100% (610/610), done.\u001b[K\n",
            "remote: Compressing objects: 100% (466/466), done.\u001b[K\n",
            "remote: Total 865 (delta 184), reused 567 (delta 143), pack-reused 255\u001b[K\n",
            "Receiving objects: 100% (865/865), 163.33 MiB | 33.09 MiB/s, done.\n",
            "Resolving deltas: 100% (314/314), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EYhgPvB7c2GH",
        "outputId": "4ac07e48-f77a-43e9-c355-41f4ad340559"
      },
      "source": [
        "cd NIDS-Project/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/NIDS-Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIy2DmMTpu3X",
        "outputId": "893cf46b-aa39-4ca7-8029-b2ab2b271fb3"
      },
      "source": [
        "!mkdir Output\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_generator.py  Dataset_NSLKDD_2  models  Output  training_file.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FrgOewPpv24",
        "outputId": "42ddd4e2-9f20-49a3-97bd-d75b19dd9fab"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import Linear\n",
        "from data_generator import get_training_data, dataset_test, dataset_train, cat_dict\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import os, glob\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "import random\n",
        "\n",
        "# 0.01: trainstop 0.005, cluster /25, min_imp_dec: 0.01, (80,100,150) , 5:15PM 26/11/20\n",
        "\n",
        "\n",
        "random.seed(12345)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "total_dataset, labeled_dataset, unlabeled_dataset = get_training_data(label_ratio=0.5)\n",
        "test_dataset = dataset_test()\n",
        "print(total_dataset.get_x().shape)\n",
        "test_dataset_neg = dataset_test(test_neg=True)\n",
        "\n",
        "ae_epoch = 80\n",
        "pretrain_epoch = 200\n",
        "train_epoch = 150\n",
        "\n",
        "num_data = total_dataset.get_x()\n",
        "labels = total_dataset.get_y()\n",
        "\n",
        "total_original_label_counts = dict()\n",
        "distinct_labels, distinct_label_counts = np.unique(labels, return_counts=True)\n",
        "for i in range(len(distinct_labels)):\n",
        "    if distinct_labels[i] != -1:\n",
        "        total_original_label_counts[distinct_labels[i]] = distinct_label_counts[i]\n",
        "\n",
        "\n",
        "# print(total_original_label_counts)\n",
        "\n",
        "\n",
        "def tree_work(load_cluster_from_file=False):\n",
        "    if load_cluster_from_file:\n",
        "        clustering = pickle.load(file=open('models/clustering.pkl', 'rb'))\n",
        "    else:\n",
        "        clustering = KMeans(n_clusters=int(total_dataset.__len__() / 175), random_state=0)\n",
        "        print(\"Clustering Started.\")\n",
        "        clustering.fit(num_data)\n",
        "        print(\"Clustering ended.\")\n",
        "\n",
        "    cluster_assignment = clustering.labels_\n",
        "    all_clusters = dict()\n",
        "    for j in range(len(cluster_assignment)):\n",
        "        all_clusters.setdefault(cluster_assignment[j], []).append(num_data[j])\n",
        "\n",
        "    cluster_to_label_dict = dict()\n",
        "    for j in range(len(cluster_assignment)):\n",
        "        if labels[j] != -1:\n",
        "            cluster_to_label_dict.setdefault(cluster_assignment[j], []).append(labels[j])\n",
        "\n",
        "    #print(\"Clusters:\")\n",
        "    label_to_cluster_dict = dict()\n",
        "    for k, v in cluster_to_label_dict.items():\n",
        "        cl_labels, cl_label_counts = np.unique(np.array(v), return_counts=True)  # labeled member count in each cluster\n",
        "        # print(k)\n",
        "        # print(cl_labels)\n",
        "        # print(cl_label_counts)\n",
        "        # print(\"\\n\")\n",
        "        total_labeled_counts = np.sum(cl_label_counts)\n",
        "\n",
        "        max_label = np.argmax(cl_label_counts)\n",
        "\n",
        "        '''\n",
        "        If a cluster contains more than 10% of all samples of a label present in the training dataset, it is considered\n",
        "        important for this label.\n",
        "        If the majority label is not the normal label and there are no other labels for which this cluster is important,\n",
        "        having more than 50% of the labeled members would be enough for soft labeling\n",
        "        If the majority label is the normal label, then all labeled members must be normal for soft labeling.\n",
        "        In any other case, we do not soft label.\n",
        "        '''\n",
        "\n",
        "        imp_for_label = []\n",
        "        for label, total_label_count in total_original_label_counts.items():\n",
        "            for j in range(len(cl_labels)):\n",
        "                if cl_labels[j] == label and cl_label_counts[j] > 0.1 * total_label_count:\n",
        "                    imp_for_label.append(label)\n",
        "\n",
        "        if (cl_label_counts[max_label] / total_labeled_counts) > 0.5:\n",
        "            selected_label = cl_labels[max_label]\n",
        "            if len(imp_for_label) == 1:\n",
        "                if imp_for_label[0] == selected_label:\n",
        "                    if selected_label != (int(cat_dict['Normal'])-1):\n",
        "                        label = selected_label\n",
        "                        size = len(v)\n",
        "                        label_to_cluster_dict.setdefault(label, []).append([k, size])\n",
        "                    else:\n",
        "                        if len(cl_labels) == 1:\n",
        "                            label = selected_label\n",
        "                            size = len(v)\n",
        "                            label_to_cluster_dict.setdefault(label, []).append([k, size])\n",
        "            elif len(imp_for_label) == 0:\n",
        "                if selected_label != (int(cat_dict['Normal'])-1):\n",
        "                    label = selected_label\n",
        "                    size = len(v)\n",
        "                    label_to_cluster_dict.setdefault(label, []).append([k, size])\n",
        "                else:\n",
        "                    if len(cl_labels) == 1:\n",
        "                        label = selected_label\n",
        "                        size = len(v)\n",
        "                        label_to_cluster_dict.setdefault(label, []).append([k, size])\n",
        "\n",
        "    '''\n",
        "    clusters that belong to a particular label, after soft labeling.\n",
        "    '''\n",
        "    soft_label_mapping = dict()\n",
        "    for k, v in label_to_cluster_dict.items():\n",
        "        for cluster_index in v:\n",
        "            soft_label_mapping[cluster_index[0]] = k\n",
        "\n",
        "    '''\n",
        "    soft labeling particular unlabeled samples.\n",
        "    Also add this to labeled dataset.\n",
        "    '''\n",
        "\n",
        "    for j in range(len(labels)):\n",
        "        if labels[j] == -1 and (int(cluster_assignment[j]) in soft_label_mapping.keys()):\n",
        "            labels[j] = soft_label_mapping[cluster_assignment[j]]\n",
        "            labeled_dataset.add_sample(num_data[j], labels[j])\n",
        "\n",
        "\n",
        "    '''\n",
        "    checking total labeled and soft labeled members.\n",
        "    '''\n",
        "\n",
        "    total_soft_label_counts = dict()\n",
        "    distinct_slabels, distinct_slabel_counts = np.unique(labels, return_counts=True)\n",
        "    for j in range(len(distinct_slabels)):\n",
        "        if distinct_slabels[j] != -1:\n",
        "            total_soft_label_counts[distinct_slabels[j]] = distinct_slabel_counts[j]\n",
        "\n",
        "    print(total_soft_label_counts)\n",
        "\n",
        "    cluster_to_labels_dict = dict()\n",
        "    for k, v in cluster_to_label_dict.items():\n",
        "        cl_labels = np.unique(np.array(v))\n",
        "        cl_labels = sorted(list(cl_labels))\n",
        "        if cl_labels[0] == -1:\n",
        "            cl_labels = cl_labels[1:]\n",
        "        cluster_to_labels_dict[k] = cl_labels\n",
        "\n",
        "    total_dataset.set_y(labels)\n",
        "\n",
        "    dt_X = labeled_dataset.get_x()\n",
        "    dt_Y = labeled_dataset.get_y()\n",
        "\n",
        "    print(\"labeled members dimensions:\")\n",
        "    print(dt_X.shape)\n",
        "    print(dt_Y.shape)\n",
        "\n",
        "    clf = DecisionTreeClassifier(random_state=0, max_leaf_nodes=7)\n",
        "    clf.fit(dt_X, dt_Y)\n",
        "\n",
        "    print(\"No. of leaves of decision tree:\")\n",
        "    print(clf.get_n_leaves())\n",
        "\n",
        "    '''\n",
        "    saving decision tree, cluster membership info (this is just to skip the clustering step for our faster use) and soft\n",
        "    labeling info.\n",
        "    '''\n",
        "\n",
        "    file = open('models/tree.pkl', 'wb')\n",
        "    pickle.dump(clf, file)\n",
        "    file.close()\n",
        "\n",
        "    for k in list(all_clusters.keys()):\n",
        "        if int(k) not in soft_label_mapping.keys():\n",
        "            soft_label_mapping[int(k)] = (int(cat_dict['Normal'])-1)\n",
        "\n",
        "    file = open('models/soft_label_mapping.pkl', 'wb')\n",
        "    pickle.dump(soft_label_mapping, file)\n",
        "    file.close()\n",
        "\n",
        "    if not load_cluster_from_file:\n",
        "        file = open('models/clustering.pkl', 'wb')\n",
        "        pickle.dump(clustering, file)\n",
        "        file.close()\n",
        "\n",
        "    leaf_dataset_X = dict()\n",
        "    leaf_dataset_Y = dict()\n",
        "\n",
        "    '''\n",
        "    finding out corresponding leaf for each training sample.\n",
        "    '''\n",
        "\n",
        "    for j in range(len(num_data)):\n",
        "        leaf = clf.apply([num_data[j]])[0]\n",
        "        if labels[j] != -1:\n",
        "            leaf_dataset_X.setdefault(leaf, []).append(num_data[j])\n",
        "            leaf_dataset_Y.setdefault(leaf, []).append(labels[j])\n",
        "\n",
        "    for k, v in leaf_dataset_X.items():\n",
        "        leaf_dataset_X[k] = np.array(leaf_dataset_X[k])\n",
        "        leaf_dataset_Y[k] = np.array(leaf_dataset_Y[k])\n",
        "\n",
        "    return leaf_dataset_X, leaf_dataset_Y\n",
        "\n",
        "'''\n",
        "The dictionary of X-Y training dataset for each individual leaf.\n",
        "'''\n",
        "leaf_dataset_X, leaf_dataset_Y = tree_work(load_cluster_from_file=False)\n",
        "\n",
        "'''\n",
        "undercomplete autoencoder for embedding.\n",
        "'''\n",
        "\n",
        "class AE(nn.Module):\n",
        "\n",
        "    def __init__(self, n_input, n_z):\n",
        "        super(AE, self).__init__()\n",
        "        # encoder\n",
        "        self.enc_1 = Linear(n_input, 80)\n",
        "        self.enc_2 = Linear(80, 50)\n",
        "        self.z_layer = Linear(50, n_z)\n",
        "\n",
        "        # decoder\n",
        "        self.dec_1 = Linear(n_z, 50)\n",
        "        self.dec_2 = Linear(50, 80)\n",
        "        self.x_bar_layer = Linear(80, n_input)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder\n",
        "        enc_h1 = F.relu(self.enc_1(x))\n",
        "        enc_h2 = F.relu(self.enc_2(enc_h1))\n",
        "        z = self.z_layer(enc_h2)\n",
        "\n",
        "        # decoder\n",
        "        dec_h1 = F.relu(self.dec_1(z))\n",
        "        dec_h2 = F.relu(self.dec_2(dec_h1))\n",
        "        x_bar = self.x_bar_layer(dec_h2)\n",
        "\n",
        "        return x_bar, z\n",
        "\n",
        "\n",
        "def train_ae(epochs, load_from_file=False, save_path='models/train_ae'):\n",
        "    '''\n",
        "    train autoencoder\n",
        "    '''\n",
        "\n",
        "    model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    model.to(device)\n",
        "\n",
        "    ae_train_ds = total_dataset\n",
        "    training_data_length = int(0.7 * ae_train_ds.__len__())\n",
        "    validation_data_length = ae_train_ds.__len__() - training_data_length\n",
        "    training_data, validation_data = torch.utils.data.random_split(ae_train_ds,\n",
        "                                                                   [training_data_length, validation_data_length])\n",
        "\n",
        "    print(total_dataset.__len__())\n",
        "    print(training_data.__len__())\n",
        "    print(validation_data.__len__())\n",
        "\n",
        "    train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "    validation_loader = DataLoader(validation_data, batch_size=32, shuffle=True)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    min_val_loss = 1000000\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        training_loss = 0.\n",
        "        validation_loss = 0.\n",
        "        train_batch_num = 0\n",
        "        val_batch_num = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (x, _, idx) in enumerate(train_loader):\n",
        "            x = x.float()\n",
        "            x = x.to(device)\n",
        "\n",
        "            train_batch_num = batch_idx\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            x_bar, z = model(x)\n",
        "            loss = F.mse_loss(x_bar, x)\n",
        "            training_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        training_loss /= (train_batch_num + 1)\n",
        "\n",
        "        model.eval()\n",
        "        for batch_idx, (x, _, idx) in enumerate(validation_loader):\n",
        "            x = x.float()\n",
        "            x = x.to(device)\n",
        "\n",
        "            val_batch_num = batch_idx\n",
        "\n",
        "            x_bar, z = model(x)\n",
        "            loss = F.mse_loss(x_bar, x)\n",
        "            validation_loss += loss.item()\n",
        "\n",
        "        validation_loss /= (val_batch_num + 1)\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            print(\n",
        "                \"epoch {} , Training loss={:.4f}, Validation loss={:.4f}\".format(epoch, training_loss, validation_loss))\n",
        "\n",
        "        if epoch == 0 or min_val_loss > validation_loss:\n",
        "            min_val_loss = validation_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    print(\"model saved to {}.\".format(save_path))\n",
        "    return model\n",
        "\n",
        "\n",
        "class leaf_dnn(nn.Module):\n",
        "\n",
        "    def __init__(self, n_input, n_output):\n",
        "        super(leaf_dnn, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_input, 8)\n",
        "        self.fc2 = nn.Linear(8, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1 = torch.relu(self.fc1(x))\n",
        "        out_2 = self.fc2(out_1)\n",
        "\n",
        "        return out_2\n",
        "\n",
        "'''\n",
        "pretraining using labeled and soft labeled dataset.\n",
        "'''\n",
        "\n",
        "def pretrain_leaf_dnn(save_path, epochs):\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "\n",
        "    model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "    model.to(device)\n",
        "\n",
        "    weights = torch.FloatTensor(labeled_dataset.get_weight()).to(device)\n",
        "\n",
        "    train_loader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)  # soft label must be assigned\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    min_train_loss = 1000000\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        train_batch_num = 0\n",
        "        train_num_correct = 0\n",
        "        train_num_examples = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (x, y_t, idx) in enumerate(train_loader):\n",
        "            x = x.float()\n",
        "            x = x.to(device)\n",
        "            train_batch_num = batch_idx\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x_emb = ae_model(x)[1]\n",
        "            y_pred = model(x_emb)\n",
        "\n",
        "            y_t = y_t.clone().detach().to(device)\n",
        "\n",
        "            loss = torch.nn.CrossEntropyLoss(weight=weights)(y_pred, y_t)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            correct = torch.eq(torch.max(torch.softmax(y_pred, dim=-1), dim=1)[1], y_t).view(-1)\n",
        "            train_num_correct += torch.sum(correct).item()\n",
        "            train_num_examples += correct.shape[0]\n",
        "\n",
        "        train_loss /= (train_batch_num + 1)\n",
        "        train_acc = train_num_correct / train_num_examples\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            print(\"epoch {}; T loss={:.4f} T Accuracy={:.4f}\".\n",
        "                  format(epoch, train_loss, train_num_correct / train_num_examples))\n",
        "\n",
        "        if epoch == 0 or min_train_loss > train_loss:\n",
        "            min_train_loss = train_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    print(\"model saved to {}.\".format(save_path))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "training dataset of an individual leaf.\n",
        "'''\n",
        "\n",
        "def train_leaf_dnn(model, dataset, save_path, epochs):\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "\n",
        "    weights = torch.FloatTensor(dataset.get_weight()).to(device)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)  # soft label must be assigned\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "    min_train_loss = 1000000\n",
        "    prev_train_acc = 0\n",
        "    stop_flag = 1\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        train_batch_num = 0\n",
        "        train_num_correct = 0\n",
        "        train_num_examples = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (x, y_t, idx) in enumerate(train_loader):\n",
        "            x = x.float()\n",
        "            x = x.to(device)\n",
        "            train_batch_num = batch_idx\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x_emb = ae_model(x)[1]\n",
        "            y_pred = model(x_emb)\n",
        "\n",
        "            y_t = y_t.clone().detach().to(device)\n",
        "\n",
        "            loss = torch.nn.CrossEntropyLoss(weight=weights)(y_pred, y_t)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            correct = torch.eq(torch.max(torch.softmax(y_pred, dim=-1), dim=1)[1], y_t).view(-1)\n",
        "            train_num_correct += torch.sum(correct).item()\n",
        "            train_num_examples += correct.shape[0]\n",
        "\n",
        "        train_loss /= (train_batch_num + 1)\n",
        "        train_acc = train_num_correct / train_num_examples\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            # print(\"epoch {}; T loss={:.4f} T Accuracy={:.4f}\".\n",
        "            #       format(epoch, train_loss, train_num_correct / train_num_examples))\n",
        "\n",
        "            if train_acc - prev_train_acc > 0.005:\n",
        "                    stop_flag = 0\n",
        "\n",
        "\n",
        "        if epoch == 0 or min_train_loss > train_loss:\n",
        "            min_train_loss = train_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "        if train_acc == 1.0:\n",
        "            break\n",
        "\n",
        "        if epoch % 30 == 0:\n",
        "            if stop_flag == 1:\n",
        "                break\n",
        "                # if train_acc > 0.96:\n",
        "                #     break\n",
        "                # for g in optimizer.param_groups:\n",
        "                #     g['lr'] = g['lr']*2\n",
        "            stop_flag = 1\n",
        "            prev_train_acc = train_acc\n",
        "\n",
        "    #print(\"model saved to {}.\".format(save_path))\n",
        "\n",
        "    return model\n",
        "\n",
        "'''\n",
        "training all leaves.\n",
        "'''\n",
        "\n",
        "def create_leaf_dnns(remove_priors=True,epochs=150):\n",
        "    if remove_priors:\n",
        "        #print(\"Removing...\")\n",
        "        filelist = glob.glob(os.path.join('models/leaf_models', \"*\"))\n",
        "        for f in filelist:\n",
        "            os.remove(f)\n",
        "\n",
        "    for key in leaf_dataset_Y.keys():\n",
        "        dataset_X = leaf_dataset_X[key]\n",
        "        dataset_Y = leaf_dataset_Y[key]\n",
        "\n",
        "        # print(key)\n",
        "        # print(dataset_X.shape)\n",
        "        # print(dataset_Y.shape)\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        data = dataset_X, dataset_Y\n",
        "        dataset = dataset_train(data)\n",
        "\n",
        "        save_path = \"models/leaf_models/leaf_\" + str(key)\n",
        "\n",
        "        model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "        model.load_state_dict(torch.load('models/pretrain_leaf_dnn'))\n",
        "        # if os.path.exists(save_path):\n",
        "        #     model.load_state_dict(torch.load(save_path))\n",
        "        # else:\n",
        "        #     print(\"Loading Pre Trained...\")\n",
        "        #     model.load_state_dict(torch.load('models/pretrain_leaf_dnn'))\n",
        "        model.to(device)\n",
        "\n",
        "\n",
        "        train_leaf_dnn(model, dataset, save_path, epochs)\n",
        "\n",
        "\n",
        "#create_leaf_dnns()\n",
        "\n",
        "\n",
        "def generate_result():\n",
        "    clf = pickle.load(file=open('models/tree.pkl', 'rb'))\n",
        "    soft_label_mapping = pickle.load(file=open('models/soft_label_mapping.pkl', 'rb'))\n",
        "    clustering = pickle.load(file=open('models/clustering.pkl', 'rb'))\n",
        "\n",
        "    test_X = test_dataset.get_x()\n",
        "    test_Y = test_dataset.get_y()\n",
        "\n",
        "    leaf_nodes = clf.apply(test_X)\n",
        "    cluster_assignment = clustering.predict(test_X)\n",
        "\n",
        "    print(\"done\")\n",
        "\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "    '''\n",
        "    If model does not exist for a particular leaf, use the default pretrained model.\n",
        "    '''\n",
        "\n",
        "    model_dict = dict()\n",
        "    leaf_model_files = os.listdir('models/leaf_models')\n",
        "    for file in leaf_model_files:\n",
        "        spl = str(file).split(\"_\")\n",
        "\n",
        "        leaf_model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "\n",
        "        if not os.path.exists('models/leaf_models/leaf_' + spl[1]):\n",
        "            leaf_model.load_state_dict(torch.load('models/pretrain_leaf_dnn'))\n",
        "        else:\n",
        "            leaf_model.load_state_dict(torch.load('models/leaf_models/leaf_' + spl[1]))\n",
        "\n",
        "        leaf_model.to(device)\n",
        "\n",
        "        model_dict[int(spl[1])] = leaf_model\n",
        "\n",
        "    leaf_pred_dict = dict()\n",
        "    for k, v in model_dict.items():\n",
        "        leaf_model = v\n",
        "        X_emb = ae_model(torch.FloatTensor(test_X).to(device))[1]\n",
        "        Y = torch.softmax(leaf_model(X_emb), dim=-1)\n",
        "        Y_ = Y.cpu().detach().numpy()\n",
        "        leaf_pred_dict[k] = Y_\n",
        "\n",
        "    test_Y_pred = np.zeros(test_Y.shape)\n",
        "\n",
        "    for j in range(len(leaf_nodes)):\n",
        "        y_ = leaf_pred_dict[leaf_nodes[j]][j]\n",
        "        if soft_label_mapping[cluster_assignment[j]] != cat_dict['Normal']:\n",
        "            y_[int(cat_dict['Normal'])] = 0\n",
        "        y_pred = np.argmax(y_)\n",
        "        test_Y_pred[j] = y_pred\n",
        "\n",
        "    print(confusion_matrix(test_Y, test_Y_pred))\n",
        "    print(classification_report(test_Y, test_Y_pred))\n",
        "\n",
        "    test_Y_df = pd.DataFrame(test_Y, columns=['True Label'])\n",
        "    pred_Y_df = pd.DataFrame(test_Y_pred, columns=['Pred Label'])\n",
        "    result = pd.concat([test_Y_df,pred_Y_df],axis=1, sort=False)\n",
        "    #result.to_csv('Output/result-frac-10'+'.csv', index=False)\n",
        "    #files.download('Output/result-frac-10'+'.csv')\n",
        "\n",
        "\n",
        "def generate_test_result():\n",
        "    clf = pickle.load(file=open('models/tree.pkl', 'rb'))\n",
        "    soft_label_mapping = pickle.load(file=open('models/soft_label_mapping.pkl', 'rb'))\n",
        "    clustering = pickle.load(file=open('models/clustering.pkl', 'rb'))\n",
        "\n",
        "    test_X = test_dataset.get_x()\n",
        "    test_Y = test_dataset.get_y()\n",
        "\n",
        "    leaf_nodes = clf.apply(test_X)\n",
        "    cluster_assignment = clustering.predict(test_X)\n",
        "\n",
        "    print(\"done\")\n",
        "\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "    '''\n",
        "    If model does not exist for a particular leaf, use the default pretrained model.\n",
        "    '''\n",
        "\n",
        "    model_dict = dict()\n",
        "    leaf_model_files = os.listdir('models/leaf_models')\n",
        "    for file in leaf_model_files:\n",
        "        spl = str(file).split(\"_\")\n",
        "\n",
        "        leaf_model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "\n",
        "        if not os.path.exists('models/leaf_models/leaf_' + spl[1]):\n",
        "            leaf_model.load_state_dict(torch.load('models/pretrain_leaf_dnn'))\n",
        "        else:\n",
        "            leaf_model.load_state_dict(torch.load('models/leaf_models/leaf_' + spl[1]))\n",
        "\n",
        "        leaf_model.to(device)\n",
        "\n",
        "        model_dict[int(spl[1])] = leaf_model\n",
        "\n",
        "    leaf_pred_dict = dict()\n",
        "    for k, v in model_dict.items():\n",
        "        leaf_model = v\n",
        "        X_emb = ae_model(torch.FloatTensor(test_X).to(device))[1]\n",
        "        Y = torch.softmax(leaf_model(X_emb), dim=-1)\n",
        "        Y_ = Y.cpu().detach().numpy()\n",
        "        leaf_pred_dict[k] = Y_\n",
        "\n",
        "    test_Y_pred = np.zeros(test_Y.shape)\n",
        "\n",
        "    for j in range(len(leaf_nodes)):\n",
        "        y_ = leaf_pred_dict[leaf_nodes[j]][j]\n",
        "        if soft_label_mapping[cluster_assignment[j]] != (int(cat_dict['Normal'])-1):\n",
        "            y_[(int(cat_dict['Normal'])-1)] = 0\n",
        "        y_pred = np.argmax(y_)\n",
        "        test_Y_pred[j] = y_pred\n",
        "\n",
        "    return accuracy_score(test_Y, test_Y_pred)\n",
        "\n",
        "\n",
        "def generate_train_result():\n",
        "    clf = pickle.load(file=open('models/tree.pkl', 'rb'))\n",
        "    soft_label_mapping = pickle.load(file=open('models/soft_label_mapping.pkl', 'rb'))\n",
        "    clustering = pickle.load(file=open('models/clustering.pkl', 'rb'))\n",
        "\n",
        "    dataset, _ , _ = get_training_data(label_ratio=1.0)\n",
        "\n",
        "    test_X = dataset.get_x()\n",
        "    test_Y = dataset.get_y()\n",
        "\n",
        "    leaf_nodes = clf.apply(test_X)\n",
        "    cluster_assignment = clustering.predict(test_X)\n",
        "\n",
        "    print(\"done\")\n",
        "\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "    '''\n",
        "    If model does not exist for a particular leaf, use the default pretrained model.\n",
        "    '''\n",
        "\n",
        "    model_dict = dict()\n",
        "    leaf_model_files = os.listdir('models/leaf_models')\n",
        "    for file in leaf_model_files:\n",
        "        spl = str(file).split(\"_\")\n",
        "\n",
        "        leaf_model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "\n",
        "        if not os.path.exists('models/leaf_models/leaf_' + spl[1]):\n",
        "            leaf_model.load_state_dict(torch.load('models/pretrain_leaf_dnn'))\n",
        "        else:\n",
        "            leaf_model.load_state_dict(torch.load('models/leaf_models/leaf_' + spl[1]))\n",
        "\n",
        "        leaf_model.to(device)\n",
        "\n",
        "        model_dict[int(spl[1])] = leaf_model\n",
        "\n",
        "    leaf_pred_dict = dict()\n",
        "    for k, v in model_dict.items():\n",
        "        leaf_model = v\n",
        "        X_emb = ae_model(torch.FloatTensor(test_X).to(device))[1]\n",
        "        Y = torch.softmax(leaf_model(X_emb), dim=-1)\n",
        "        Y_ = Y.cpu().detach().numpy()\n",
        "        leaf_pred_dict[k] = Y_\n",
        "\n",
        "    test_Y_pred = np.zeros(test_Y.shape)\n",
        "\n",
        "    for j in range(len(leaf_nodes)):\n",
        "        y_ = leaf_pred_dict[leaf_nodes[j]][j]\n",
        "        if soft_label_mapping[cluster_assignment[j]] != (int(cat_dict['Normal'])-1):\n",
        "            y_[(int(cat_dict['Normal'])-1)] = 0\n",
        "        y_pred = np.argmax(y_)\n",
        "        test_Y_pred[j] = y_pred\n",
        "\n",
        "    return accuracy_score(test_Y, test_Y_pred)\n",
        "\n",
        "\n",
        "train_ae(ae_epoch, False)\n",
        "pretrain_leaf_dnn('models/pretrain_leaf_dnn', pretrain_epoch)\n",
        "create_leaf_dnns()\n",
        "generate_result()\n",
        "\n",
        "\n",
        "# train_accuracies = []\n",
        "# test_accuracies = []\n",
        "# # for i in range(150):\n",
        "# #     create_leaf_dnns(remove_priors=True,epochs=i+1)\n",
        "# #     train_accuracies.append(generate_train_result())\n",
        "# #     test_accuracies.append(generate_test_result())\n",
        "# #     print('Epoch: '+str(i)+' -> '+str(train_accuracies[i])+'\\t'+str(test_accuracies[i]))\n",
        "\n",
        "# train_ae(ae_epoch, False)\n",
        "\n",
        "# j = 0\n",
        "# for i in range(5,201,5):\n",
        "#     pretrain_leaf_dnn('models/pretrain_leaf_dnn', i)\n",
        "#     create_leaf_dnns(remove_priors=True,epochs=150)\n",
        "#     train_accuracies.append(generate_train_result())\n",
        "#     test_accuracies.append(generate_test_result())\n",
        "#     print('Epoch: '+str(i)+' -> '+str(train_accuracies[j])+'\\t'+str(test_accuracies[j]))\n",
        "#     j = j+1\n",
        "\n",
        "\n",
        "# train_accuracies = np.array(train_accuracies)\n",
        "# test_accuracies = np.array(test_accuracies)\n",
        "\n",
        "# train_accuracies_df = pd.DataFrame(train_accuracies, columns=['Train Accuracy'])\n",
        "# test_accuracies_df = pd.DataFrame(test_accuracies, columns=['Test Accuracy'])\n",
        "# result = pd.concat([train_accuracies_df,test_accuracies_df], axis=1,sort=False )\n",
        "# result.to_csv('Output/pre-accuracies-frac-100'+'.csv', index=False)\n",
        "# from google.colab import files\n",
        "# files.download('Output/pre-accuracies-frac-100'+'.csv')\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(25192, 118)\n",
            "Clustering Started.\n",
            "Clustering ended.\n",
            "{0.0: 105, 1.0: 6876, 2.0: 13353, 3.0: 5, 4.0: 2144}\n",
            "labeled members dimensions:\n",
            "(22483, 118)\n",
            "(22483,)\n",
            "No. of leaves of decision tree:\n",
            "7\n",
            "25192\n",
            "17634\n",
            "7558\n",
            "epoch 0 , Training loss=0.7018, Validation loss=0.5154\n",
            "epoch 1 , Training loss=0.4727, Validation loss=0.3719\n",
            "epoch 2 , Training loss=0.3679, Validation loss=0.2882\n",
            "epoch 3 , Training loss=0.3055, Validation loss=0.2379\n",
            "epoch 4 , Training loss=0.2672, Validation loss=0.2084\n",
            "epoch 5 , Training loss=0.2418, Validation loss=0.1885\n",
            "epoch 6 , Training loss=0.2257, Validation loss=0.1650\n",
            "epoch 7 , Training loss=0.2110, Validation loss=0.1560\n",
            "epoch 8 , Training loss=0.2022, Validation loss=0.1534\n",
            "epoch 9 , Training loss=0.1947, Validation loss=0.1523\n",
            "epoch 10 , Training loss=0.1920, Validation loss=0.1481\n",
            "epoch 11 , Training loss=0.1851, Validation loss=0.1460\n",
            "epoch 12 , Training loss=0.1788, Validation loss=0.1398\n",
            "epoch 13 , Training loss=0.1760, Validation loss=0.1424\n",
            "epoch 14 , Training loss=0.1808, Validation loss=0.1383\n",
            "epoch 15 , Training loss=0.1700, Validation loss=0.1359\n",
            "epoch 16 , Training loss=0.1673, Validation loss=0.1372\n",
            "epoch 17 , Training loss=0.1699, Validation loss=0.1381\n",
            "epoch 18 , Training loss=0.1658, Validation loss=0.1351\n",
            "epoch 19 , Training loss=0.1611, Validation loss=0.1329\n",
            "epoch 20 , Training loss=0.1626, Validation loss=0.1381\n",
            "epoch 21 , Training loss=0.1618, Validation loss=0.1381\n",
            "epoch 22 , Training loss=0.1595, Validation loss=0.1450\n",
            "epoch 23 , Training loss=0.1591, Validation loss=0.1321\n",
            "epoch 24 , Training loss=0.1575, Validation loss=0.1395\n",
            "epoch 25 , Training loss=0.1546, Validation loss=0.1385\n",
            "epoch 26 , Training loss=0.1526, Validation loss=0.1322\n",
            "epoch 27 , Training loss=0.1566, Validation loss=0.1309\n",
            "epoch 28 , Training loss=0.1510, Validation loss=0.1335\n",
            "epoch 29 , Training loss=0.1513, Validation loss=0.1348\n",
            "epoch 30 , Training loss=0.1526, Validation loss=0.1288\n",
            "epoch 31 , Training loss=0.1495, Validation loss=0.1304\n",
            "epoch 32 , Training loss=0.1458, Validation loss=0.1332\n",
            "epoch 33 , Training loss=0.1484, Validation loss=0.1274\n",
            "epoch 34 , Training loss=0.1462, Validation loss=0.1366\n",
            "epoch 35 , Training loss=0.1444, Validation loss=0.1331\n",
            "epoch 36 , Training loss=0.1478, Validation loss=0.1317\n",
            "epoch 37 , Training loss=0.1460, Validation loss=0.1357\n",
            "epoch 38 , Training loss=0.1414, Validation loss=0.1449\n",
            "epoch 39 , Training loss=0.1484, Validation loss=0.1316\n",
            "epoch 40 , Training loss=0.1422, Validation loss=0.1461\n",
            "epoch 41 , Training loss=0.1417, Validation loss=0.1297\n",
            "epoch 42 , Training loss=0.1384, Validation loss=0.1484\n",
            "epoch 43 , Training loss=0.1429, Validation loss=0.1353\n",
            "epoch 44 , Training loss=0.1369, Validation loss=0.1410\n",
            "epoch 45 , Training loss=0.1421, Validation loss=0.1554\n",
            "epoch 46 , Training loss=0.1403, Validation loss=0.1312\n",
            "epoch 47 , Training loss=0.1401, Validation loss=0.1403\n",
            "epoch 48 , Training loss=0.1445, Validation loss=0.1395\n",
            "epoch 49 , Training loss=0.1354, Validation loss=0.1730\n",
            "epoch 50 , Training loss=0.1398, Validation loss=0.1430\n",
            "epoch 51 , Training loss=0.1418, Validation loss=0.1416\n",
            "epoch 52 , Training loss=0.1343, Validation loss=0.1386\n",
            "epoch 53 , Training loss=0.1381, Validation loss=0.1330\n",
            "epoch 54 , Training loss=0.1290, Validation loss=0.1621\n",
            "epoch 55 , Training loss=0.1360, Validation loss=0.1333\n",
            "epoch 56 , Training loss=0.1318, Validation loss=0.1427\n",
            "epoch 57 , Training loss=0.1376, Validation loss=0.2411\n",
            "epoch 58 , Training loss=0.1384, Validation loss=0.1352\n",
            "epoch 59 , Training loss=0.1296, Validation loss=0.1358\n",
            "epoch 60 , Training loss=0.1308, Validation loss=0.1337\n",
            "epoch 61 , Training loss=0.1329, Validation loss=0.1441\n",
            "epoch 62 , Training loss=0.1377, Validation loss=0.1406\n",
            "epoch 63 , Training loss=0.1366, Validation loss=0.1375\n",
            "epoch 64 , Training loss=0.1321, Validation loss=0.1343\n",
            "epoch 65 , Training loss=0.1329, Validation loss=0.1453\n",
            "epoch 66 , Training loss=0.1303, Validation loss=0.1407\n",
            "epoch 67 , Training loss=0.1363, Validation loss=0.1327\n",
            "epoch 68 , Training loss=0.1327, Validation loss=0.1336\n",
            "epoch 69 , Training loss=0.1356, Validation loss=0.1412\n",
            "epoch 70 , Training loss=0.1261, Validation loss=0.1347\n",
            "epoch 71 , Training loss=0.1291, Validation loss=0.1360\n",
            "epoch 72 , Training loss=0.1326, Validation loss=0.1325\n",
            "epoch 73 , Training loss=0.1337, Validation loss=0.1490\n",
            "epoch 74 , Training loss=0.1329, Validation loss=0.1359\n",
            "epoch 75 , Training loss=0.1383, Validation loss=0.1427\n",
            "epoch 76 , Training loss=0.1290, Validation loss=0.1398\n",
            "epoch 77 , Training loss=0.1311, Validation loss=0.1351\n",
            "epoch 78 , Training loss=0.1260, Validation loss=0.1478\n",
            "epoch 79 , Training loss=0.1357, Validation loss=0.1387\n",
            "model saved to models/train_ae.\n",
            "epoch 0; T loss=0.8388 T Accuracy=0.8124\n",
            "epoch 1; T loss=0.3781 T Accuracy=0.9025\n",
            "epoch 2; T loss=0.2801 T Accuracy=0.9001\n",
            "epoch 3; T loss=0.2434 T Accuracy=0.9164\n",
            "epoch 4; T loss=0.2202 T Accuracy=0.9220\n",
            "epoch 5; T loss=0.2024 T Accuracy=0.9224\n",
            "epoch 6; T loss=0.1901 T Accuracy=0.9255\n",
            "epoch 7; T loss=0.1777 T Accuracy=0.9326\n",
            "epoch 8; T loss=0.1756 T Accuracy=0.9396\n",
            "epoch 9; T loss=0.1663 T Accuracy=0.9501\n",
            "epoch 10; T loss=0.1631 T Accuracy=0.9537\n",
            "epoch 11; T loss=0.1603 T Accuracy=0.9530\n",
            "epoch 12; T loss=0.1550 T Accuracy=0.9553\n",
            "epoch 13; T loss=0.1518 T Accuracy=0.9570\n",
            "epoch 14; T loss=0.1487 T Accuracy=0.9560\n",
            "epoch 15; T loss=0.1467 T Accuracy=0.9575\n",
            "epoch 16; T loss=0.1425 T Accuracy=0.9587\n",
            "epoch 17; T loss=0.1432 T Accuracy=0.9593\n",
            "epoch 18; T loss=0.1412 T Accuracy=0.9592\n",
            "epoch 19; T loss=0.1394 T Accuracy=0.9611\n",
            "epoch 20; T loss=0.1363 T Accuracy=0.9595\n",
            "epoch 21; T loss=0.1277 T Accuracy=0.9606\n",
            "epoch 22; T loss=0.1338 T Accuracy=0.9606\n",
            "epoch 23; T loss=0.1337 T Accuracy=0.9601\n",
            "epoch 24; T loss=0.1301 T Accuracy=0.9616\n",
            "epoch 25; T loss=0.1305 T Accuracy=0.9608\n",
            "epoch 26; T loss=0.1356 T Accuracy=0.9607\n",
            "epoch 27; T loss=0.1288 T Accuracy=0.9613\n",
            "epoch 28; T loss=0.1294 T Accuracy=0.9617\n",
            "epoch 29; T loss=0.1274 T Accuracy=0.9620\n",
            "epoch 30; T loss=0.1271 T Accuracy=0.9623\n",
            "epoch 31; T loss=0.1245 T Accuracy=0.9623\n",
            "epoch 32; T loss=0.1258 T Accuracy=0.9623\n",
            "epoch 33; T loss=0.1249 T Accuracy=0.9635\n",
            "epoch 34; T loss=0.1231 T Accuracy=0.9626\n",
            "epoch 35; T loss=0.1242 T Accuracy=0.9628\n",
            "epoch 36; T loss=0.1210 T Accuracy=0.9614\n",
            "epoch 37; T loss=0.1181 T Accuracy=0.9621\n",
            "epoch 38; T loss=0.1222 T Accuracy=0.9627\n",
            "epoch 39; T loss=0.1184 T Accuracy=0.9629\n",
            "epoch 40; T loss=0.1187 T Accuracy=0.9638\n",
            "epoch 41; T loss=0.1191 T Accuracy=0.9631\n",
            "epoch 42; T loss=0.1181 T Accuracy=0.9633\n",
            "epoch 43; T loss=0.1168 T Accuracy=0.9626\n",
            "epoch 44; T loss=0.1148 T Accuracy=0.9633\n",
            "epoch 45; T loss=0.1189 T Accuracy=0.9621\n",
            "epoch 46; T loss=0.1150 T Accuracy=0.9637\n",
            "epoch 47; T loss=0.1152 T Accuracy=0.9639\n",
            "epoch 48; T loss=0.1142 T Accuracy=0.9641\n",
            "epoch 49; T loss=0.1104 T Accuracy=0.9640\n",
            "epoch 50; T loss=0.1085 T Accuracy=0.9645\n",
            "epoch 51; T loss=0.1133 T Accuracy=0.9643\n",
            "epoch 52; T loss=0.1151 T Accuracy=0.9646\n",
            "epoch 53; T loss=0.1115 T Accuracy=0.9653\n",
            "epoch 54; T loss=0.1092 T Accuracy=0.9638\n",
            "epoch 55; T loss=0.1117 T Accuracy=0.9658\n",
            "epoch 56; T loss=0.1092 T Accuracy=0.9650\n",
            "epoch 57; T loss=0.1120 T Accuracy=0.9646\n",
            "epoch 58; T loss=0.1091 T Accuracy=0.9654\n",
            "epoch 59; T loss=0.1034 T Accuracy=0.9659\n",
            "epoch 60; T loss=0.1064 T Accuracy=0.9668\n",
            "epoch 61; T loss=0.1065 T Accuracy=0.9657\n",
            "epoch 62; T loss=0.1043 T Accuracy=0.9668\n",
            "epoch 63; T loss=0.1112 T Accuracy=0.9661\n",
            "epoch 64; T loss=0.1026 T Accuracy=0.9677\n",
            "epoch 65; T loss=0.1053 T Accuracy=0.9672\n",
            "epoch 66; T loss=0.1053 T Accuracy=0.9667\n",
            "epoch 67; T loss=0.1031 T Accuracy=0.9669\n",
            "epoch 68; T loss=0.1019 T Accuracy=0.9681\n",
            "epoch 69; T loss=0.0999 T Accuracy=0.9663\n",
            "epoch 70; T loss=0.1027 T Accuracy=0.9673\n",
            "epoch 71; T loss=0.1012 T Accuracy=0.9672\n",
            "epoch 72; T loss=0.1082 T Accuracy=0.9671\n",
            "epoch 73; T loss=0.0970 T Accuracy=0.9681\n",
            "epoch 74; T loss=0.1043 T Accuracy=0.9667\n",
            "epoch 75; T loss=0.1020 T Accuracy=0.9664\n",
            "epoch 76; T loss=0.1008 T Accuracy=0.9679\n",
            "epoch 77; T loss=0.1011 T Accuracy=0.9674\n",
            "epoch 78; T loss=0.1005 T Accuracy=0.9683\n",
            "epoch 79; T loss=0.0991 T Accuracy=0.9680\n",
            "epoch 80; T loss=0.1015 T Accuracy=0.9688\n",
            "epoch 81; T loss=0.0969 T Accuracy=0.9689\n",
            "epoch 82; T loss=0.1082 T Accuracy=0.9686\n",
            "epoch 83; T loss=0.0976 T Accuracy=0.9676\n",
            "epoch 84; T loss=0.0970 T Accuracy=0.9684\n",
            "epoch 85; T loss=0.0977 T Accuracy=0.9680\n",
            "epoch 86; T loss=0.0970 T Accuracy=0.9687\n",
            "epoch 87; T loss=0.0963 T Accuracy=0.9693\n",
            "epoch 88; T loss=0.0988 T Accuracy=0.9694\n",
            "epoch 89; T loss=0.1025 T Accuracy=0.9682\n",
            "epoch 90; T loss=0.0959 T Accuracy=0.9682\n",
            "epoch 91; T loss=0.0979 T Accuracy=0.9695\n",
            "epoch 92; T loss=0.0973 T Accuracy=0.9682\n",
            "epoch 93; T loss=0.0977 T Accuracy=0.9682\n",
            "epoch 94; T loss=0.0949 T Accuracy=0.9695\n",
            "epoch 95; T loss=0.0930 T Accuracy=0.9688\n",
            "epoch 96; T loss=0.1003 T Accuracy=0.9674\n",
            "epoch 97; T loss=0.0977 T Accuracy=0.9693\n",
            "epoch 98; T loss=0.0972 T Accuracy=0.9684\n",
            "epoch 99; T loss=0.0947 T Accuracy=0.9689\n",
            "epoch 100; T loss=0.0930 T Accuracy=0.9684\n",
            "epoch 101; T loss=0.0956 T Accuracy=0.9694\n",
            "epoch 102; T loss=0.0958 T Accuracy=0.9689\n",
            "epoch 103; T loss=0.0938 T Accuracy=0.9690\n",
            "epoch 104; T loss=0.0955 T Accuracy=0.9687\n",
            "epoch 105; T loss=0.0934 T Accuracy=0.9689\n",
            "epoch 106; T loss=0.0931 T Accuracy=0.9700\n",
            "epoch 107; T loss=0.0963 T Accuracy=0.9702\n",
            "epoch 108; T loss=0.0938 T Accuracy=0.9697\n",
            "epoch 109; T loss=0.1010 T Accuracy=0.9669\n",
            "epoch 110; T loss=0.0953 T Accuracy=0.9694\n",
            "epoch 111; T loss=0.0934 T Accuracy=0.9693\n",
            "epoch 112; T loss=0.0973 T Accuracy=0.9695\n",
            "epoch 113; T loss=0.0896 T Accuracy=0.9695\n",
            "epoch 114; T loss=0.0949 T Accuracy=0.9706\n",
            "epoch 115; T loss=0.0913 T Accuracy=0.9699\n",
            "epoch 116; T loss=0.0902 T Accuracy=0.9692\n",
            "epoch 117; T loss=0.0974 T Accuracy=0.9694\n",
            "epoch 118; T loss=0.0909 T Accuracy=0.9693\n",
            "epoch 119; T loss=0.0904 T Accuracy=0.9704\n",
            "epoch 120; T loss=0.0940 T Accuracy=0.9686\n",
            "epoch 121; T loss=0.0896 T Accuracy=0.9706\n",
            "epoch 122; T loss=0.0918 T Accuracy=0.9701\n",
            "epoch 123; T loss=0.0905 T Accuracy=0.9694\n",
            "epoch 124; T loss=0.0905 T Accuracy=0.9706\n",
            "epoch 125; T loss=0.0987 T Accuracy=0.9676\n",
            "epoch 126; T loss=0.0898 T Accuracy=0.9702\n",
            "epoch 127; T loss=0.0917 T Accuracy=0.9693\n",
            "epoch 128; T loss=0.0928 T Accuracy=0.9702\n",
            "epoch 129; T loss=0.0901 T Accuracy=0.9696\n",
            "epoch 130; T loss=0.0905 T Accuracy=0.9696\n",
            "epoch 131; T loss=0.0907 T Accuracy=0.9689\n",
            "epoch 132; T loss=0.0918 T Accuracy=0.9684\n",
            "epoch 133; T loss=0.0886 T Accuracy=0.9710\n",
            "epoch 134; T loss=0.0901 T Accuracy=0.9705\n",
            "epoch 135; T loss=0.0917 T Accuracy=0.9694\n",
            "epoch 136; T loss=0.0899 T Accuracy=0.9701\n",
            "epoch 137; T loss=0.0914 T Accuracy=0.9712\n",
            "epoch 138; T loss=0.0892 T Accuracy=0.9706\n",
            "epoch 139; T loss=0.0876 T Accuracy=0.9706\n",
            "epoch 140; T loss=0.0856 T Accuracy=0.9708\n",
            "epoch 141; T loss=0.0900 T Accuracy=0.9703\n",
            "epoch 142; T loss=0.0899 T Accuracy=0.9708\n",
            "epoch 143; T loss=0.0902 T Accuracy=0.9708\n",
            "epoch 144; T loss=0.0856 T Accuracy=0.9722\n",
            "epoch 145; T loss=0.0904 T Accuracy=0.9714\n",
            "epoch 146; T loss=0.0941 T Accuracy=0.9719\n",
            "epoch 147; T loss=0.0936 T Accuracy=0.9708\n",
            "epoch 148; T loss=0.0891 T Accuracy=0.9703\n",
            "epoch 149; T loss=0.0901 T Accuracy=0.9715\n",
            "epoch 150; T loss=0.0906 T Accuracy=0.9713\n",
            "epoch 151; T loss=0.0855 T Accuracy=0.9725\n",
            "epoch 152; T loss=0.0931 T Accuracy=0.9718\n",
            "epoch 153; T loss=0.0932 T Accuracy=0.9707\n",
            "epoch 154; T loss=0.0912 T Accuracy=0.9699\n",
            "epoch 155; T loss=0.1014 T Accuracy=0.9707\n",
            "epoch 156; T loss=0.0890 T Accuracy=0.9711\n",
            "epoch 157; T loss=0.0891 T Accuracy=0.9726\n",
            "epoch 158; T loss=0.0864 T Accuracy=0.9726\n",
            "epoch 159; T loss=0.0903 T Accuracy=0.9716\n",
            "epoch 160; T loss=0.0897 T Accuracy=0.9715\n",
            "epoch 161; T loss=0.0856 T Accuracy=0.9724\n",
            "epoch 162; T loss=0.0850 T Accuracy=0.9724\n",
            "epoch 163; T loss=0.0877 T Accuracy=0.9709\n",
            "epoch 164; T loss=0.0868 T Accuracy=0.9713\n",
            "epoch 165; T loss=0.0896 T Accuracy=0.9706\n",
            "epoch 166; T loss=0.0890 T Accuracy=0.9718\n",
            "epoch 167; T loss=0.0880 T Accuracy=0.9714\n",
            "epoch 168; T loss=0.0905 T Accuracy=0.9713\n",
            "epoch 169; T loss=0.0879 T Accuracy=0.9716\n",
            "epoch 170; T loss=0.0876 T Accuracy=0.9712\n",
            "epoch 171; T loss=0.0837 T Accuracy=0.9730\n",
            "epoch 172; T loss=0.0838 T Accuracy=0.9734\n",
            "epoch 173; T loss=0.0935 T Accuracy=0.9719\n",
            "epoch 174; T loss=0.0842 T Accuracy=0.9736\n",
            "epoch 175; T loss=0.0872 T Accuracy=0.9727\n",
            "epoch 176; T loss=0.0818 T Accuracy=0.9738\n",
            "epoch 177; T loss=0.0840 T Accuracy=0.9725\n",
            "epoch 178; T loss=0.0845 T Accuracy=0.9746\n",
            "epoch 179; T loss=0.0820 T Accuracy=0.9728\n",
            "epoch 180; T loss=0.0887 T Accuracy=0.9729\n",
            "epoch 181; T loss=0.0946 T Accuracy=0.9729\n",
            "epoch 182; T loss=0.0856 T Accuracy=0.9707\n",
            "epoch 183; T loss=0.0843 T Accuracy=0.9722\n",
            "epoch 184; T loss=0.0803 T Accuracy=0.9737\n",
            "epoch 185; T loss=0.0874 T Accuracy=0.9711\n",
            "epoch 186; T loss=0.0863 T Accuracy=0.9712\n",
            "epoch 187; T loss=0.0848 T Accuracy=0.9738\n",
            "epoch 188; T loss=0.0854 T Accuracy=0.9729\n",
            "epoch 189; T loss=0.0859 T Accuracy=0.9725\n",
            "epoch 190; T loss=0.0853 T Accuracy=0.9729\n",
            "epoch 191; T loss=0.0833 T Accuracy=0.9736\n",
            "epoch 192; T loss=0.0814 T Accuracy=0.9728\n",
            "epoch 193; T loss=0.0849 T Accuracy=0.9718\n",
            "epoch 194; T loss=0.0865 T Accuracy=0.9731\n",
            "epoch 195; T loss=0.0835 T Accuracy=0.9737\n",
            "epoch 196; T loss=0.0955 T Accuracy=0.9722\n",
            "epoch 197; T loss=0.0859 T Accuracy=0.9707\n",
            "epoch 198; T loss=0.0889 T Accuracy=0.9718\n",
            "epoch 199; T loss=0.0820 T Accuracy=0.9730\n",
            "model saved to models/pretrain_leaf_dnn.\n",
            "done\n",
            "[[ 927    7 1715   92   13]\n",
            " [ 428 6048  502    0  480]\n",
            " [  51  150 9249   34  227]\n",
            " [   9    5   30   36  120]\n",
            " [ 105  551  390    1 1374]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.34      0.43      2754\n",
            "           1       0.89      0.81      0.85      7458\n",
            "           2       0.78      0.95      0.86      9711\n",
            "           3       0.22      0.18      0.20       200\n",
            "           4       0.62      0.57      0.59      2421\n",
            "\n",
            "    accuracy                           0.78     22544\n",
            "   macro avg       0.62      0.57      0.59     22544\n",
            "weighted avg       0.77      0.78      0.77     22544\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1owp0H8Gav9"
      },
      "source": [
        "def black_box_test(x):\n",
        "    clf = pickle.load(file=open('models/tree.pkl', 'rb'))\n",
        "    soft_label_mapping = pickle.load(file=open('models/soft_label_mapping.pkl', 'rb'))\n",
        "    clustering = pickle.load(file=open('models/clustering.pkl', 'rb'))\n",
        "\n",
        "    leaf_nodes = clf.apply(x)\n",
        "    cluster_assignment = clustering.predict(x)\n",
        "\n",
        "    #print(\"done\")\n",
        "\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "    '''\n",
        "    If model does not exist for a particular leaf, use the default pretrained model.\n",
        "    '''\n",
        "\n",
        "    model_dict = dict()\n",
        "    leaf_model_files = os.listdir('models/leaf_models')\n",
        "    for file in leaf_model_files:\n",
        "        spl = str(file).split(\"_\")\n",
        "\n",
        "        leaf_model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "\n",
        "        if not os.path.exists('models/leaf_models/leaf_' + spl[1]):\n",
        "            leaf_model.load_state_dict(torch.load('models/pretrain_leaf_dnn'))\n",
        "        else:\n",
        "            leaf_model.load_state_dict(torch.load('models/leaf_models/leaf_' + spl[1]))\n",
        "\n",
        "        leaf_model.to(device)\n",
        "\n",
        "        model_dict[int(spl[1])] = leaf_model\n",
        "\n",
        "    leaf_pred_dict = dict()\n",
        "    for k, v in model_dict.items():\n",
        "        leaf_model = v\n",
        "        X_emb = ae_model(torch.FloatTensor(test_X).to(device))[1]\n",
        "        Y = torch.softmax(leaf_model(X_emb), dim=-1)\n",
        "        Y_ = Y.cpu().detach().numpy()\n",
        "        leaf_pred_dict[k] = Y_\n",
        "\n",
        "    test_Y_pred = np.zeros(test_Y.shape)\n",
        "\n",
        "    for j in range(len(leaf_nodes)):\n",
        "        y_ = leaf_pred_dict[leaf_nodes[j]][j]\n",
        "        if soft_label_mapping[cluster_assignment[j]] != int(cat_dict['Normal']):\n",
        "            y_[int(cat_dict['Normal'])-1] = 0\n",
        "        y_pred = np.argmax(y_)\n",
        "        test_Y_pred[j] = y_pred\n",
        "    return test_Y_pred\n",
        "\n",
        "def getBinPrediction(pred_Y):\n",
        "    bin_pred_y = []\n",
        "    for y in pred_Y:\n",
        "        if y != (int(cat_dict['Normal'])-1):\n",
        "            bin_pred_y.append(1)\n",
        "        else:\n",
        "            bin_pred_y.append(0)\n",
        "    return bin_pred_y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1z1hMclCX8jN",
        "outputId": "28c02622-117a-4ead-ee3e-ea71f22d11b3"
      },
      "source": [
        "print(int(cat_dict['Normal'])-1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLFBk0y4J8CV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12ae285c-3aa7-42a2-b037-803b11f2dc39"
      },
      "source": [
        "test_X = test_dataset.get_x()\n",
        "test_Y = test_dataset.get_y()\n",
        "\n",
        "pred_Y = black_box_test(test_X)\n",
        "bin_pred_y = getBinPrediction(pred_Y)\n",
        "print(accuracy_score(test_Y, pred_Y))\n",
        "\n",
        "\n",
        "print(len(pred_Y))\n",
        "print(np.sum(np.array(bin_pred_y)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.5461320085166785\n",
            "22544\n",
            "22180\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqZ-2ur7VboE"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from google.colab import files\n",
        "\n",
        "def make_confusion_matrix(cf,\n",
        "                          group_names=None,\n",
        "                          categories='auto',\n",
        "                          count=True,\n",
        "                          percent=True,\n",
        "                          cbar=True,\n",
        "                          xyticks=True,\n",
        "                          xyplotlabels=True,\n",
        "                          sum_stats=False,\n",
        "                          figsize=None,\n",
        "                          cmap='Blues',\n",
        "                          title=None,\n",
        "                          download=True):\n",
        "    '''\n",
        "    This function will make a pretty plot of an sklearn Confusion Matrix cm using a Seaborn heatmap visualization.\n",
        "    Arguments\n",
        "    ---------\n",
        "    cf:            confusion matrix to be passed in\n",
        "    group_names:   List of strings that represent the labels row by row to be shown in each square.\n",
        "    categories:    List of strings containing the categories to be displayed on the x,y axis. Default is 'auto'\n",
        "    count:         If True, show the raw number in the confusion matrix. Default is True.\n",
        "    normalize:     If True, show the proportions for each category. Default is True.\n",
        "    cbar:          If True, show the color bar. The cbar values are based off the values in the confusion matrix.\n",
        "                   Default is True.\n",
        "    xyticks:       If True, show x and y ticks. Default is True.\n",
        "    xyplotlabels:  If True, show 'True Label' and 'Predicted Label' on the figure. Default is True.\n",
        "    sum_stats:     If True, display summary statistics below the figure. Default is True.\n",
        "    figsize:       Tuple representing the figure size. Default will be the matplotlib rcParams value.\n",
        "    cmap:          Colormap of the values displayed from matplotlib.pyplot.cm. Default is 'Blues'\n",
        "                   See http://matplotlib.org/examples/color/colormaps_reference.html\n",
        "\n",
        "    title:         Title for the heatmap. Default is None.\n",
        "    '''\n",
        "\n",
        "\n",
        "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
        "    blanks = ['' for i in range(cf.size)]\n",
        "\n",
        "    if group_names and len(group_names)==cf.size:\n",
        "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
        "    else:\n",
        "        group_labels = blanks\n",
        "\n",
        "    if count:\n",
        "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
        "    else:\n",
        "        group_counts = blanks\n",
        "\n",
        "    if percent:\n",
        "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
        "    else:\n",
        "        group_percentages = blanks\n",
        "\n",
        "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
        "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
        "\n",
        "\n",
        "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
        "    if sum_stats:\n",
        "        #Accuracy is sum of diagonal divided by total observations\n",
        "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
        "\n",
        "        #if it is a binary confusion matrix, show some more stats\n",
        "        if len(cf)==2:\n",
        "            #Metrics for Binary Confusion Matrices\n",
        "            tpr = cf[0,0] / np.sum(cf[0])\n",
        "            fpr = cf[1,0]/np.sum(cf[1])\n",
        "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nTrue Positive Rate={:0.3f}\\nFalse Postitive Rate={:0.3f}\".format(\n",
        "                accuracy,tpr,fpr)\n",
        "        else:\n",
        "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
        "    else:\n",
        "        stats_text = \"\"\n",
        "\n",
        "\n",
        "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
        "    if figsize==None:\n",
        "        #Get default figure size if not set\n",
        "        figsize = plt.rcParams.get('figure.figsize')\n",
        "\n",
        "    if xyticks==False:\n",
        "        #Do not show categories if xyticks is False\n",
        "        categories=False\n",
        "\n",
        "\n",
        "    # MAKE THE HEATMAP VISUALIZATION\n",
        "    plt.figure(figsize=figsize)\n",
        "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
        "\n",
        "    if xyplotlabels:\n",
        "        plt.ylabel('True label')\n",
        "        plt.xlabel('Predicted label' + stats_text)\n",
        "    else:\n",
        "        plt.xlabel(stats_text)\n",
        "\n",
        "    if title:\n",
        "        plt.title(title)\n",
        "    if download:\n",
        "        plt.savefig(\"cm_bin_2.png\")\n",
        "        files.download('cm_bin_2.png')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "id": "vwpVPPUUCp-W",
        "outputId": "c45047f5-1a6a-4313-eb7a-d6fd9e59f59a"
      },
      "source": [
        "categories = ['R2L', 'DoS', 'Normal', 'U2R', 'Probe']\n",
        "matrix_1 = np.array([[ 845,   22, 1130,   35,  722],\n",
        " [ 358, 6236,  722,   16,  126],\n",
        " [  38,   63, 9260,   50,  300],\n",
        " [  45,   12,  100,  26,   17],\n",
        " [  41,  138,  330,   25, 1887]])\n",
        "\n",
        "matrix_2 = np.array([[ 845,   22, 1130,   35,  722],\n",
        " [ 358, 3208,  721,    6,   49],\n",
        " [  27,   61, 1738,   33,  293],\n",
        " [  45,   12,  100,   26,   17],\n",
        " [  41,  138,  330,   25, 1868]])\n",
        "\n",
        "tn = matrix_2[2][2]\n",
        "fp = np.sum(matrix_2[2]) - tn\n",
        "fn = matrix_2[0][2] + matrix_2[1][2] + matrix_2[3][2] + matrix_2[4][2]\n",
        "tp = np.sum(matrix_2) - tn - fn - fp\n",
        "matrix_3 = np.array([[tp,fn],[fp,tn]])\n",
        "\n",
        "tn = matrix_1[2][2]\n",
        "fp = np.sum(matrix_1[2]) - tn\n",
        "fn = matrix_1[0][2] + matrix_1[1][2] + matrix_2[3][2] + matrix_2[4][2]\n",
        "tp = np.sum(matrix_1) - tn - fn - fp\n",
        "matrix_4 = np.array([[tp,fn],[fp,tn]])\n",
        "\n",
        "categories = ['Attack','Normal']\n",
        "make_confusion_matrix(matrix_3, figsize=(8,6), cbar=False, categories=categories, percent=False, download=False, sum_stats=True)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}