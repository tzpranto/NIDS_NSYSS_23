{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "afsAob2-d7cE",
        "Rjc14eNYMV9o",
        "elwwzKe7eGQM",
        "63QD0lJqeWrv"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hi-ya4VEWrjY",
        "outputId": "143544ed-7851-4ad8-cbf4-5f379475340b"
      },
      "source": [
        "! git clone https://github.com/tzpranto/NIDS_NSYSS_23"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'idsgan18'...\n",
            "remote: Enumerating objects: 46, done.\u001b[K\n",
            "remote: Counting objects: 100% (46/46), done.\u001b[K\n",
            "remote: Compressing objects: 100% (45/45), done.\u001b[K\n",
            "remote: Total 46 (delta 10), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (46/46), done.\n",
            "Checking out files: 100% (34/34), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8W_1yFO5XhOt",
        "outputId": "555109bf-a50b-4e05-a895-e221d21f7d17"
      },
      "source": [
        "cd NIDS_NSYSS_23/idsgan18/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/idsgan18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xb4EMkB57rZC",
        "outputId": "66c6f36e-f25d-4ac6-8cd3-6888d1e4ba67"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/idsgan18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lQveAbQkX88-",
        "outputId": "6a689131-526e-4047-c9b3-3ddd21d78856"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 808916\n",
            "-rw-r--r-- 1 root root 160082766 Apr 23 22:41 blackbox_train.csv\n",
            "-rw-r--r-- 1 root root  85665467 Apr 23 22:41 discriminator_train_normal.csv\n",
            "-rw-r--r-- 1 root root 100442630 Apr 23 22:41 GAN_train.csv\n",
            "-rw-r--r-- 1 root root   6137305 Apr 23 22:41 generator_test_attack.csv\n",
            "-rw-r--r-- 1 root root  30848053 Apr 23 22:41 generator_test_combined.csv\n",
            "-rw-r--r-- 1 root root  24712515 Apr 23 22:41 generator_test_normal.csv\n",
            "-rw-r--r-- 1 root root  14778930 Apr 23 22:41 generator_train.csv\n",
            "-rw-r--r-- 1 root root   1814092 Apr 23 22:15 KDDTest-21.txt\n",
            "-rw-r--r-- 1 root root  57270074 Apr 23 22:15 KDDTest+.csv\n",
            "-rw-r--r-- 1 root root   3441513 Apr 23 22:15 KDDTest+.txt\n",
            "-rw-r--r-- 1 root root   3822033 Apr 23 22:15 KDDTrain+_20Percent.txt\n",
            "-rw-r--r-- 1 root root 320170229 Apr 23 22:15 KDDTrain+.csv\n",
            "-rw-r--r-- 1 root root  19109424 Apr 23 22:15 KDDTrain+.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGGvixDr5LBX"
      },
      "source": [
        "!python data_generator.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afsAob2-d7cE"
      },
      "source": [
        "#Training RNN IDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2UHg9Ng8Yam6"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class Blackbox_IDS(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, 80, 1, batch_first=True)\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(80, 40),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(40, 20),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(20, 5),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(5, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lstm(x)[0]\n",
        "        x = torch.squeeze(x)\n",
        "        x = self.layer(x)\n",
        "        x = torch.nn.Sigmoid()(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oNmfpwZZXGoh"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from preprocessing import preprocess3, create_batch1\n",
        "#from model.model_class import Blackbox_IDS\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(12345)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train = pd.read_csv('dataset/blackbox_train.csv')\n",
        "test = pd.read_csv('dataset/KDDTest+.csv')\n",
        "\n",
        "trainx, trainy, testx, testy = preprocess3(train, test)\n",
        "#trainx, _, trainy, _ = train_test_split(trainx, trainy, test_size=0.1, random_state=12345)\n",
        "print(trainx.shape)\n",
        "\n",
        "input_dim = trainx.shape[1]\n",
        "output_dim = 1\n",
        "batch_size = 64\n",
        "tr_N = len(trainx)\n",
        "te_N = len(testx)\n",
        "ids_model = Blackbox_IDS(input_dim, output_dim).to(device)\n",
        "optimizer = torch.optim.Adam(ids_model.parameters(), lr=0.001)\n",
        "loss_f = torch.nn.BCELoss()\n",
        "max_epoch = 50\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "\n",
        "def train(x, y):\n",
        "    ids_model.train()\n",
        "    batch_x, batch_y = create_batch1(x, y, batch_size)\n",
        "    run_loss = 0\n",
        "    for x, y in zip(batch_x, batch_y):\n",
        "        ids_model.zero_grad()\n",
        "        x_t = torch.FloatTensor(x).to(device)\n",
        "        x_t = torch.reshape(x_t, [batch_size,1,-1])\n",
        "        y_t = torch.FloatTensor(y).to(device)\n",
        "        y_t = torch.reshape(y_t, [batch_size])\n",
        "\n",
        "        out = ids_model(x_t)\n",
        "        out = torch.reshape(out, [batch_size])\n",
        "\n",
        "        loss = loss_f(out, y_t)\n",
        "\n",
        "        run_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return run_loss / tr_N\n",
        "\n",
        "\n",
        "def test(x, y):\n",
        "    ids_model.eval()\n",
        "    batch_x, batch_y = create_batch1(x, y, batch_size)\n",
        "    run_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in zip(batch_x, batch_y):\n",
        "            x_t = torch.FloatTensor(x).to(device)\n",
        "            x_t = torch.reshape(x_t, [batch_size,1,-1])\n",
        "            y_t = torch.FloatTensor(y).to(device)\n",
        "            y_t = torch.reshape(y_t, [batch_size])\n",
        "\n",
        "            out = ids_model(x_t)\n",
        "            out = torch.reshape(out, [batch_size])\n",
        "\n",
        "            loss = loss_f(out, y_t)\n",
        "\n",
        "            run_loss += loss.item()\n",
        "    return run_loss / te_N\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"IDS start training\")\n",
        "    print(\"-\" * 100)\n",
        "    for epoch in range(max_epoch):\n",
        "        train_loss = train(trainx, trainy)\n",
        "        test_loss = test(testx, testy)\n",
        "\n",
        "        x_t = torch.FloatTensor(testx).to(device)\n",
        "        x_t = torch.reshape(x_t, [x_t.shape[0],1,-1])\n",
        "\n",
        "        out = ids_model(x_t)\n",
        "        out = torch.reshape(out, [len(testy)])\n",
        "        out_np = out.cpu().detach().numpy()\n",
        "\n",
        "        ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "\n",
        "        correct = np.sum(np.equal(ids_pred_label, testy))\n",
        "        acc = correct / len(testy)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(f\"{epoch} : {train_loss} \\t {test_loss} \\t {acc}\")\n",
        "\n",
        "    print(\"IDS finished training\")\n",
        "\n",
        "    x_t = torch.FloatTensor(testx).to(device)\n",
        "    x_t = torch.reshape(x_t, [x_t.shape[0],1,-1])\n",
        "    out = ids_model(x_t)\n",
        "    out = torch.reshape(out, [len(testy)])\n",
        "    out_np = out.cpu().detach().numpy()\n",
        "\n",
        "    ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "\n",
        "    conf_mat = confusion_matrix(testy, ids_pred_label)\n",
        "    print(conf_mat)\n",
        "\n",
        "    torch.save(ids_model.state_dict(), 'model/IDS.pth')\n",
        "    plt.plot(train_losses, label=\"train\")\n",
        "    plt.plot(test_losses, label=\"test\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0CfUrS0SYEtl"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOgyJA_Hezzo"
      },
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torch import IntTensor\n",
        "var = Variable(IntTensor([[1,0],[0,1]]))\n",
        "print(list(var.size())[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wz2bX9HWLqZO"
      },
      "source": [
        "import os\n",
        "class BlackBoxWrapper():\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.model = Blackbox_IDS(input_dim, output_dim).to(device)\n",
        "        if os.path.exists('model/IDS.pth'):\n",
        "            param = torch.load('model/IDS.pth')\n",
        "            self.model.load_state_dict(param)\n",
        "        else:\n",
        "            print('BlackBox Model not saved yet')\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = torch.reshape(x, [x.shape[0],1,-1])\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "    def eval(self):\n",
        "        self.model.eval()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE8kaO1eKH5p"
      },
      "source": [
        "#Training Fuzzyness IDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlzE5lLPMroo"
      },
      "source": [
        "percent = 0.1\n",
        "normal_index = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjc14eNYMV9o"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJmQ7-BXMMUg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pprint\n",
        "from pickle import dump\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "debug = False\n",
        "\n",
        "NSLKDD_ATTACK_DICT = {\n",
        "    'DoS': ['apache2', 'back', 'land', 'neptune', 'mailbomb', 'pod', 'processtable', 'smurf', 'teardrop', 'udpstorm'],\n",
        "    'Probe': ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan'],\n",
        "    'Privilege': ['buffer_overflow', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm','httptunnel'],\n",
        "    'Access': ['ftp_write', 'guess_passwd', 'imap', 'multihop', 'named', 'phf', 'sendmail',\n",
        "               'snmpgetattack', 'snmpguess', 'spy', 'warezclient', 'warezmaster', 'xlock', 'xsnoop','worm'],\n",
        "    'Normal': ['normal']\n",
        "}\n",
        "\n",
        "\n",
        "NSLKDD_COL_NAMES = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
        "                    \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "                    \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n",
        "                    \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "                    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
        "                    \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "                    \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
        "                    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "                    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
        "                    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"class\", \"difficulty_level\"]\n",
        "\n",
        "binary = False\n",
        "\n",
        "NSLKDD_ATTACK_MAP = dict()\n",
        "for k, v in NSLKDD_ATTACK_DICT.items():\n",
        "    for att in v:\n",
        "        if not binary:\n",
        "            NSLKDD_ATTACK_MAP[att] = k\n",
        "        else:\n",
        "            if k == 'Normal':\n",
        "                NSLKDD_ATTACK_MAP[att] = k\n",
        "            else:\n",
        "                NSLKDD_ATTACK_MAP[att] = 'Attack'\n",
        "\n",
        "\n",
        "FEATURE_CATEGORY = dict()\n",
        "\n",
        "for i in range(9):\n",
        "    FEATURE_CATEGORY.setdefault('intrinsic', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(9, 22):\n",
        "    FEATURE_CATEGORY.setdefault('content', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(22, 31):\n",
        "    FEATURE_CATEGORY.setdefault('time_based', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(32, 41):\n",
        "    FEATURE_CATEGORY.setdefault('host_based', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "\n",
        "FEATURE_CATEGORY_REV = dict()\n",
        "\n",
        "for k,v in FEATURE_CATEGORY.items():\n",
        "    for cat in list(v):\n",
        "        FEATURE_CATEGORY_REV[cat] = k\n",
        "\n",
        "\n",
        "FEATURE_TYPE = dict()\n",
        "\n",
        "for i in [2, 3, 4]:\n",
        "    FEATURE_TYPE.setdefault('categorical', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [7, 12, 14, 20, 21, 22]:\n",
        "    FEATURE_TYPE.setdefault('binary', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [8, 9, 15, 43] + list(range(23, 42)):\n",
        "    FEATURE_TYPE.setdefault('discrete', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [1, 5, 6, 10, 11, 13, 16, 17, 18, 19]:\n",
        "    FEATURE_TYPE.setdefault('continuous', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "\n",
        "def create_batch1(x, y, batch_size):\n",
        "    a = list(range(len(x)))\n",
        "    np.random.shuffle(a)\n",
        "    x = x[a]\n",
        "    y = y[a]\n",
        "\n",
        "    batch_x = [x[batch_size * i: (i + 1) * batch_size, :].tolist() for i in range(len(x) // batch_size)]\n",
        "    batch_y = [y[batch_size * i: (i + 1) * batch_size].tolist() for i in range(len(x) // batch_size)]\n",
        "    return batch_x, batch_y\n",
        "\n",
        "\n",
        "def create_batch2(x, batch_size):\n",
        "    a = list(range(len(x)))\n",
        "    np.random.shuffle(a)\n",
        "    x = x[a]\n",
        "    batch_x = [x[batch_size * i: (i + 1) * batch_size, :] for i in range(len(x) // batch_size)]\n",
        "    return np.array(batch_x).astype(float)\n",
        "\n",
        "\n",
        "def preprocess4(train, functional_categories=None):\n",
        "    if functional_categories is None:\n",
        "        functional_categories = ['intrinsic', 'time_based']\n",
        "    train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "\n",
        "    raw_attack = np.array(train[train[\"class\"] == 1])[:, :-1]\n",
        "    normal = np.array(train[train[\"class\"] == 0])[:, :-1]\n",
        "    true_label = train[\"class\"]\n",
        "\n",
        "    del train[\"class\"]\n",
        "\n",
        "    train_columns = list(train.columns)\n",
        "\n",
        "    modification_mask = np.ones((len(train.columns)))\n",
        "    if functional_categories is None:\n",
        "        functional_categories = ['intrinsic', 'time_based']\n",
        "\n",
        "    for cat_ in functional_categories:\n",
        "        cat_cols = list(FEATURE_CATEGORY[cat_])\n",
        "        for cat_col in cat_cols:\n",
        "            for idx in range(len(train_columns)):\n",
        "                if str(train_columns[idx]).startswith(cat_col):\n",
        "                    modification_mask[idx] = 0\n",
        "\n",
        "    return train, raw_attack, normal, true_label, modification_mask\n",
        "\n",
        "\n",
        "# all\n",
        "def preprocess2(train, test, data_generation=False, attack_map=None):\n",
        "    train.drop(['difficulty_level'], axis=1, inplace=True)\n",
        "    test.drop(['difficulty_level'], axis=1, inplace=True)\n",
        "\n",
        "    train.sample(frac=1)\n",
        "\n",
        "    obj_cols = train.select_dtypes(include=['object']).copy().columns\n",
        "    obj_cols = list(obj_cols)\n",
        "\n",
        "    for col in obj_cols:\n",
        "\n",
        "        if col != 'class':\n",
        "            onehot_cols_train = pd.get_dummies(train[col], prefix=col, dtype='float64')\n",
        "            onehot_cols_test = pd.get_dummies(test[col], prefix=col, dtype='float64')\n",
        "\n",
        "            idx = 0\n",
        "            for find_col_idx in range(len(list(train.columns))):\n",
        "                if list(train.columns)[find_col_idx] == col:\n",
        "                    idx = find_col_idx\n",
        "\n",
        "            itr = 0\n",
        "            for new_col in list(onehot_cols_train.columns):\n",
        "                train.insert(idx + itr + 1, new_col, onehot_cols_train[new_col].values, True)\n",
        "\n",
        "                if new_col not in list(onehot_cols_test.columns):\n",
        "                    zero_col = np.zeros(test.values.shape[0])\n",
        "                    test.insert(idx + itr + 1, new_col, zero_col, True)\n",
        "                else:\n",
        "                    test.insert(idx + itr + 1, new_col, onehot_cols_test[new_col].values, True)\n",
        "\n",
        "                itr += 1\n",
        "\n",
        "            del train[col]\n",
        "            del test[col]\n",
        "\n",
        "    if not data_generation:\n",
        "        train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "        test[\"class\"] = test[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "    else:\n",
        "        train[\"class\"] = train[\"class\"].map(attack_map)\n",
        "        test[\"class\"] = test[\"class\"].map(attack_map)\n",
        "\n",
        "    columns = train.columns\n",
        "    train[\"num_outbound_cmds\"] = train[\"num_outbound_cmds\"].map(lambda x: 0)\n",
        "    test[\"num_outbound_cmds\"] = test[\"num_outbound_cmds\"].map(lambda x: 0)\n",
        "\n",
        "    if not data_generation:\n",
        "        trainx, trainy = np.array(train[train.columns[train.columns != \"class\"]]), np.array(train[\"class\"])\n",
        "        testx, testy = np.array(test[train.columns[train.columns != \"class\"]]), np.array(test[\"class\"])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(trainx)\n",
        "\n",
        "        trainx = scaler.transform(trainx)\n",
        "        testx = scaler.transform(testx)\n",
        "\n",
        "        return trainx, trainy, testx, testy\n",
        "    else:\n",
        "        trainx, trainy = train[train.columns[train.columns != \"class\"]], train[\"class\"]\n",
        "        testx, testy = test[train.columns[train.columns != \"class\"]], test[\"class\"]\n",
        "\n",
        "        columns = trainx.columns\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(trainx)\n",
        "\n",
        "        trainx = scaler.transform(trainx)\n",
        "        testx = scaler.transform(testx)\n",
        "\n",
        "        train_processed = pd.DataFrame(trainx, columns=columns)\n",
        "        train_processed[\"class\"] = trainy\n",
        "\n",
        "        test_processed = pd.DataFrame(testx, columns=columns)\n",
        "        test_processed[\"class\"] = testy\n",
        "\n",
        "        return train_processed, test_processed\n",
        "\n",
        "\n",
        "def preprocess3(train, test):\n",
        "    train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "    test[\"class\"] = test[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "\n",
        "    train_np = train.values\n",
        "    test_np = test.values\n",
        "\n",
        "    trainx, trainy = train_np[:, :-1], train_np[:, -1]\n",
        "    testx, testy = test_np[:, :-1], test_np[:, -1]\n",
        "\n",
        "    testy = np.array(testy).astype(int)\n",
        "    trainy = np.array(trainy).astype(int)\n",
        "\n",
        "    return trainx, trainy, testx, testy\n",
        "\n",
        "\n",
        "\n",
        "def preprocess5(train):\n",
        "    #print(train[\"class\"])\n",
        "    #train[\"class\"] = train[\"class\"].map(NSLKDD_ATTACK_MAP)\n",
        "    train[\"class\"] = train[\"class\"].astype('category')\n",
        "    #print(train[\"class\"].cat.categories)\n",
        "    cat_dict_r = dict(enumerate(train[\"class\"].cat.categories))\n",
        "    cat_dict = dict()\n",
        "\n",
        "    #print(\"Cat dict R\")\n",
        "    #print(cat_dict_r)\n",
        "    for key, value in cat_dict_r.items():\n",
        "        cat_dict[value] = key\n",
        "\n",
        "    #print(\"Cat dict\")\n",
        "    #print(cat_dict)\n",
        "\n",
        "    train = train.replace({\"class\": cat_dict})\n",
        "    #print(train[\"class\"])\n",
        "    train[\"class\"] = train[\"class\"].astype('int64')\n",
        "\n",
        "    train_np = train.values\n",
        "\n",
        "    trainx, trainy = train_np[:, :-1], train_np[:, -1]\n",
        "    #print(trainy)\n",
        "    trainy = np.array(trainy).astype(int)\n",
        "\n",
        "    return trainx, trainy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ittZBxIvMiKW"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35uxDYjwJ-mo",
        "outputId": "3ac50475-b5c8-46f8-a0ea-417e2d32e554"
      },
      "source": [
        "from preprocessing import preprocess3\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "from math import log\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train = pd.read_csv('dataset/blackbox_train.csv')\n",
        "test = pd.read_csv('dataset/KDDTest+.csv')\n",
        "\n",
        "trainx, trainy = preprocess5(train)\n",
        "testx, testy = preprocess5(test)\n",
        "#print(np.unique(trainy, return_counts=True))\n",
        "#print(np.unique(testy, return_counts=True))\n",
        "\n",
        "trainx, valx, trainy, valy = train_test_split(trainx, trainy, test_size=percent, random_state=12345)\n",
        "\n",
        "# valx = np.array([])\n",
        "# valy = np.array([1,2,3,0,4])\n",
        "#print(trainy[0], testy[0])\n",
        "\n",
        "trainy = pd.DataFrame(to_categorical(trainy, 5)).values\n",
        "valy = pd.DataFrame(to_categorical(valy, 5)).values\n",
        "testy = pd.DataFrame(to_categorical(testy, 5)).values\n",
        "\n",
        "#print(trainy[0], testy[0])\n",
        "print(trainx.shape, trainy.shape)\n",
        "print(valx.shape, valy.shape)\n",
        "print(testx.shape, testy.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(56686, 122) (56686, 5)\n",
            "(6299, 122) (6299, 5)\n",
            "(22544, 122) (22544, 5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8AeDw9dLmXZ",
        "outputId": "4643196b-dd4b-4405-a488-c4794f9a241c"
      },
      "source": [
        "def train_classifier(X, y):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    #model.add(Dense(40, input_dim=122, kernel_initializer='random_normal',bias_initializer='random_normal', activation='relu'))\n",
        "    model.add(Dense(80, input_dim=122, activation='relu'))\n",
        "    #model.add(Dense(60, activation='relu'))\n",
        "    model.add(Dense(40, activation='relu'))\n",
        "    model.add(Dense(20, activation='relu'))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X, y, epochs=200, batch_size=256)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = train_classifier(trainx, trainy)\n",
        "\n",
        "score = model.evaluate(testx, testy)\n",
        "\n",
        "print(\"Accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "222/222 [==============================] - 4s 2ms/step - loss: 0.5361 - accuracy: 0.8522\n",
            "Epoch 2/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0487 - accuracy: 0.9829\n",
            "Epoch 3/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0327 - accuracy: 0.9908\n",
            "Epoch 4/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0274 - accuracy: 0.9922\n",
            "Epoch 5/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0220 - accuracy: 0.9925\n",
            "Epoch 6/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0221 - accuracy: 0.9928\n",
            "Epoch 7/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0202 - accuracy: 0.9936\n",
            "Epoch 8/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0201 - accuracy: 0.9935\n",
            "Epoch 9/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9947\n",
            "Epoch 10/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0182 - accuracy: 0.9940\n",
            "Epoch 11/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0166 - accuracy: 0.9944\n",
            "Epoch 12/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0160 - accuracy: 0.9946\n",
            "Epoch 13/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0140 - accuracy: 0.9956\n",
            "Epoch 14/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0141 - accuracy: 0.9953\n",
            "Epoch 15/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0156 - accuracy: 0.9948\n",
            "Epoch 16/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0150 - accuracy: 0.9951\n",
            "Epoch 17/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0144 - accuracy: 0.9949\n",
            "Epoch 18/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9953\n",
            "Epoch 19/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0141 - accuracy: 0.9953\n",
            "Epoch 20/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0124 - accuracy: 0.9957\n",
            "Epoch 21/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0123 - accuracy: 0.9955\n",
            "Epoch 22/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0138 - accuracy: 0.9957\n",
            "Epoch 23/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0124 - accuracy: 0.9954\n",
            "Epoch 24/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0117 - accuracy: 0.9957\n",
            "Epoch 25/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0122 - accuracy: 0.9962\n",
            "Epoch 26/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0120 - accuracy: 0.9957\n",
            "Epoch 27/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0121 - accuracy: 0.9959\n",
            "Epoch 28/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0118 - accuracy: 0.9958\n",
            "Epoch 29/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0116 - accuracy: 0.9958\n",
            "Epoch 30/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0102 - accuracy: 0.9962\n",
            "Epoch 31/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0107 - accuracy: 0.9963\n",
            "Epoch 32/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0125 - accuracy: 0.9960\n",
            "Epoch 33/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0136 - accuracy: 0.9955\n",
            "Epoch 34/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9962\n",
            "Epoch 35/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0109 - accuracy: 0.9961\n",
            "Epoch 36/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9962\n",
            "Epoch 37/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0100 - accuracy: 0.9965\n",
            "Epoch 38/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0093 - accuracy: 0.9968\n",
            "Epoch 39/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0106 - accuracy: 0.9960\n",
            "Epoch 40/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0110 - accuracy: 0.9960\n",
            "Epoch 41/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0093 - accuracy: 0.9968\n",
            "Epoch 42/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0097 - accuracy: 0.9964\n",
            "Epoch 43/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0112 - accuracy: 0.9958\n",
            "Epoch 44/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0094 - accuracy: 0.9966\n",
            "Epoch 45/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9966\n",
            "Epoch 46/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0085 - accuracy: 0.9968\n",
            "Epoch 47/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9965\n",
            "Epoch 48/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0085 - accuracy: 0.9968\n",
            "Epoch 49/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0086 - accuracy: 0.9968\n",
            "Epoch 50/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0100 - accuracy: 0.9969\n",
            "Epoch 51/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0086 - accuracy: 0.9968\n",
            "Epoch 52/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9968\n",
            "Epoch 53/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0085 - accuracy: 0.9966\n",
            "Epoch 54/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0083 - accuracy: 0.9970\n",
            "Epoch 55/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0078 - accuracy: 0.9971\n",
            "Epoch 56/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9968\n",
            "Epoch 57/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0087 - accuracy: 0.9970\n",
            "Epoch 58/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0079 - accuracy: 0.9970\n",
            "Epoch 59/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0078 - accuracy: 0.9974\n",
            "Epoch 60/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0075 - accuracy: 0.9974\n",
            "Epoch 61/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0078 - accuracy: 0.9972\n",
            "Epoch 62/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0085 - accuracy: 0.9973\n",
            "Epoch 63/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0077 - accuracy: 0.9972\n",
            "Epoch 64/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0083 - accuracy: 0.9973\n",
            "Epoch 65/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9974\n",
            "Epoch 66/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0070 - accuracy: 0.9975\n",
            "Epoch 67/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0078 - accuracy: 0.9974\n",
            "Epoch 68/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9975\n",
            "Epoch 69/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0068 - accuracy: 0.9976\n",
            "Epoch 70/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0078 - accuracy: 0.9971\n",
            "Epoch 71/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0068 - accuracy: 0.9977\n",
            "Epoch 72/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0070 - accuracy: 0.9973\n",
            "Epoch 73/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0103 - accuracy: 0.9967\n",
            "Epoch 74/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0078 - accuracy: 0.9974\n",
            "Epoch 75/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9976\n",
            "Epoch 76/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9974\n",
            "Epoch 77/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0068 - accuracy: 0.9974\n",
            "Epoch 78/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0063 - accuracy: 0.9977\n",
            "Epoch 79/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0067 - accuracy: 0.9977\n",
            "Epoch 80/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0068 - accuracy: 0.9976\n",
            "Epoch 81/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9976\n",
            "Epoch 82/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9978\n",
            "Epoch 83/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0067 - accuracy: 0.9977\n",
            "Epoch 84/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9979\n",
            "Epoch 85/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0060 - accuracy: 0.9978\n",
            "Epoch 86/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0065 - accuracy: 0.9977\n",
            "Epoch 87/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9980\n",
            "Epoch 88/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9978\n",
            "Epoch 89/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0063 - accuracy: 0.9981\n",
            "Epoch 90/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9979\n",
            "Epoch 91/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9981\n",
            "Epoch 92/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9973\n",
            "Epoch 93/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0092 - accuracy: 0.9969\n",
            "Epoch 94/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9974\n",
            "Epoch 95/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0069 - accuracy: 0.9973\n",
            "Epoch 96/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0058 - accuracy: 0.9979\n",
            "Epoch 97/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0059 - accuracy: 0.9979\n",
            "Epoch 98/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9976\n",
            "Epoch 99/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9980\n",
            "Epoch 100/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9980\n",
            "Epoch 101/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9978\n",
            "Epoch 102/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0060 - accuracy: 0.9977\n",
            "Epoch 103/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9982\n",
            "Epoch 104/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9981\n",
            "Epoch 105/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9980\n",
            "Epoch 106/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9981\n",
            "Epoch 107/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9976\n",
            "Epoch 108/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9978\n",
            "Epoch 109/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9980\n",
            "Epoch 110/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9979\n",
            "Epoch 111/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9980\n",
            "Epoch 112/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9976\n",
            "Epoch 113/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9978\n",
            "Epoch 114/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9982\n",
            "Epoch 115/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9979\n",
            "Epoch 116/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9978\n",
            "Epoch 117/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9981\n",
            "Epoch 118/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9976\n",
            "Epoch 119/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9980\n",
            "Epoch 120/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9978\n",
            "Epoch 121/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9976\n",
            "Epoch 122/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0075 - accuracy: 0.9977\n",
            "Epoch 123/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9979\n",
            "Epoch 124/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9965\n",
            "Epoch 125/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9979\n",
            "Epoch 126/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9978\n",
            "Epoch 127/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0067 - accuracy: 0.9971\n",
            "Epoch 128/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9979\n",
            "Epoch 129/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9973\n",
            "Epoch 130/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9973\n",
            "Epoch 131/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0070 - accuracy: 0.9969\n",
            "Epoch 132/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0063 - accuracy: 0.9975\n",
            "Epoch 133/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9980\n",
            "Epoch 134/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9979\n",
            "Epoch 135/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0067 - accuracy: 0.9975\n",
            "Epoch 136/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9977\n",
            "Epoch 137/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9978\n",
            "Epoch 138/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9978\n",
            "Epoch 139/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9978\n",
            "Epoch 140/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9977\n",
            "Epoch 141/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9980\n",
            "Epoch 142/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9979\n",
            "Epoch 143/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9974\n",
            "Epoch 144/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0063 - accuracy: 0.9982\n",
            "Epoch 145/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9980\n",
            "Epoch 146/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9981\n",
            "Epoch 147/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9977\n",
            "Epoch 148/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0056 - accuracy: 0.9979\n",
            "Epoch 149/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9980\n",
            "Epoch 150/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0078 - accuracy: 0.9972\n",
            "Epoch 151/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0046 - accuracy: 0.9983\n",
            "Epoch 152/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9981\n",
            "Epoch 153/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9979\n",
            "Epoch 154/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9978\n",
            "Epoch 155/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9981\n",
            "Epoch 156/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9981\n",
            "Epoch 157/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9982\n",
            "Epoch 158/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9980\n",
            "Epoch 159/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9977\n",
            "Epoch 160/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9978\n",
            "Epoch 161/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9979\n",
            "Epoch 162/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9980\n",
            "Epoch 163/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9983\n",
            "Epoch 164/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0067 - accuracy: 0.9972\n",
            "Epoch 165/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9983\n",
            "Epoch 166/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9980\n",
            "Epoch 167/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9975\n",
            "Epoch 168/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9980\n",
            "Epoch 169/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9982\n",
            "Epoch 170/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9980\n",
            "Epoch 171/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9977\n",
            "Epoch 172/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9981\n",
            "Epoch 173/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9975\n",
            "Epoch 174/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9981\n",
            "Epoch 175/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9979\n",
            "Epoch 176/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9979\n",
            "Epoch 177/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9978\n",
            "Epoch 178/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9981\n",
            "Epoch 179/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9981\n",
            "Epoch 180/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9981\n",
            "Epoch 181/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9979\n",
            "Epoch 182/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9981\n",
            "Epoch 183/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9978\n",
            "Epoch 184/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9977\n",
            "Epoch 185/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9984\n",
            "Epoch 186/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9982\n",
            "Epoch 187/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9979\n",
            "Epoch 188/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9977\n",
            "Epoch 189/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0053 - accuracy: 0.9981\n",
            "Epoch 190/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0061 - accuracy: 0.9979\n",
            "Epoch 191/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0053 - accuracy: 0.9981\n",
            "Epoch 192/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0042 - accuracy: 0.9984\n",
            "Epoch 193/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9982\n",
            "Epoch 194/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0041 - accuracy: 0.9982\n",
            "Epoch 195/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9981\n",
            "Epoch 196/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0049 - accuracy: 0.9981\n",
            "Epoch 197/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0048 - accuracy: 0.9982\n",
            "Epoch 198/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0050 - accuracy: 0.9982\n",
            "Epoch 199/200\n",
            "222/222 [==============================] - 1s 3ms/step - loss: 0.0053 - accuracy: 0.9983\n",
            "Epoch 200/200\n",
            "222/222 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9979\n",
            "705/705 [==============================] - 1s 2ms/step - loss: 6.4872 - accuracy: 0.7807\n",
            "Accuracy:  0.7806511521339417\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW5heIY9VceT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d084d466-0545-47a2-df0d-56dbe3576a30"
      },
      "source": [
        "membershipVectors = model.predict(valx) # Membership Matriloc\n",
        "binarizedLabels = (membershipVectors == membershipVectors.max(axis=1, keepdims=1)).astype(float)\n",
        "\n",
        "def F(V):\n",
        "    def inner(mu):\n",
        "        try:\n",
        "            result = (mu * log(mu, 2)) + ((1 - mu) * log(1 - mu, 2))\n",
        "        except:\n",
        "            result = 0\n",
        "        return result\n",
        "    return - np.mean(list(map(inner, V)))\n",
        "\n",
        "fuzziness = np.array(list(map(F, membershipVectors)))\n",
        "\n",
        "print(fuzziness[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.6458549620620446e-05\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqttIel6Vvt8"
      },
      "source": [
        "#lowFuzzinessIndices = np.append(np.where( fuzziness >= 9/10), np.where( fuzziness <= 1/10 ) )\n",
        "lowFuzzinessIndices = np.where( fuzziness <= 1/10 )\n",
        "highFuzzinessIndices = np.logical_and(fuzziness >= 2/6, fuzziness <= 2/3)\n",
        "\n",
        "# Fuzziness values >= 5/6\n",
        "lowFuzzinessGroup = valx[ lowFuzzinessIndices ]\n",
        "lowFuzzinessLabels = binarizedLabels[ lowFuzzinessIndices ]\n",
        "\n",
        "# Fuzziness values 2/6 <= x <= 2/3\n",
        "highFuzzinessGroup = valx[ highFuzzinessIndices ]\n",
        "highFuzzinessLabels = binarizedLabels [ highFuzzinessIndices ]\n",
        "\n",
        "# Append new samples to training set\n",
        "#train_set_x = np.concatenate((train_set_x, lowFuzzinessGroup, highFuzzinessGroup), axis=0)\n",
        "\n",
        "#train_set_y = np.concatenate((train_set_y, lowFuzzinessLabels, highFuzzinessLabels), axis=0)\n",
        "trainx = np.concatenate((trainx, lowFuzzinessGroup), axis=0)\n",
        "\n",
        "trainy = np.concatenate((trainy, lowFuzzinessLabels), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCp79Sf_WRVd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "234167ce-31f2-49ff-80f7-bf6f5333f33c"
      },
      "source": [
        "print(trainx.shape)\n",
        "print(trainy.shape)\n",
        "model = train_classifier(trainx, trainy)\n",
        "score = model.evaluate(testx, testy)\n",
        "\n",
        "print(\"Accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(62927, 122)\n",
            "(62927, 5)\n",
            "Epoch 1/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.4645 - accuracy: 0.8694\n",
            "Epoch 2/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0489 - accuracy: 0.9825\n",
            "Epoch 3/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0315 - accuracy: 0.9917\n",
            "Epoch 4/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0243 - accuracy: 0.9925\n",
            "Epoch 5/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0220 - accuracy: 0.9935\n",
            "Epoch 6/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0212 - accuracy: 0.9935\n",
            "Epoch 7/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0173 - accuracy: 0.9949\n",
            "Epoch 8/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0185 - accuracy: 0.9948\n",
            "Epoch 9/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0177 - accuracy: 0.9944\n",
            "Epoch 10/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0165 - accuracy: 0.9947\n",
            "Epoch 11/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0141 - accuracy: 0.9955\n",
            "Epoch 12/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0144 - accuracy: 0.9955\n",
            "Epoch 13/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0180 - accuracy: 0.9946\n",
            "Epoch 14/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0137 - accuracy: 0.9956\n",
            "Epoch 15/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0133 - accuracy: 0.9959\n",
            "Epoch 16/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0149 - accuracy: 0.9950\n",
            "Epoch 17/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0125 - accuracy: 0.9959\n",
            "Epoch 18/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0112 - accuracy: 0.9962\n",
            "Epoch 19/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0118 - accuracy: 0.9963\n",
            "Epoch 20/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0115 - accuracy: 0.9961\n",
            "Epoch 21/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0106 - accuracy: 0.9960\n",
            "Epoch 22/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0104 - accuracy: 0.9965\n",
            "Epoch 23/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0101 - accuracy: 0.9965\n",
            "Epoch 24/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0123 - accuracy: 0.9958\n",
            "Epoch 25/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0098 - accuracy: 0.9967\n",
            "Epoch 26/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0092 - accuracy: 0.9967\n",
            "Epoch 27/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0110 - accuracy: 0.9961\n",
            "Epoch 28/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0085 - accuracy: 0.9968\n",
            "Epoch 29/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0092 - accuracy: 0.9969\n",
            "Epoch 30/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0098 - accuracy: 0.9966\n",
            "Epoch 31/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0088 - accuracy: 0.9971\n",
            "Epoch 32/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0089 - accuracy: 0.9966\n",
            "Epoch 33/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0101 - accuracy: 0.9965\n",
            "Epoch 34/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0126 - accuracy: 0.9967\n",
            "Epoch 35/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0083 - accuracy: 0.9971\n",
            "Epoch 36/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0107 - accuracy: 0.9969\n",
            "Epoch 37/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0082 - accuracy: 0.9972\n",
            "Epoch 38/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0078 - accuracy: 0.9976\n",
            "Epoch 39/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9977\n",
            "Epoch 40/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0076 - accuracy: 0.9975\n",
            "Epoch 41/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0081 - accuracy: 0.9970\n",
            "Epoch 42/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0080 - accuracy: 0.9971\n",
            "Epoch 43/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0074 - accuracy: 0.9974\n",
            "Epoch 44/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0091 - accuracy: 0.9966\n",
            "Epoch 45/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9977\n",
            "Epoch 46/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9976\n",
            "Epoch 47/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9979\n",
            "Epoch 48/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0076 - accuracy: 0.9973\n",
            "Epoch 49/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9978\n",
            "Epoch 50/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9979\n",
            "Epoch 51/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9981\n",
            "Epoch 52/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9975\n",
            "Epoch 53/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0071 - accuracy: 0.9975\n",
            "Epoch 54/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9978\n",
            "Epoch 55/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0061 - accuracy: 0.9979\n",
            "Epoch 56/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9977\n",
            "Epoch 57/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9980\n",
            "Epoch 58/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9978\n",
            "Epoch 59/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0067 - accuracy: 0.9975\n",
            "Epoch 60/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0085 - accuracy: 0.9972\n",
            "Epoch 61/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9975\n",
            "Epoch 62/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9976\n",
            "Epoch 63/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0064 - accuracy: 0.9978\n",
            "Epoch 64/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0067 - accuracy: 0.9977\n",
            "Epoch 65/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9979\n",
            "Epoch 66/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9980\n",
            "Epoch 67/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9979\n",
            "Epoch 68/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9974\n",
            "Epoch 69/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9978\n",
            "Epoch 70/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0070 - accuracy: 0.9977\n",
            "Epoch 71/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0063 - accuracy: 0.9976\n",
            "Epoch 72/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9978\n",
            "Epoch 73/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0072 - accuracy: 0.9974\n",
            "Epoch 74/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9980\n",
            "Epoch 75/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9978\n",
            "Epoch 76/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9979\n",
            "Epoch 77/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0073 - accuracy: 0.9975\n",
            "Epoch 78/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9980\n",
            "Epoch 79/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0056 - accuracy: 0.9978\n",
            "Epoch 80/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9983\n",
            "Epoch 81/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9976\n",
            "Epoch 82/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9977\n",
            "Epoch 83/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9981\n",
            "Epoch 84/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0060 - accuracy: 0.9978\n",
            "Epoch 85/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0063 - accuracy: 0.9978\n",
            "Epoch 86/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9979\n",
            "Epoch 87/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0065 - accuracy: 0.9979\n",
            "Epoch 88/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9981\n",
            "Epoch 89/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9982\n",
            "Epoch 90/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9979\n",
            "Epoch 91/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9983\n",
            "Epoch 92/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9982\n",
            "Epoch 93/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9980\n",
            "Epoch 94/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0090 - accuracy: 0.9973\n",
            "Epoch 95/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9980\n",
            "Epoch 96/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9976\n",
            "Epoch 97/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9984\n",
            "Epoch 98/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0057 - accuracy: 0.9981\n",
            "Epoch 99/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0098 - accuracy: 0.9980\n",
            "Epoch 100/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0052 - accuracy: 0.9980\n",
            "Epoch 101/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9979\n",
            "Epoch 102/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9983\n",
            "Epoch 103/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0058 - accuracy: 0.9977\n",
            "Epoch 104/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9983\n",
            "Epoch 105/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9982\n",
            "Epoch 106/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9983\n",
            "Epoch 107/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9980\n",
            "Epoch 108/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9981\n",
            "Epoch 109/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9981\n",
            "Epoch 110/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0048 - accuracy: 0.9982\n",
            "Epoch 111/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0044 - accuracy: 0.9984\n",
            "Epoch 112/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0044 - accuracy: 0.9984\n",
            "Epoch 113/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9980\n",
            "Epoch 114/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9979\n",
            "Epoch 115/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9982\n",
            "Epoch 116/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0055 - accuracy: 0.9977\n",
            "Epoch 117/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9983\n",
            "Epoch 118/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9983\n",
            "Epoch 119/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9980\n",
            "Epoch 120/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9982\n",
            "Epoch 121/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9981\n",
            "Epoch 122/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0062 - accuracy: 0.9976\n",
            "Epoch 123/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9981\n",
            "Epoch 124/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9984\n",
            "Epoch 125/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9983\n",
            "Epoch 126/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9981\n",
            "Epoch 127/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9982\n",
            "Epoch 128/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9983\n",
            "Epoch 129/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9982\n",
            "Epoch 130/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9976\n",
            "Epoch 131/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9981\n",
            "Epoch 132/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9981\n",
            "Epoch 133/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9983\n",
            "Epoch 134/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0138 - accuracy: 0.9970\n",
            "Epoch 135/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9982\n",
            "Epoch 136/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9979\n",
            "Epoch 137/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9980\n",
            "Epoch 138/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9982\n",
            "Epoch 139/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9983\n",
            "Epoch 140/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9980\n",
            "Epoch 141/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9980\n",
            "Epoch 142/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9981\n",
            "Epoch 143/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9984\n",
            "Epoch 144/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9982\n",
            "Epoch 145/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0047 - accuracy: 0.9984\n",
            "Epoch 146/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9983\n",
            "Epoch 147/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0059 - accuracy: 0.9978\n",
            "Epoch 148/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9981\n",
            "Epoch 149/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9984\n",
            "Epoch 150/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9980\n",
            "Epoch 151/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9979\n",
            "Epoch 152/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9978\n",
            "Epoch 153/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9984\n",
            "Epoch 154/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9981\n",
            "Epoch 155/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9982\n",
            "Epoch 156/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9986\n",
            "Epoch 157/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9981\n",
            "Epoch 158/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9981\n",
            "Epoch 159/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9984\n",
            "Epoch 160/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0061 - accuracy: 0.9980\n",
            "Epoch 161/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0048 - accuracy: 0.9982\n",
            "Epoch 162/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9983\n",
            "Epoch 163/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9977\n",
            "Epoch 164/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0051 - accuracy: 0.9982\n",
            "Epoch 165/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0039 - accuracy: 0.9983\n",
            "Epoch 166/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9984\n",
            "Epoch 167/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0053 - accuracy: 0.9978\n",
            "Epoch 168/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9984\n",
            "Epoch 169/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9981\n",
            "Epoch 170/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9984\n",
            "Epoch 171/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0050 - accuracy: 0.9982\n",
            "Epoch 172/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0049 - accuracy: 0.9984\n",
            "Epoch 173/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0051 - accuracy: 0.9985\n",
            "Epoch 174/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9983\n",
            "Epoch 175/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9983\n",
            "Epoch 176/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9986\n",
            "Epoch 177/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9979\n",
            "Epoch 178/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9985\n",
            "Epoch 179/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0046 - accuracy: 0.9982\n",
            "Epoch 180/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0043 - accuracy: 0.9984\n",
            "Epoch 181/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9983\n",
            "Epoch 182/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0045 - accuracy: 0.9983\n",
            "Epoch 183/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0048 - accuracy: 0.9980\n",
            "Epoch 184/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0054 - accuracy: 0.9979\n",
            "Epoch 185/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0040 - accuracy: 0.9984\n",
            "Epoch 186/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0066 - accuracy: 0.9980\n",
            "Epoch 187/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9983\n",
            "Epoch 188/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0042 - accuracy: 0.9982\n",
            "Epoch 189/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0041 - accuracy: 0.9986\n",
            "Epoch 190/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9981\n",
            "Epoch 191/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0043 - accuracy: 0.9982\n",
            "Epoch 192/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9982\n",
            "Epoch 193/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0044 - accuracy: 0.9981\n",
            "Epoch 194/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9987\n",
            "Epoch 195/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0046 - accuracy: 0.9982\n",
            "Epoch 196/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0052 - accuracy: 0.9980\n",
            "Epoch 197/200\n",
            "246/246 [==============================] - 1s 2ms/step - loss: 0.0038 - accuracy: 0.9985\n",
            "Epoch 198/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0045 - accuracy: 0.9980\n",
            "Epoch 199/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0037 - accuracy: 0.9986\n",
            "Epoch 200/200\n",
            "246/246 [==============================] - 1s 3ms/step - loss: 0.0038 - accuracy: 0.9985\n",
            "705/705 [==============================] - 1s 2ms/step - loss: 6.7768 - accuracy: 0.7613\n",
            "Accuracy:  0.761266827583313\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRYB67ZY3vD"
      },
      "source": [
        "##BlackBox IDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJhC02EoZU2K"
      },
      "source": [
        "def getPred(x):\n",
        "    y = model.predict(x)\n",
        "    y_c = y.argmax(axis=-1)\n",
        "    y_bin = []\n",
        "    for val in y_c:\n",
        "        if val == normal_index:\n",
        "            y_bin.append(0)\n",
        "        else:\n",
        "            y_bin.append(1)\n",
        "    return np.array(y_bin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIt8yZSXY2l9"
      },
      "source": [
        "import os\n",
        "class BlackBoxWrapper():\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = x.cpu().detach().numpy()\n",
        "        x = getPred(x)\n",
        "        x = torch.FloatTensor(x).to(device)\n",
        "        return x\n",
        "\n",
        "    def eval(self):\n",
        "        x = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elwwzKe7eGQM"
      },
      "source": [
        "#Training Our IDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UpEaaZoEenRJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c05a509-f312-40f8-c0f4-ded17c889204"
      },
      "source": [
        "! git clone -b master https://github.com/smmr1405020/NIDS-Project"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'NIDS-Project'...\n",
            "remote: Enumerating objects: 865, done.\u001b[K\n",
            "remote: Counting objects: 100% (610/610), done.\u001b[K\n",
            "remote: Compressing objects: 100% (466/466), done.\u001b[K\n",
            "remote: Total 865 (delta 184), reused 567 (delta 143), pack-reused 255\u001b[K\n",
            "Receiving objects: 100% (865/865), 163.33 MiB | 33.90 MiB/s, done.\n",
            "Resolving deltas: 100% (314/314), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9YtdAIboe76r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "96dfac72-7ae6-44f0-c1d9-d2f4ad6be153"
      },
      "source": [
        "cd NIDS-Project/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/idsgan18/NIDS-Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbdZx_foe9Df",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ec1ecc0-c978-4ec1-dcd6-eeb21f1851d4"
      },
      "source": [
        "!mkdir Output\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_generator.py  Dataset_NSLKDD_2  models  Output  training_file.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2wMY9fefqmU"
      },
      "source": [
        "##Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YLpX32HHkdfl"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pprint\n",
        "from pickle import dump\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "debug = False\n",
        "\n",
        "NSLKDD_ATTACK_DICT = {\n",
        "    'DoS': ['apache2', 'back', 'land', 'neptune', 'mailbomb', 'pod', 'processtable', 'smurf', 'teardrop', 'udpstorm'],\n",
        "    'Probe': ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan'],\n",
        "    'Privilege': ['buffer_overflow', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm','httptunnel'],\n",
        "    'Access': ['ftp_write', 'guess_passwd', 'imap', 'multihop', 'named', 'phf', 'sendmail',\n",
        "               'snmpgetattack', 'snmpguess', 'spy', 'warezclient', 'warezmaster', 'xlock', 'xsnoop','worm'],\n",
        "    'Normal': ['normal']\n",
        "}\n",
        "\n",
        "\n",
        "NSLKDD_COL_NAMES = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
        "                    \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "                    \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n",
        "                    \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "                    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
        "                    \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "                    \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
        "                    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "                    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
        "                    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"class\", \"difficulty_level\"]\n",
        "\n",
        "binary = False\n",
        "\n",
        "NSLKDD_ATTACK_MAP = dict()\n",
        "for k, v in NSLKDD_ATTACK_DICT.items():\n",
        "    for att in v:\n",
        "        if not binary:\n",
        "            NSLKDD_ATTACK_MAP[att] = k\n",
        "        else:\n",
        "            if k == 'Normal':\n",
        "                NSLKDD_ATTACK_MAP[att] = k\n",
        "            else:\n",
        "                NSLKDD_ATTACK_MAP[att] = 'Attack'\n",
        "\n",
        "\n",
        "FEATURE_CATEGORY = dict()\n",
        "\n",
        "for i in range(9):\n",
        "    FEATURE_CATEGORY.setdefault('intrinsic', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(9, 22):\n",
        "    FEATURE_CATEGORY.setdefault('content', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(22, 31):\n",
        "    FEATURE_CATEGORY.setdefault('time_based', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(32, 41):\n",
        "    FEATURE_CATEGORY.setdefault('host_based', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "\n",
        "FEATURE_CATEGORY_REV = dict()\n",
        "\n",
        "for k,v in FEATURE_CATEGORY.items():\n",
        "    for cat in list(v):\n",
        "        FEATURE_CATEGORY_REV[cat] = k\n",
        "\n",
        "\n",
        "FEATURE_TYPE = dict()\n",
        "\n",
        "for i in [2, 3, 4]:\n",
        "    FEATURE_TYPE.setdefault('categorical', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [7, 12, 14, 20, 21, 22]:\n",
        "    FEATURE_TYPE.setdefault('binary', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [8, 9, 15, 43] + list(range(23, 42)):\n",
        "    FEATURE_TYPE.setdefault('discrete', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [1, 5, 6, 10, 11, 13, 16, 17, 18, 19]:\n",
        "    FEATURE_TYPE.setdefault('continuous', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "\n",
        "def create_batch1(x, y, batch_size):\n",
        "    a = list(range(len(x)))\n",
        "    np.random.shuffle(a)\n",
        "    x = x[a]\n",
        "    y = y[a]\n",
        "\n",
        "    batch_x = [x[batch_size * i: (i + 1) * batch_size, :].tolist() for i in range(len(x) // batch_size)]\n",
        "    batch_y = [y[batch_size * i: (i + 1) * batch_size].tolist() for i in range(len(x) // batch_size)]\n",
        "    return batch_x, batch_y\n",
        "\n",
        "\n",
        "def create_batch2(x, batch_size):\n",
        "    a = list(range(len(x)))\n",
        "    np.random.shuffle(a)\n",
        "    x = x[a]\n",
        "    batch_x = [x[batch_size * i: (i + 1) * batch_size, :] for i in range(len(x) // batch_size)]\n",
        "    return np.array(batch_x).astype(float)\n",
        "\n",
        "\n",
        "def preprocess4(train, functional_categories=None):\n",
        "    if functional_categories is None:\n",
        "        functional_categories = ['intrinsic', 'time_based']\n",
        "    train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "\n",
        "    raw_attack = np.array(train[train[\"class\"] == 1])[:, :-1]\n",
        "    normal = np.array(train[train[\"class\"] == 0])[:, :-1]\n",
        "    true_label = train[\"class\"]\n",
        "\n",
        "    del train[\"class\"]\n",
        "\n",
        "    train_columns = list(train.columns)\n",
        "\n",
        "    modification_mask = np.ones((len(train.columns)))\n",
        "    if functional_categories is None:\n",
        "        functional_categories = ['intrinsic', 'time_based']\n",
        "\n",
        "    for cat_ in functional_categories:\n",
        "        cat_cols = list(FEATURE_CATEGORY[cat_])\n",
        "        for cat_col in cat_cols:\n",
        "            for idx in range(len(train_columns)):\n",
        "                if str(train_columns[idx]).startswith(cat_col):\n",
        "                    modification_mask[idx] = 0\n",
        "\n",
        "    return train, raw_attack, normal, true_label, modification_mask\n",
        "\n",
        "\n",
        "# all\n",
        "def preprocess2(train, test, data_generation=False, attack_map=None):\n",
        "    train.drop(['difficulty_level'], axis=1, inplace=True)\n",
        "    test.drop(['difficulty_level'], axis=1, inplace=True)\n",
        "\n",
        "    train.sample(frac=1)\n",
        "\n",
        "    obj_cols = train.select_dtypes(include=['object']).copy().columns\n",
        "    obj_cols = list(obj_cols)\n",
        "\n",
        "    for col in obj_cols:\n",
        "\n",
        "        if col != 'class':\n",
        "            onehot_cols_train = pd.get_dummies(train[col], prefix=col, dtype='float64')\n",
        "            onehot_cols_test = pd.get_dummies(test[col], prefix=col, dtype='float64')\n",
        "\n",
        "            idx = 0\n",
        "            for find_col_idx in range(len(list(train.columns))):\n",
        "                if list(train.columns)[find_col_idx] == col:\n",
        "                    idx = find_col_idx\n",
        "\n",
        "            itr = 0\n",
        "            for new_col in list(onehot_cols_train.columns):\n",
        "                train.insert(idx + itr + 1, new_col, onehot_cols_train[new_col].values, True)\n",
        "\n",
        "                if new_col not in list(onehot_cols_test.columns):\n",
        "                    zero_col = np.zeros(test.values.shape[0])\n",
        "                    test.insert(idx + itr + 1, new_col, zero_col, True)\n",
        "                else:\n",
        "                    test.insert(idx + itr + 1, new_col, onehot_cols_test[new_col].values, True)\n",
        "\n",
        "                itr += 1\n",
        "\n",
        "            del train[col]\n",
        "            del test[col]\n",
        "\n",
        "    if not data_generation:\n",
        "        train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "        test[\"class\"] = test[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "    else:\n",
        "        train[\"class\"] = train[\"class\"].map(attack_map)\n",
        "        test[\"class\"] = test[\"class\"].map(attack_map)\n",
        "\n",
        "    columns = train.columns\n",
        "    train[\"num_outbound_cmds\"] = train[\"num_outbound_cmds\"].map(lambda x: 0)\n",
        "    test[\"num_outbound_cmds\"] = test[\"num_outbound_cmds\"].map(lambda x: 0)\n",
        "\n",
        "    if not data_generation:\n",
        "        trainx, trainy = np.array(train[train.columns[train.columns != \"class\"]]), np.array(train[\"class\"])\n",
        "        testx, testy = np.array(test[train.columns[train.columns != \"class\"]]), np.array(test[\"class\"])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(trainx)\n",
        "\n",
        "        trainx = scaler.transform(trainx)\n",
        "        testx = scaler.transform(testx)\n",
        "\n",
        "        return trainx, trainy, testx, testy\n",
        "    else:\n",
        "        trainx, trainy = train[train.columns[train.columns != \"class\"]], train[\"class\"]\n",
        "        testx, testy = test[train.columns[train.columns != \"class\"]], test[\"class\"]\n",
        "\n",
        "        columns = trainx.columns\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(trainx)\n",
        "\n",
        "        trainx = scaler.transform(trainx)\n",
        "        testx = scaler.transform(testx)\n",
        "\n",
        "        train_processed = pd.DataFrame(trainx, columns=columns)\n",
        "        train_processed[\"class\"] = trainy\n",
        "\n",
        "        test_processed = pd.DataFrame(testx, columns=columns)\n",
        "        test_processed[\"class\"] = testy\n",
        "\n",
        "        return train_processed, test_processed\n",
        "\n",
        "\n",
        "def preprocess3(train, test):\n",
        "    train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "    test[\"class\"] = test[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "\n",
        "    train_np = train.values\n",
        "    test_np = test.values\n",
        "\n",
        "    trainx, trainy = train_np[:, :-1], train_np[:, -1]\n",
        "    testx, testy = test_np[:, :-1], test_np[:, -1]\n",
        "\n",
        "    testy = np.array(testy).astype(int)\n",
        "    trainy = np.array(trainy).astype(int)\n",
        "\n",
        "    return trainx, trainy, testx, testy\n",
        "\n",
        "\n",
        "\n",
        "def preprocess5(train):\n",
        "    #print(train[\"class\"])\n",
        "    #train[\"class\"] = train[\"class\"].map(NSLKDD_ATTACK_MAP)\n",
        "    train[\"class\"] = train[\"class\"].astype('category')\n",
        "    #print(train[\"class\"].cat.categories)\n",
        "    cat_dict_r = dict(enumerate(train[\"class\"].cat.categories))\n",
        "    cat_dict = dict()\n",
        "\n",
        "    #print(\"Cat dict R\")\n",
        "    #print(cat_dict_r)\n",
        "    for key, value in cat_dict_r.items():\n",
        "        cat_dict[value] = key\n",
        "\n",
        "    #print(\"Cat dict\")\n",
        "    #print(cat_dict)\n",
        "\n",
        "    train = train.replace({\"class\": cat_dict})\n",
        "    #print(train[\"class\"])\n",
        "    train[\"class\"] = train[\"class\"].astype('int64')\n",
        "\n",
        "    train_np = train.values\n",
        "\n",
        "    trainx, trainy = train_np[:, :-1], train_np[:, -1]\n",
        "    #print(trainy)\n",
        "    trainy = np.array(trainy).astype(int)\n",
        "\n",
        "    return trainx, trainy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLPtB_Ho7RF4"
      },
      "source": [
        "###Main Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CB76o5a1fvVg"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "import random\n",
        "\n",
        "random.seed(12345)\n",
        "\n",
        "NSLKDD_ATTACK_DICT = {\n",
        "    'DoS': ['apache2', 'back', 'land', 'neptune', 'mailbomb', 'pod', 'processtable', 'smurf', 'teardrop', 'udpstorm'],\n",
        "    'Probe': ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan'],\n",
        "    'Privilege': ['buffer_overflow', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm','httptunnel'],\n",
        "    'Access': ['ftp_write', 'guess_passwd', 'imap', 'multihop', 'named', 'phf', 'sendmail',\n",
        "               'snmpgetattack', 'snmpguess', 'spy', 'warezclient', 'warezmaster', 'xlock', 'xsnoop','worm'],\n",
        "    'Normal': ['normal']\n",
        "}\n",
        "\n",
        "nslkdd_col_names = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
        "                    \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "                    \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n",
        "                    \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "                    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
        "                    \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "                    \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
        "                    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "                    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
        "                    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"label\", \"difficulty_level\"]\n",
        "\n",
        "binary = False\n",
        "\n",
        "NSLKDD_ATTACK_MAP = dict()\n",
        "for k, v in NSLKDD_ATTACK_DICT.items():\n",
        "    for att in v:\n",
        "        if not binary:\n",
        "            NSLKDD_ATTACK_MAP[att] = k\n",
        "        else:\n",
        "            if k == 'Normal':\n",
        "                NSLKDD_ATTACK_MAP[att] = k\n",
        "            else:\n",
        "                NSLKDD_ATTACK_MAP[att] = 'Attack'\n",
        "\n",
        "cat_dict = dict()\n",
        "\n",
        "max_weight = 100\n",
        "\n",
        "\n",
        "def load_nslkdd(train_data=True, test_data_neg=False):\n",
        "    nRowsRead = None  # specify 'None' if want to read whole file\n",
        "\n",
        "    df1 = pd.read_csv('Dataset_NSLKDD_2/KDDTrain+_20Percent.txt', delimiter=',', header=None, names=nslkdd_col_names,\n",
        "                      nrows=nRowsRead)\n",
        "    df1.dataframeName = 'KDDTrain+_20Percent'\n",
        "\n",
        "    df2 = pd.read_csv('Dataset_NSLKDD_2/KDDTest+.txt', delimiter=',', header=None, names=nslkdd_col_names)\n",
        "    df2.dataframeName = 'KDDTest+.txt'\n",
        "\n",
        "    df3 = pd.read_csv('Dataset_NSLKDD_2/KDDTest-21.txt', delimiter=',', header=None, names=nslkdd_col_names)\n",
        "    df3.dataframeName = 'KDDTest-21.txt'\n",
        "\n",
        "    df1.drop(['difficulty_level'], axis=1, inplace=True)\n",
        "    df2.drop(['difficulty_level'], axis=1, inplace=True)\n",
        "    df3.drop(['difficulty_level'], axis=1, inplace=True)\n",
        "\n",
        "    df1.sample(frac=1)\n",
        "\n",
        "    obj_cols = df1.select_dtypes(include=['object']).copy().columns\n",
        "    obj_cols = list(obj_cols)\n",
        "\n",
        "    for col in obj_cols:\n",
        "\n",
        "        if col != 'label':\n",
        "            onehot_cols_train = pd.get_dummies(df1[col], prefix=col, dtype='float64')\n",
        "            onehot_cols_test = pd.get_dummies(df2[col], prefix=col, dtype='float64')\n",
        "            onehot_cols_test2 = pd.get_dummies(df3[col], prefix=col, dtype='float64')\n",
        "\n",
        "            idx = 0\n",
        "            for find_col_idx in range(len(list(df1.columns))):\n",
        "                if list(df1.columns)[find_col_idx] == col:\n",
        "                    idx = find_col_idx\n",
        "\n",
        "            itr = 0\n",
        "            for new_col in list(onehot_cols_train.columns):\n",
        "                df1.insert(idx + itr + 1, new_col, onehot_cols_train[new_col].values, True)\n",
        "\n",
        "                if new_col not in list(onehot_cols_test.columns):\n",
        "                    zero_col = np.zeros(df2.values.shape[0])\n",
        "                    df2.insert(idx + itr + 1, new_col, zero_col, True)\n",
        "                else:\n",
        "                    df2.insert(idx + itr + 1, new_col, onehot_cols_test[new_col].values, True)\n",
        "\n",
        "                if new_col not in list(onehot_cols_test2.columns):\n",
        "                    zero_col = np.zeros(df3.values.shape[0])\n",
        "                    df3.insert(idx + itr + 1, new_col, zero_col, True)\n",
        "                else:\n",
        "                    df3.insert(idx + itr + 1, new_col, onehot_cols_test2[new_col].values, True)\n",
        "\n",
        "                itr += 1\n",
        "\n",
        "            del df1[col]\n",
        "            del df2[col]\n",
        "            del df3[col]\n",
        "\n",
        "        else:\n",
        "\n",
        "            df1[col] = df1[col].map(NSLKDD_ATTACK_MAP)\n",
        "            df2[col] = df2[col].map(NSLKDD_ATTACK_MAP)\n",
        "            df3[col] = df3[col].map(NSLKDD_ATTACK_MAP)\n",
        "\n",
        "            df1[col] = df1[col].astype('category')\n",
        "\n",
        "            cat_dict_r = dict(enumerate(df1[col].cat.categories))\n",
        "\n",
        "            for key, value in cat_dict_r.items():\n",
        "                cat_dict[value] = key\n",
        "\n",
        "            df1 = df1.replace({col: cat_dict})\n",
        "            df1[col] = df1[col].astype('int64')\n",
        "\n",
        "            df2[col] = df2[col].astype('category')\n",
        "            df2 = df2.replace({col: cat_dict})\n",
        "\n",
        "            for i in range(len(df2[col])):\n",
        "                if type(df2[col][i]) is str:\n",
        "                    df2.at[i, col] = len(cat_dict) + 1\n",
        "\n",
        "            df2[col] = df2[col].astype('int64')\n",
        "\n",
        "            df3[col] = df3[col].astype('category')\n",
        "            df3 = df3.replace({col: cat_dict})\n",
        "\n",
        "            for i in range(len(df3[col])):\n",
        "                if type(df3[col][i]) is str:\n",
        "                    df3.at[i, col] = len(cat_dict) + 1\n",
        "\n",
        "            df3[col] = df3[col].astype('int64')\n",
        "\n",
        "    # df1 = df1[df1.labels != cat_dict['Normal']]\n",
        "\n",
        "    normal_label = cat_dict['Normal']\n",
        "\n",
        "    train_X = df1.values[:, :-1]\n",
        "    train_Y = df1.values[:, -1]\n",
        "\n",
        "    test_X = df2.values[:, :-1]\n",
        "    test_Y = df2.values[:, -1]\n",
        "\n",
        "    test_Y = np.array(test_Y).astype(np.int64)\n",
        "\n",
        "    test_X2 = df3.values[:, :-1]\n",
        "    test_Y2 = df3.values[:, -1]\n",
        "\n",
        "    test_Y2 = np.array(test_Y2).astype(np.int64)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_X)\n",
        "\n",
        "    train_X = scaler.transform(train_X)\n",
        "    test_X = scaler.transform(test_X)\n",
        "    test_X2 = scaler.transform(test_X2)\n",
        "\n",
        "    if train_data:\n",
        "        return train_X, train_Y\n",
        "    elif not test_data_neg:\n",
        "        return test_X, test_Y\n",
        "    else:\n",
        "        return test_X2, test_Y2\n",
        "\n",
        "\n",
        "def load_unsw_nb15(train_data=True):\n",
        "    nRowsRead = None  # specify 'None' if want to read whole file\n",
        "\n",
        "    df1 = pd.read_csv('Dataset_UNSW_NB15/UNSW_NB15_train.csv', delimiter=',',\n",
        "                      nrows=nRowsRead)\n",
        "    df1.dataframeName = 'UNSW_NB15_train.csv'\n",
        "\n",
        "    df2 = pd.read_csv('Dataset_UNSW_NB15/UNSW_NB15_test.csv', delimiter=',')\n",
        "    df2.dataframeName = 'UNSW_NB15_test.csv'\n",
        "\n",
        "    # print(df1['attack_cat'].unique())\n",
        "\n",
        "    df1.sample(frac=1)\n",
        "    df1.drop(['id'], axis=1, inplace=True)\n",
        "    df2.drop(['id'], axis=1, inplace=True)\n",
        "\n",
        "    if not binary:\n",
        "        df1.drop(['label'], axis=1, inplace=True)\n",
        "        df2.drop(['label'], axis=1, inplace=True)\n",
        "        lbl = 'attack_cat'\n",
        "    else:\n",
        "        df1.drop(['attack_map'], axis=1, inplace=True)\n",
        "        df2.drop(['attack_map'], axis=1, inplace=True)\n",
        "        lbl = 'label'\n",
        "\n",
        "    obj_cols = df1.select_dtypes(include=['object']).copy().columns\n",
        "    obj_cols = list(obj_cols)\n",
        "\n",
        "    for col in obj_cols:\n",
        "\n",
        "        if col != lbl:\n",
        "            onehot_cols_train = pd.get_dummies(df1[col], prefix=col, dtype='float64')\n",
        "            onehot_cols_test = pd.get_dummies(df2[col], prefix=col, dtype='float64')\n",
        "\n",
        "            idx = 0\n",
        "            for find_col_idx in range(len(list(df1.columns))):\n",
        "                if list(df1.columns)[find_col_idx] == col:\n",
        "                    idx = find_col_idx\n",
        "\n",
        "            itr = 0\n",
        "            for new_col in list(onehot_cols_train.columns):\n",
        "                df1.insert(idx + itr + 1, new_col, onehot_cols_train[new_col].values, True)\n",
        "\n",
        "                if new_col not in list(onehot_cols_test.columns):\n",
        "                    zero_col = np.zeros(df2.values.shape[0])\n",
        "                    df2.insert(idx + itr + 1, new_col, zero_col, True)\n",
        "                else:\n",
        "                    df2.insert(idx + itr + 1, new_col, onehot_cols_test[new_col].values, True)\n",
        "\n",
        "                itr += 1\n",
        "\n",
        "            del df1[col]\n",
        "            del df2[col]\n",
        "\n",
        "        else:\n",
        "\n",
        "            df1[col] = df1[col].astype('category')\n",
        "\n",
        "            cat_dict_r = dict(enumerate(df1[col].cat.categories))\n",
        "\n",
        "            for key, value in cat_dict_r.items():\n",
        "                cat_dict[value] = key\n",
        "\n",
        "            df1 = df1.replace({col: cat_dict})\n",
        "            df1[col] = df1[col].astype('int64')\n",
        "\n",
        "            df2[col] = df2[col].astype('category')\n",
        "            df2 = df2.replace({col: cat_dict})\n",
        "\n",
        "            for i in range(len(df2[col])):\n",
        "                if type(df2[col][i]) is str:\n",
        "                    df2.at[i, col] = len(cat_dict) + 1\n",
        "\n",
        "            df2[col] = df2[col].astype('int64')\n",
        "\n",
        "    train_X = df1.values[:, :-1]\n",
        "    train_Y = df1.values[:, -1]\n",
        "\n",
        "    test_X = df2.values[:, :-1]\n",
        "    test_Y = df2.values[:, -1]\n",
        "\n",
        "    test_Y = np.array(test_Y).astype(np.int64)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit(train_X)\n",
        "\n",
        "    train_X = scaler.transform(train_X)\n",
        "    test_X = scaler.transform(test_X)\n",
        "\n",
        "    # print(train_X.shape)\n",
        "    # print(train_Y.shape)\n",
        "    # print(test_X.shape)\n",
        "    # print(test_Y.shape)\n",
        "\n",
        "    if train_data:\n",
        "        return train_X, train_Y\n",
        "    else:\n",
        "        return test_X, test_Y\n",
        "\n",
        "load_nslkdd()\n",
        "\n",
        "def get_training_data(label_ratio):\n",
        "    train = pd.read_csv('../dataset/blackbox_train.csv')\n",
        "    print(train.shape)\n",
        "    train_X, train_Y = preprocess5(train)\n",
        "\n",
        "    trYunique, trYcounts = np.unique(train_Y, return_counts=True)\n",
        "    got_once = np.zeros(len(trYunique))\n",
        "\n",
        "    global max_weight\n",
        "    max_weight = np.max(trYcounts) / np.min(trYcounts)\n",
        "\n",
        "    for i in range(len(train_Y)):\n",
        "        p = np.random.rand()\n",
        "        if p > label_ratio and got_once[int(train_Y[i])] == 1:\n",
        "            train_Y[i] = -1\n",
        "        else:\n",
        "            got_once[int(train_Y[i])] = 1\n",
        "\n",
        "    labeled_data_X = []\n",
        "    labeled_data_Y = []\n",
        "    unlabeled_data_X = []\n",
        "    unlabeled_data_Y = []\n",
        "    for i in range(len(train_Y)):\n",
        "        if train_Y[i] != -1:\n",
        "            labeled_data_X.append(list(train_X[i]))\n",
        "            labeled_data_Y.append(train_Y[i])\n",
        "        else:\n",
        "            unlabeled_data_X.append(list(train_X[i]))\n",
        "            unlabeled_data_Y.append(train_Y[i])\n",
        "\n",
        "    labeled_data = np.array(labeled_data_X), np.array(labeled_data_Y)\n",
        "    unlabeled_data = np.array(unlabeled_data_X), np.array(unlabeled_data_Y)\n",
        "\n",
        "    if len(unlabeled_data[1]) != 0 and len(labeled_data[1]) != 0:\n",
        "        total_data_X = np.append(labeled_data[0], unlabeled_data[0], axis=0)\n",
        "        total_data_Y = np.append(labeled_data[1], unlabeled_data[1], axis=0)\n",
        "    elif len(unlabeled_data[1]) == 0:\n",
        "        total_data_X = labeled_data[0]\n",
        "        total_data_Y = labeled_data[1]\n",
        "    else:\n",
        "        total_data_X = unlabeled_data[0]\n",
        "        total_data_Y = unlabeled_data[1]\n",
        "\n",
        "    total_data = total_data_X, total_data_Y\n",
        "\n",
        "    total_dataset = dataset_train(total_data)\n",
        "    labeled_dataset = dataset_train(labeled_data)\n",
        "    unlabeled_dataset = dataset_train(unlabeled_data)\n",
        "\n",
        "    return total_dataset, labeled_dataset, unlabeled_dataset\n",
        "\n",
        "\n",
        "class dataset_train(Dataset):\n",
        "\n",
        "    def __init__(self, data):\n",
        "        self.x = data[0]\n",
        "        self.y = data[1]\n",
        "\n",
        "    def set_x(self, new_x):\n",
        "        self.x = new_x\n",
        "\n",
        "    def get_x(self):\n",
        "        return self.x\n",
        "\n",
        "    def set_y(self, new_y):\n",
        "        self.y = new_y\n",
        "\n",
        "    def get_y(self):\n",
        "        return self.y\n",
        "\n",
        "    def get_feature_shape(self):\n",
        "        return self.x.shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if debug:\n",
        "            print(\"Inside get item....\")\n",
        "            print(self.y[idx])\n",
        "            print(np.array(self.x).shape , np.array(self.y).shape)\n",
        "            print(torch.from_numpy(np.array(self.x[idx])).shape)\n",
        "            print(torch.LongTensor(np.array(self.y))[idx])\n",
        "            print(torch.LongTensor([idx]).squeeze())\n",
        "        return torch.from_numpy(np.array(self.x[idx])), torch.LongTensor(np.array(self.y))[idx], \\\n",
        "               torch.LongTensor([idx]).squeeze()\n",
        "\n",
        "    def get_weight(self):\n",
        "\n",
        "        trYunique, trYcounts = np.unique(self.y, return_counts=True)\n",
        "        max_count = 0\n",
        "        for i in range(len(trYcounts)):\n",
        "            if trYunique[i] != -1 and trYcounts[i] > max_count:\n",
        "                max_count = trYcounts[i]\n",
        "\n",
        "        labels = list(cat_dict.values())\n",
        "        no_labels = len(labels)\n",
        "        weights = np.ones(no_labels)\n",
        "        for i in range(len(trYunique)):\n",
        "            if trYunique[i] >= 0 and trYcounts[i] > 0:\n",
        "                weights[int(trYunique[i])] = min(max_weight, max_count / trYcounts[i])\n",
        "\n",
        "        return weights\n",
        "\n",
        "    def add_sample(self, sample_X, sample_Y):\n",
        "        self.x = np.concatenate([self.x, np.expand_dims(sample_X, axis=0)], axis=0)\n",
        "        self.y = np.append(self.y, sample_Y)\n",
        "\n",
        "\n",
        "class dataset_test(Dataset):\n",
        "\n",
        "    def __init__(self, test_neg=False):\n",
        "        test = pd.read_csv('../dataset/KDDTest+.csv')\n",
        "        self.x, self.y = preprocess5(test)\n",
        "        self.feature_size = self.x.shape[1]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "    def set_x(self, new_x):\n",
        "        self.x = new_x\n",
        "\n",
        "    def get_x(self):\n",
        "        return self.x\n",
        "\n",
        "    def get_y(self):\n",
        "        return self.y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.from_numpy(np.array(self.x[idx])), torch.LongTensor(np.array(self.y))[idx], \\\n",
        "               torch.LongTensor([idx]).squeeze()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3kr0k91Qni75"
      },
      "source": [
        "##Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3x4rIrhnUto",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e74e60f2-0db5-4a66-cc3a-402e713f7379"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn import Linear\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import os, glob\n",
        "import pickle\n",
        "from google.colab import files\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "import random\n",
        "\n",
        "# 0.01: trainstop 0.005, cluster /25, min_imp_dec: 0.01, (80,100,150) , 5:15PM 26/11/20\n",
        "\n",
        "\n",
        "random.seed(12345)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "total_dataset, labeled_dataset, unlabeled_dataset = get_training_data(label_ratio=0.1)\n",
        "test_dataset = dataset_test()\n",
        "print(total_dataset.get_x().shape)\n",
        "\n",
        "ae_epoch = 80\n",
        "pretrain_epoch = 200\n",
        "train_epoch = 150\n",
        "\n",
        "num_data = total_dataset.get_x()\n",
        "labels = total_dataset.get_y()\n",
        "\n",
        "if debug:\n",
        "    print(\"Inside train code: \",labels)\n",
        "\n",
        "total_original_label_counts = dict()\n",
        "distinct_labels, distinct_label_counts = np.unique(labels, return_counts=True)\n",
        "for i in range(len(distinct_labels)):\n",
        "    if distinct_labels[i] != -1:\n",
        "        total_original_label_counts[distinct_labels[i]] = distinct_label_counts[i]\n",
        "\n",
        "\n",
        "# print(total_original_label_counts)\n",
        "\n",
        "\n",
        "def tree_work(load_cluster_from_file=False):\n",
        "    if load_cluster_from_file:\n",
        "        clustering = pickle.load(file=open('models/clustering.pkl', 'rb'))\n",
        "    else:\n",
        "        clustering = KMeans(n_clusters=int(total_dataset.__len__() / 175), random_state=0)\n",
        "        print(\"Clustering Started.\")\n",
        "        clustering.fit(num_data)\n",
        "        print(\"Clustering ended.\")\n",
        "\n",
        "    cluster_assignment = clustering.labels_\n",
        "    all_clusters = dict()\n",
        "    for j in range(len(cluster_assignment)):\n",
        "        all_clusters.setdefault(cluster_assignment[j], []).append(num_data[j])\n",
        "\n",
        "    cluster_to_label_dict = dict()\n",
        "    for j in range(len(cluster_assignment)):\n",
        "        if labels[j] != -1:\n",
        "            cluster_to_label_dict.setdefault(cluster_assignment[j], []).append(labels[j])\n",
        "\n",
        "    #print(\"Clusters:\")\n",
        "    label_to_cluster_dict = dict()\n",
        "    for k, v in cluster_to_label_dict.items():\n",
        "        cl_labels, cl_label_counts = np.unique(np.array(v), return_counts=True)  # labeled member count in each cluster\n",
        "        # print(k)\n",
        "        # print(cl_labels)\n",
        "        # print(cl_label_counts)\n",
        "        # print(\"\\n\")\n",
        "        total_labeled_counts = np.sum(cl_label_counts)\n",
        "\n",
        "        max_label = np.argmax(cl_label_counts)\n",
        "\n",
        "        '''\n",
        "        If a cluster contains more than 10% of all samples of a label present in the training dataset, it is considered\n",
        "        important for this label.\n",
        "        If the majority label is not the normal label and there are no other labels for which this cluster is important,\n",
        "        having more than 50% of the labeled members would be enough for soft labeling\n",
        "        If the majority label is the normal label, then all labeled members must be normal for soft labeling.\n",
        "        In any other case, we do not soft label.\n",
        "        '''\n",
        "\n",
        "        imp_for_label = []\n",
        "        for label, total_label_count in total_original_label_counts.items():\n",
        "            for j in range(len(cl_labels)):\n",
        "                if cl_labels[j] == label and cl_label_counts[j] > 0.1 * total_label_count:\n",
        "                    imp_for_label.append(label)\n",
        "\n",
        "        if (cl_label_counts[max_label] / total_labeled_counts) > 0.5:\n",
        "            selected_label = cl_labels[max_label]\n",
        "            if len(imp_for_label) == 1:\n",
        "                if imp_for_label[0] == selected_label:\n",
        "                    if selected_label != int(cat_dict['Normal']):\n",
        "                        label = selected_label\n",
        "                        size = len(v)\n",
        "                        label_to_cluster_dict.setdefault(label, []).append([k, size])\n",
        "                    else:\n",
        "                        if len(cl_labels) == 1:\n",
        "                            label = selected_label\n",
        "                            size = len(v)\n",
        "                            label_to_cluster_dict.setdefault(label, []).append([k, size])\n",
        "            elif len(imp_for_label) == 0:\n",
        "                if selected_label != int(cat_dict['Normal']):\n",
        "                    label = selected_label\n",
        "                    size = len(v)\n",
        "                    label_to_cluster_dict.setdefault(label, []).append([k, size])\n",
        "                else:\n",
        "                    if len(cl_labels) == 1:\n",
        "                        label = selected_label\n",
        "                        size = len(v)\n",
        "                        label_to_cluster_dict.setdefault(label, []).append([k, size])\n",
        "\n",
        "    '''\n",
        "    clusters that belong to a particular label, after soft labeling.\n",
        "    '''\n",
        "    soft_label_mapping = dict()\n",
        "    for k, v in label_to_cluster_dict.items():\n",
        "        for cluster_index in v:\n",
        "            soft_label_mapping[cluster_index[0]] = k\n",
        "\n",
        "    '''\n",
        "    soft labeling particular unlabeled samples.\n",
        "    Also add this to labeled dataset.\n",
        "    '''\n",
        "\n",
        "    for j in range(len(labels)):\n",
        "        if labels[j] == -1 and (int(cluster_assignment[j]) in soft_label_mapping.keys()):\n",
        "            labels[j] = soft_label_mapping[cluster_assignment[j]]\n",
        "            labeled_dataset.add_sample(num_data[j], labels[j])\n",
        "\n",
        "\n",
        "    '''\n",
        "    checking total labeled and soft labeled members.\n",
        "    '''\n",
        "\n",
        "    total_soft_label_counts = dict()\n",
        "    distinct_slabels, distinct_slabel_counts = np.unique(labels, return_counts=True)\n",
        "    for j in range(len(distinct_slabels)):\n",
        "        if distinct_slabels[j] != -1:\n",
        "            total_soft_label_counts[distinct_slabels[j]] = distinct_slabel_counts[j]\n",
        "\n",
        "    print(total_soft_label_counts)\n",
        "\n",
        "    cluster_to_labels_dict = dict()\n",
        "    for k, v in cluster_to_label_dict.items():\n",
        "        cl_labels = np.unique(np.array(v))\n",
        "        cl_labels = sorted(list(cl_labels))\n",
        "        if cl_labels[0] == -1:\n",
        "            cl_labels = cl_labels[1:]\n",
        "        cluster_to_labels_dict[k] = cl_labels\n",
        "\n",
        "    total_dataset.set_y(labels)\n",
        "\n",
        "    dt_X = labeled_dataset.get_x()\n",
        "    dt_Y = labeled_dataset.get_y()\n",
        "\n",
        "    print(\"labeled members dimensions:\")\n",
        "    print(dt_X.shape)\n",
        "    print(dt_Y.shape)\n",
        "\n",
        "    clf = DecisionTreeClassifier(random_state=0, max_leaf_nodes=7)\n",
        "    clf.fit(dt_X, dt_Y)\n",
        "\n",
        "    print(\"No. of leaves of decision tree:\")\n",
        "    print(clf.get_n_leaves())\n",
        "\n",
        "    '''\n",
        "    saving decision tree, cluster membership info (this is just to skip the clustering step for our faster use) and soft\n",
        "    labeling info.\n",
        "    '''\n",
        "\n",
        "    file = open('models/tree.pkl', 'wb')\n",
        "    pickle.dump(clf, file)\n",
        "    file.close()\n",
        "\n",
        "    for k in list(all_clusters.keys()):\n",
        "        if int(k) not in soft_label_mapping.keys():\n",
        "            soft_label_mapping[int(k)] = int(cat_dict['Normal'])\n",
        "\n",
        "    file = open('models/soft_label_mapping.pkl', 'wb')\n",
        "    pickle.dump(soft_label_mapping, file)\n",
        "    file.close()\n",
        "\n",
        "    if not load_cluster_from_file:\n",
        "        file = open('models/clustering.pkl', 'wb')\n",
        "        pickle.dump(clustering, file)\n",
        "        file.close()\n",
        "\n",
        "    leaf_dataset_X = dict()\n",
        "    leaf_dataset_Y = dict()\n",
        "\n",
        "    '''\n",
        "    finding out corresponding leaf for each training sample.\n",
        "    '''\n",
        "\n",
        "    for j in range(len(num_data)):\n",
        "        leaf = clf.apply([num_data[j]])[0]\n",
        "        if labels[j] != -1:\n",
        "            leaf_dataset_X.setdefault(leaf, []).append(num_data[j])\n",
        "            leaf_dataset_Y.setdefault(leaf, []).append(labels[j])\n",
        "\n",
        "    for k, v in leaf_dataset_X.items():\n",
        "        leaf_dataset_X[k] = np.array(leaf_dataset_X[k])\n",
        "        leaf_dataset_Y[k] = np.array(leaf_dataset_Y[k])\n",
        "\n",
        "    return leaf_dataset_X, leaf_dataset_Y\n",
        "\n",
        "'''\n",
        "The dictionary of X-Y training dataset for each individual leaf.\n",
        "'''\n",
        "leaf_dataset_X, leaf_dataset_Y = tree_work(load_cluster_from_file=False)\n",
        "\n",
        "'''\n",
        "undercomplete autoencoder for embedding.\n",
        "'''\n",
        "\n",
        "class AE(nn.Module):\n",
        "\n",
        "    def __init__(self, n_input, n_z):\n",
        "        super(AE, self).__init__()\n",
        "        # encoder\n",
        "        self.enc_1 = Linear(n_input, 80)\n",
        "        self.enc_2 = Linear(80, 50)\n",
        "        self.z_layer = Linear(50, n_z)\n",
        "\n",
        "        # decoder\n",
        "        self.dec_1 = Linear(n_z, 50)\n",
        "        self.dec_2 = Linear(50, 80)\n",
        "        self.x_bar_layer = Linear(80, n_input)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # encoder\n",
        "        enc_h1 = F.relu(self.enc_1(x))\n",
        "        enc_h2 = F.relu(self.enc_2(enc_h1))\n",
        "        z = self.z_layer(enc_h2)\n",
        "\n",
        "        # decoder\n",
        "        dec_h1 = F.relu(self.dec_1(z))\n",
        "        dec_h2 = F.relu(self.dec_2(dec_h1))\n",
        "        x_bar = self.x_bar_layer(dec_h2)\n",
        "\n",
        "        return x_bar, z\n",
        "\n",
        "\n",
        "def train_ae(epochs, load_from_file=False, save_path='models/train_ae'):\n",
        "    '''\n",
        "    train autoencoder\n",
        "    '''\n",
        "\n",
        "    model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    model.to(device)\n",
        "\n",
        "    ae_train_ds = total_dataset\n",
        "    training_data_length = int(0.7 * ae_train_ds.__len__())\n",
        "    validation_data_length = ae_train_ds.__len__() - training_data_length\n",
        "    training_data, validation_data = torch.utils.data.random_split(ae_train_ds,\n",
        "                                                                   [training_data_length, validation_data_length])\n",
        "\n",
        "    # print(total_dataset.__len__())\n",
        "    # print(training_data.__len__())\n",
        "    # print(validation_data.__len__())\n",
        "\n",
        "    train_loader = DataLoader(training_data, batch_size=32, shuffle=True)\n",
        "    validation_loader = DataLoader(validation_data, batch_size=32, shuffle=True)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    min_val_loss = 1000000\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        training_loss = 0.\n",
        "        validation_loss = 0.\n",
        "        train_batch_num = 0\n",
        "        val_batch_num = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (x, _, idx) in enumerate(train_loader):\n",
        "            x = x.float()\n",
        "            x = x.to(device)\n",
        "\n",
        "            train_batch_num = batch_idx\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            x_bar, z = model(x)\n",
        "            loss = F.mse_loss(x_bar, x)\n",
        "            training_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        training_loss /= (train_batch_num + 1)\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        for batch_idx, (x, _, idx) in enumerate(validation_loader):\n",
        "            x = x.float()\n",
        "            x = x.to(device)\n",
        "\n",
        "            val_batch_num = batch_idx\n",
        "\n",
        "            x_bar, z = model(x)\n",
        "            loss = F.mse_loss(x_bar, x)\n",
        "            validation_loss += loss.item()\n",
        "\n",
        "        validation_loss /= (val_batch_num + 1)\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            print(\n",
        "                \"epoch {} , Training loss={:.4f}, Validation loss={:.4f}\".format(epoch, training_loss, validation_loss))\n",
        "\n",
        "        if epoch == 0 or min_val_loss > validation_loss:\n",
        "            min_val_loss = validation_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    print(\"model saved to {}.\".format(save_path))\n",
        "    return model\n",
        "\n",
        "\n",
        "class leaf_dnn(nn.Module):\n",
        "\n",
        "    def __init__(self, n_input, n_output):\n",
        "        super(leaf_dnn, self).__init__()\n",
        "        self.fc1 = nn.Linear(n_input, 8)\n",
        "        self.fc2 = nn.Linear(8, n_output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out_1 = torch.relu(self.fc1(x))\n",
        "        out_2 = self.fc2(out_1)\n",
        "\n",
        "        return out_2\n",
        "\n",
        "'''\n",
        "pretraining using labeled and soft labeled dataset.\n",
        "'''\n",
        "\n",
        "def pretrain_leaf_dnn(save_path, epochs):\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "\n",
        "    model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "    model.to(device)\n",
        "\n",
        "    weights = torch.FloatTensor(labeled_dataset.get_weight()).to(device)\n",
        "\n",
        "    train_loader = DataLoader(labeled_dataset, batch_size=32, shuffle=True)  # soft label must be assigned\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "    min_train_loss = 1000000\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        train_batch_num = 0\n",
        "        train_num_correct = 0\n",
        "        train_num_examples = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (x, y_t, idx) in enumerate(train_loader):\n",
        "            x = x.float()\n",
        "            x = x.to(device)\n",
        "            train_batch_num = batch_idx\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x_emb = ae_model(x)[1]\n",
        "            y_pred = model(x_emb)\n",
        "\n",
        "            y_t = y_t.clone().detach().to(device)\n",
        "\n",
        "            loss = torch.nn.CrossEntropyLoss(weight=weights)(y_pred, y_t)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            correct = torch.eq(torch.max(torch.softmax(y_pred, dim=-1), dim=1)[1], y_t).view(-1)\n",
        "            train_num_correct += torch.sum(correct).item()\n",
        "            train_num_examples += correct.shape[0]\n",
        "\n",
        "        train_loss /= (train_batch_num + 1)\n",
        "        train_acc = train_num_correct / train_num_examples\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            print(\"epoch {}; T loss={:.4f} T Accuracy={:.4f}\".\n",
        "                  format(epoch, train_loss, train_num_correct / train_num_examples))\n",
        "\n",
        "        if epoch == 0 or min_train_loss > train_loss:\n",
        "            min_train_loss = train_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "    print(\"model saved to {}.\".format(save_path))\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "training dataset of an individual leaf.\n",
        "'''\n",
        "\n",
        "def train_leaf_dnn(model, dataset, save_path, epochs):\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "\n",
        "    weights = torch.FloatTensor(dataset.get_weight()).to(device)\n",
        "\n",
        "    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)  # soft label must be assigned\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
        "    min_train_loss = 1000000\n",
        "    prev_train_acc = 0\n",
        "    stop_flag = 1\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        train_loss = 0.0\n",
        "        train_batch_num = 0\n",
        "        train_num_correct = 0\n",
        "        train_num_examples = 0\n",
        "\n",
        "        model.train()\n",
        "        for batch_idx, (x, y_t, idx) in enumerate(train_loader):\n",
        "            x = x.float()\n",
        "            x = x.to(device)\n",
        "            train_batch_num = batch_idx\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            x_emb = ae_model(x)[1]\n",
        "            y_pred = model(x_emb)\n",
        "\n",
        "            y_t = y_t.clone().detach().to(device)\n",
        "\n",
        "            loss = torch.nn.CrossEntropyLoss(weight=weights)(y_pred, y_t)\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            correct = torch.eq(torch.max(torch.softmax(y_pred, dim=-1), dim=1)[1], y_t).view(-1)\n",
        "            train_num_correct += torch.sum(correct).item()\n",
        "            train_num_examples += correct.shape[0]\n",
        "\n",
        "        train_loss /= (train_batch_num + 1)\n",
        "        train_acc = train_num_correct / train_num_examples\n",
        "\n",
        "        if epoch % 1 == 0:\n",
        "            # print(\"epoch {}; T loss={:.4f} T Accuracy={:.4f}\".\n",
        "            #       format(epoch, train_loss, train_num_correct / train_num_examples))\n",
        "\n",
        "            if train_acc - prev_train_acc > 0.005:\n",
        "                    stop_flag = 0\n",
        "\n",
        "\n",
        "        if epoch == 0 or min_train_loss > train_loss:\n",
        "            min_train_loss = train_loss\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "\n",
        "        if train_acc == 1.0:\n",
        "            break\n",
        "\n",
        "    #print(\"model saved to {}.\".format(save_path))\n",
        "\n",
        "    return model\n",
        "\n",
        "'''\n",
        "training all leaves.\n",
        "'''\n",
        "\n",
        "def create_leaf_dnns(remove_priors=True,epochs=150):\n",
        "    if remove_priors:\n",
        "        #print(\"Removing...\")\n",
        "        filelist = glob.glob(os.path.join('models/leaf_models', \"*\"))\n",
        "        for f in filelist:\n",
        "            os.remove(f)\n",
        "\n",
        "    for key in leaf_dataset_Y.keys():\n",
        "        dataset_X = leaf_dataset_X[key]\n",
        "        dataset_Y = leaf_dataset_Y[key]\n",
        "\n",
        "        # print(key)\n",
        "        # print(dataset_X.shape)\n",
        "        # print(dataset_Y.shape)\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        data = dataset_X, dataset_Y\n",
        "        dataset = dataset_train(data)\n",
        "\n",
        "        save_path = \"models/leaf_models/leaf_\" + str(key)\n",
        "\n",
        "        model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "        model.load_state_dict(torch.load('models/pretrain_leaf_dnn'))\n",
        "        # if os.path.exists(save_path):\n",
        "        #     model.load_state_dict(torch.load(save_path))\n",
        "        # else:\n",
        "        #     print(\"Loading Pre Trained...\")\n",
        "        #     model.load_state_dict(torch.load('models/pretrain_leaf_dnn'))\n",
        "        model.to(device)\n",
        "\n",
        "\n",
        "        train_leaf_dnn(model, dataset, save_path, epochs)\n",
        "\n",
        "\n",
        "#create_leaf_dnns()\n",
        "\n",
        "\n",
        "def generate_result():\n",
        "    clf = pickle.load(file=open('models/tree.pkl', 'rb'))\n",
        "    soft_label_mapping = pickle.load(file=open('models/soft_label_mapping.pkl', 'rb'))\n",
        "    clustering = pickle.load(file=open('models/clustering.pkl', 'rb'))\n",
        "\n",
        "    test_X = test_dataset.get_x()\n",
        "    test_Y = test_dataset.get_y()\n",
        "\n",
        "    leaf_nodes = clf.apply(test_X)\n",
        "    cluster_assignment = clustering.predict(test_X)\n",
        "\n",
        "    print(\"done\")\n",
        "\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "    '''\n",
        "    If model does not exist for a particular leaf, use the default pretrained model.\n",
        "    '''\n",
        "\n",
        "    model_dict = dict()\n",
        "    leaf_model_files = os.listdir('models/leaf_models')\n",
        "    for file in leaf_model_files:\n",
        "        spl = str(file).split(\"_\")\n",
        "\n",
        "        leaf_model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "\n",
        "        if not os.path.exists('models/leaf_models/leaf_' + spl[1]):\n",
        "            leaf_model.load_state_dict(torch.load('models/pretrain_leaf_dnn'))\n",
        "        else:\n",
        "            leaf_model.load_state_dict(torch.load('models/leaf_models/leaf_' + spl[1]))\n",
        "\n",
        "        leaf_model.to(device)\n",
        "\n",
        "        model_dict[int(spl[1])] = leaf_model\n",
        "\n",
        "    leaf_pred_dict = dict()\n",
        "    for k, v in model_dict.items():\n",
        "        leaf_model = v\n",
        "        X_emb = ae_model(torch.FloatTensor(test_X).to(device))[1]\n",
        "        Y = torch.softmax(leaf_model(X_emb), dim=-1)\n",
        "        Y_ = Y.cpu().detach().numpy()\n",
        "        leaf_pred_dict[k] = Y_\n",
        "\n",
        "    test_Y_pred = np.zeros(test_Y.shape)\n",
        "\n",
        "    for j in range(len(leaf_nodes)):\n",
        "        y_ = leaf_pred_dict[leaf_nodes[j]][j]\n",
        "        if soft_label_mapping[cluster_assignment[j]] != cat_dict['Normal']:\n",
        "            y_[int(cat_dict['Normal'])] = 0\n",
        "        y_pred = np.argmax(y_)\n",
        "        test_Y_pred[j] = y_pred\n",
        "\n",
        "    print(confusion_matrix(test_Y, test_Y_pred))\n",
        "    print(classification_report(test_Y, test_Y_pred))\n",
        "\n",
        "    test_Y_df = pd.DataFrame(test_Y, columns=['True Label'])\n",
        "    pred_Y_df = pd.DataFrame(test_Y_pred, columns=['Pred Label'])\n",
        "    result = pd.concat([test_Y_df,pred_Y_df],axis=1, sort=False)\n",
        "    #result.to_csv('Output/result-frac-10'+'.csv', index=False)\n",
        "    #files.download('Output/result-frac-10'+'.csv')\n",
        "\n",
        "\n",
        "def generate_test_result():\n",
        "    clf = pickle.load(file=open('models/tree.pkl', 'rb'))\n",
        "    soft_label_mapping = pickle.load(file=open('models/soft_label_mapping.pkl', 'rb'))\n",
        "    clustering = pickle.load(file=open('models/clustering.pkl', 'rb'))\n",
        "\n",
        "    test_X = test_dataset.get_x()\n",
        "    test_Y = test_dataset.get_y()\n",
        "\n",
        "    leaf_nodes = clf.apply(test_X)\n",
        "    cluster_assignment = clustering.predict(test_X)\n",
        "\n",
        "    print(\"done\")\n",
        "\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "    '''\n",
        "    If model does not exist for a particular leaf, use the default pretrained model.\n",
        "    '''\n",
        "\n",
        "    model_dict = dict()\n",
        "    leaf_model_files = os.listdir('models/leaf_models')\n",
        "    for file in leaf_model_files:\n",
        "        spl = str(file).split(\"_\")\n",
        "\n",
        "        leaf_model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "\n",
        "        if not os.path.exists('models/leaf_models/leaf_' + spl[1]):\n",
        "            leaf_model.load_state_dict(torch.load('models/pretrain_leaf_dnn'))\n",
        "        else:\n",
        "            leaf_model.load_state_dict(torch.load('models/leaf_models/leaf_' + spl[1]))\n",
        "\n",
        "        leaf_model.to(device)\n",
        "\n",
        "        model_dict[int(spl[1])] = leaf_model\n",
        "\n",
        "    leaf_pred_dict = dict()\n",
        "    for k, v in model_dict.items():\n",
        "        leaf_model = v\n",
        "        X_emb = ae_model(torch.FloatTensor(test_X).to(device))[1]\n",
        "        Y = torch.softmax(leaf_model(X_emb), dim=-1)\n",
        "        Y_ = Y.cpu().detach().numpy()\n",
        "        leaf_pred_dict[k] = Y_\n",
        "\n",
        "    test_Y_pred = np.zeros(test_Y.shape)\n",
        "\n",
        "    for j in range(len(leaf_nodes)):\n",
        "        y_ = leaf_pred_dict[leaf_nodes[j]][j]\n",
        "        if soft_label_mapping[cluster_assignment[j]] != cat_dict['Normal']:\n",
        "            y_[int(cat_dict['Normal'])] = 0\n",
        "        y_pred = np.argmax(y_)\n",
        "        test_Y_pred[j] = y_pred\n",
        "\n",
        "    return accuracy_score(test_Y, test_Y_pred)\n",
        "\n",
        "\n",
        "def generate_train_result():\n",
        "    clf = pickle.load(file=open('models/tree.pkl', 'rb'))\n",
        "    soft_label_mapping = pickle.load(file=open('models/soft_label_mapping.pkl', 'rb'))\n",
        "    clustering = pickle.load(file=open('models/clustering.pkl', 'rb'))\n",
        "\n",
        "    dataset, _ , _ = get_training_data(label_ratio=1.0)\n",
        "\n",
        "    test_X = dataset.get_x()\n",
        "    test_Y = dataset.get_y()\n",
        "\n",
        "    leaf_nodes = clf.apply(test_X)\n",
        "    cluster_assignment = clustering.predict(test_X)\n",
        "\n",
        "    print(\"done\")\n",
        "\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "    '''\n",
        "    If model does not exist for a particular leaf, use the default pretrained model.\n",
        "    '''\n",
        "\n",
        "    model_dict = dict()\n",
        "    leaf_model_files = os.listdir('models/leaf_models')\n",
        "    for file in leaf_model_files:\n",
        "        spl = str(file).split(\"_\")\n",
        "\n",
        "        leaf_model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "\n",
        "        if not os.path.exists('models/leaf_models/leaf_' + spl[1]):\n",
        "            leaf_model.load_state_dict(torch.load('models/pretrain_leaf_dnn'))\n",
        "        else:\n",
        "            leaf_model.load_state_dict(torch.load('models/leaf_models/leaf_' + spl[1]))\n",
        "\n",
        "        leaf_model.to(device)\n",
        "\n",
        "        model_dict[int(spl[1])] = leaf_model\n",
        "\n",
        "    leaf_pred_dict = dict()\n",
        "    for k, v in model_dict.items():\n",
        "        leaf_model = v\n",
        "        X_emb = ae_model(torch.FloatTensor(test_X).to(device))[1]\n",
        "        Y = torch.softmax(leaf_model(X_emb), dim=-1)\n",
        "        Y_ = Y.cpu().detach().numpy()\n",
        "        leaf_pred_dict[k] = Y_\n",
        "\n",
        "    test_Y_pred = np.zeros(test_Y.shape)\n",
        "\n",
        "    for j in range(len(leaf_nodes)):\n",
        "        y_ = leaf_pred_dict[leaf_nodes[j]][j]\n",
        "        if soft_label_mapping[cluster_assignment[j]] != cat_dict['Normal']:\n",
        "            y_[int(cat_dict['Normal'])] = 0\n",
        "        y_pred = np.argmax(y_)\n",
        "        test_Y_pred[j] = y_pred\n",
        "\n",
        "    return accuracy_score(test_Y, test_Y_pred)\n",
        "\n",
        "\n",
        "train_ae(ae_epoch, False)\n",
        "pretrain_leaf_dnn('models/pretrain_leaf_dnn', pretrain_epoch)\n",
        "create_leaf_dnns()\n",
        "generate_result()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(62985, 123)\n",
            "(62985, 122)\n",
            "Clustering Started.\n",
            "Clustering ended.\n",
            "{0: 22986, 1: 33139, 2: 4757, 3: 519, 4: 6}\n",
            "labeled members dimensions:\n",
            "(61407, 122)\n",
            "(61407,)\n",
            "No. of leaves of decision tree:\n",
            "7\n",
            "epoch 0 , Training loss=0.5082, Validation loss=0.4203\n",
            "epoch 1 , Training loss=0.2547, Validation loss=0.3323\n",
            "epoch 2 , Training loss=0.1924, Validation loss=0.2827\n",
            "epoch 3 , Training loss=0.1703, Validation loss=0.2838\n",
            "epoch 4 , Training loss=0.1567, Validation loss=0.2746\n",
            "epoch 5 , Training loss=0.1503, Validation loss=0.2951\n",
            "epoch 6 , Training loss=0.1490, Validation loss=0.2631\n",
            "epoch 7 , Training loss=0.1413, Validation loss=0.2578\n",
            "epoch 8 , Training loss=0.1373, Validation loss=0.2630\n",
            "epoch 9 , Training loss=0.1355, Validation loss=0.2477\n",
            "epoch 10 , Training loss=0.1286, Validation loss=0.2480\n",
            "epoch 11 , Training loss=0.1229, Validation loss=0.2599\n",
            "epoch 12 , Training loss=0.1262, Validation loss=0.2503\n",
            "epoch 13 , Training loss=0.1186, Validation loss=0.2538\n",
            "epoch 14 , Training loss=0.1206, Validation loss=0.2462\n",
            "epoch 15 , Training loss=0.1154, Validation loss=0.2496\n",
            "epoch 16 , Training loss=0.1157, Validation loss=0.2664\n",
            "epoch 17 , Training loss=0.1145, Validation loss=0.2534\n",
            "epoch 18 , Training loss=0.1141, Validation loss=0.2702\n",
            "epoch 19 , Training loss=0.1129, Validation loss=0.2608\n",
            "epoch 20 , Training loss=0.1107, Validation loss=0.2710\n",
            "epoch 21 , Training loss=0.1133, Validation loss=0.2804\n",
            "epoch 22 , Training loss=0.1078, Validation loss=0.2851\n",
            "epoch 23 , Training loss=0.1121, Validation loss=0.2669\n",
            "epoch 24 , Training loss=0.1087, Validation loss=0.2780\n",
            "epoch 25 , Training loss=0.1119, Validation loss=0.2781\n",
            "epoch 26 , Training loss=0.1077, Validation loss=0.2849\n",
            "epoch 27 , Training loss=0.1104, Validation loss=0.2874\n",
            "epoch 28 , Training loss=0.1082, Validation loss=0.2873\n",
            "epoch 29 , Training loss=0.1067, Validation loss=0.3016\n",
            "epoch 30 , Training loss=0.1083, Validation loss=0.2980\n",
            "epoch 31 , Training loss=0.1066, Validation loss=0.2824\n",
            "epoch 32 , Training loss=0.1036, Validation loss=0.2851\n",
            "epoch 33 , Training loss=0.1072, Validation loss=0.2900\n",
            "epoch 34 , Training loss=0.1065, Validation loss=0.3000\n",
            "epoch 35 , Training loss=0.1042, Validation loss=0.2967\n",
            "epoch 36 , Training loss=0.1050, Validation loss=0.3042\n",
            "epoch 37 , Training loss=0.1032, Validation loss=0.2933\n",
            "epoch 38 , Training loss=0.1035, Validation loss=0.3000\n",
            "epoch 39 , Training loss=0.1052, Validation loss=0.3107\n",
            "epoch 40 , Training loss=0.1032, Validation loss=0.3197\n",
            "epoch 41 , Training loss=0.1037, Validation loss=0.3138\n",
            "epoch 42 , Training loss=0.1027, Validation loss=0.3254\n",
            "epoch 43 , Training loss=0.1059, Validation loss=0.3127\n",
            "epoch 44 , Training loss=0.1025, Validation loss=0.3016\n",
            "epoch 45 , Training loss=0.1044, Validation loss=0.3100\n",
            "epoch 46 , Training loss=0.1037, Validation loss=0.3430\n",
            "epoch 47 , Training loss=0.1007, Validation loss=0.3382\n",
            "epoch 48 , Training loss=0.1027, Validation loss=0.3381\n",
            "epoch 49 , Training loss=0.1037, Validation loss=0.3398\n",
            "epoch 50 , Training loss=0.1016, Validation loss=0.3340\n",
            "epoch 51 , Training loss=0.1048, Validation loss=0.3455\n",
            "epoch 52 , Training loss=0.1018, Validation loss=0.3446\n",
            "epoch 53 , Training loss=0.1019, Validation loss=0.3488\n",
            "epoch 54 , Training loss=0.1009, Validation loss=0.3512\n",
            "epoch 55 , Training loss=0.1038, Validation loss=0.3649\n",
            "epoch 56 , Training loss=0.1014, Validation loss=0.3667\n",
            "epoch 57 , Training loss=0.1041, Validation loss=0.3783\n",
            "epoch 58 , Training loss=0.0976, Validation loss=0.3845\n",
            "epoch 59 , Training loss=0.1007, Validation loss=0.3616\n",
            "epoch 60 , Training loss=0.0992, Validation loss=0.4011\n",
            "epoch 61 , Training loss=0.0986, Validation loss=0.3759\n",
            "epoch 62 , Training loss=0.0981, Validation loss=0.4030\n",
            "epoch 63 , Training loss=0.0950, Validation loss=0.4330\n",
            "epoch 64 , Training loss=0.0971, Validation loss=0.4257\n",
            "epoch 65 , Training loss=0.0966, Validation loss=0.4737\n",
            "epoch 66 , Training loss=0.0967, Validation loss=0.4710\n",
            "epoch 67 , Training loss=0.0943, Validation loss=0.4925\n",
            "epoch 68 , Training loss=0.0899, Validation loss=0.4737\n",
            "epoch 69 , Training loss=0.0977, Validation loss=0.5141\n",
            "epoch 70 , Training loss=0.0906, Validation loss=0.4959\n",
            "epoch 71 , Training loss=0.0964, Validation loss=0.5081\n",
            "epoch 72 , Training loss=0.0944, Validation loss=0.5136\n",
            "epoch 73 , Training loss=0.0890, Validation loss=0.5005\n",
            "epoch 74 , Training loss=0.0909, Validation loss=0.5618\n",
            "epoch 75 , Training loss=0.0964, Validation loss=0.5725\n",
            "epoch 76 , Training loss=0.0897, Validation loss=0.5470\n",
            "epoch 77 , Training loss=0.0960, Validation loss=0.4934\n",
            "epoch 78 , Training loss=0.0885, Validation loss=0.4680\n",
            "epoch 79 , Training loss=0.0953, Validation loss=0.5453\n",
            "model saved to models/train_ae.\n",
            "epoch 0; T loss=0.4651 T Accuracy=0.8193\n",
            "epoch 1; T loss=0.2133 T Accuracy=0.9356\n",
            "epoch 2; T loss=0.1617 T Accuracy=0.9607\n",
            "epoch 3; T loss=0.1385 T Accuracy=0.9710\n",
            "epoch 4; T loss=0.1249 T Accuracy=0.9739\n",
            "epoch 5; T loss=0.1120 T Accuracy=0.9762\n",
            "epoch 6; T loss=0.1051 T Accuracy=0.9784\n",
            "epoch 7; T loss=0.1008 T Accuracy=0.9790\n",
            "epoch 8; T loss=0.0969 T Accuracy=0.9794\n",
            "epoch 9; T loss=0.0937 T Accuracy=0.9797\n",
            "epoch 10; T loss=0.0925 T Accuracy=0.9796\n",
            "epoch 11; T loss=0.0871 T Accuracy=0.9801\n",
            "epoch 12; T loss=0.0868 T Accuracy=0.9808\n",
            "epoch 13; T loss=0.0811 T Accuracy=0.9812\n",
            "epoch 14; T loss=0.0827 T Accuracy=0.9811\n",
            "epoch 15; T loss=0.0783 T Accuracy=0.9812\n",
            "epoch 16; T loss=0.0785 T Accuracy=0.9811\n",
            "epoch 17; T loss=0.0805 T Accuracy=0.9814\n",
            "epoch 18; T loss=0.0731 T Accuracy=0.9817\n",
            "epoch 19; T loss=0.0747 T Accuracy=0.9825\n",
            "epoch 20; T loss=0.0710 T Accuracy=0.9824\n",
            "epoch 21; T loss=0.0722 T Accuracy=0.9827\n",
            "epoch 22; T loss=0.0694 T Accuracy=0.9829\n",
            "epoch 23; T loss=0.0680 T Accuracy=0.9831\n",
            "epoch 24; T loss=0.0698 T Accuracy=0.9828\n",
            "epoch 25; T loss=0.0683 T Accuracy=0.9839\n",
            "epoch 26; T loss=0.0670 T Accuracy=0.9838\n",
            "epoch 27; T loss=0.0653 T Accuracy=0.9844\n",
            "epoch 28; T loss=0.0678 T Accuracy=0.9841\n",
            "epoch 29; T loss=0.0671 T Accuracy=0.9836\n",
            "epoch 30; T loss=0.0609 T Accuracy=0.9845\n",
            "epoch 31; T loss=0.0650 T Accuracy=0.9839\n",
            "epoch 32; T loss=0.0623 T Accuracy=0.9844\n",
            "epoch 33; T loss=0.0647 T Accuracy=0.9840\n",
            "epoch 34; T loss=0.0630 T Accuracy=0.9844\n",
            "epoch 35; T loss=0.0636 T Accuracy=0.9849\n",
            "epoch 36; T loss=0.0597 T Accuracy=0.9846\n",
            "epoch 37; T loss=0.0606 T Accuracy=0.9848\n",
            "epoch 38; T loss=0.0580 T Accuracy=0.9850\n",
            "epoch 39; T loss=0.0610 T Accuracy=0.9846\n",
            "epoch 40; T loss=0.0572 T Accuracy=0.9848\n",
            "epoch 41; T loss=0.0568 T Accuracy=0.9846\n",
            "epoch 42; T loss=0.0601 T Accuracy=0.9847\n",
            "epoch 43; T loss=0.0571 T Accuracy=0.9851\n",
            "epoch 44; T loss=0.0549 T Accuracy=0.9857\n",
            "epoch 45; T loss=0.0566 T Accuracy=0.9850\n",
            "epoch 46; T loss=0.0563 T Accuracy=0.9851\n",
            "epoch 47; T loss=0.0574 T Accuracy=0.9847\n",
            "epoch 48; T loss=0.0506 T Accuracy=0.9857\n",
            "epoch 49; T loss=0.0568 T Accuracy=0.9851\n",
            "epoch 50; T loss=0.0536 T Accuracy=0.9858\n",
            "epoch 51; T loss=0.0538 T Accuracy=0.9854\n",
            "epoch 52; T loss=0.0535 T Accuracy=0.9858\n",
            "epoch 53; T loss=0.0489 T Accuracy=0.9860\n",
            "epoch 54; T loss=0.0503 T Accuracy=0.9857\n",
            "epoch 55; T loss=0.0512 T Accuracy=0.9849\n",
            "epoch 56; T loss=0.0519 T Accuracy=0.9858\n",
            "epoch 57; T loss=0.0524 T Accuracy=0.9864\n",
            "epoch 58; T loss=0.0487 T Accuracy=0.9858\n",
            "epoch 59; T loss=0.0532 T Accuracy=0.9860\n",
            "epoch 60; T loss=0.0486 T Accuracy=0.9861\n",
            "epoch 61; T loss=0.0499 T Accuracy=0.9864\n",
            "epoch 62; T loss=0.0523 T Accuracy=0.9859\n",
            "epoch 63; T loss=0.0488 T Accuracy=0.9860\n",
            "epoch 64; T loss=0.0496 T Accuracy=0.9864\n",
            "epoch 65; T loss=0.0456 T Accuracy=0.9867\n",
            "epoch 66; T loss=0.0481 T Accuracy=0.9869\n",
            "epoch 67; T loss=0.0493 T Accuracy=0.9868\n",
            "epoch 68; T loss=0.0482 T Accuracy=0.9872\n",
            "epoch 69; T loss=0.0467 T Accuracy=0.9867\n",
            "epoch 70; T loss=0.0457 T Accuracy=0.9873\n",
            "epoch 71; T loss=0.0456 T Accuracy=0.9872\n",
            "epoch 72; T loss=0.0483 T Accuracy=0.9870\n",
            "epoch 73; T loss=0.0458 T Accuracy=0.9873\n",
            "epoch 74; T loss=0.0472 T Accuracy=0.9879\n",
            "epoch 75; T loss=0.0450 T Accuracy=0.9875\n",
            "epoch 76; T loss=0.0474 T Accuracy=0.9874\n",
            "epoch 77; T loss=0.0455 T Accuracy=0.9883\n",
            "epoch 78; T loss=0.0454 T Accuracy=0.9879\n",
            "epoch 79; T loss=0.0468 T Accuracy=0.9878\n",
            "epoch 80; T loss=0.0488 T Accuracy=0.9879\n",
            "epoch 81; T loss=0.0451 T Accuracy=0.9885\n",
            "epoch 82; T loss=0.0442 T Accuracy=0.9888\n",
            "epoch 83; T loss=0.0475 T Accuracy=0.9885\n",
            "epoch 84; T loss=0.0457 T Accuracy=0.9884\n",
            "epoch 85; T loss=0.0430 T Accuracy=0.9886\n",
            "epoch 86; T loss=0.0433 T Accuracy=0.9886\n",
            "epoch 87; T loss=0.0454 T Accuracy=0.9882\n",
            "epoch 88; T loss=0.0429 T Accuracy=0.9889\n",
            "epoch 89; T loss=0.0413 T Accuracy=0.9891\n",
            "epoch 90; T loss=0.0429 T Accuracy=0.9889\n",
            "epoch 91; T loss=0.0470 T Accuracy=0.9885\n",
            "epoch 92; T loss=0.0418 T Accuracy=0.9892\n",
            "epoch 93; T loss=0.0464 T Accuracy=0.9886\n",
            "epoch 94; T loss=0.0422 T Accuracy=0.9894\n",
            "epoch 95; T loss=0.0432 T Accuracy=0.9893\n",
            "epoch 96; T loss=0.0433 T Accuracy=0.9893\n",
            "epoch 97; T loss=0.0415 T Accuracy=0.9895\n",
            "epoch 98; T loss=0.0419 T Accuracy=0.9897\n",
            "epoch 99; T loss=0.0433 T Accuracy=0.9890\n",
            "epoch 100; T loss=0.0408 T Accuracy=0.9895\n",
            "epoch 101; T loss=0.0443 T Accuracy=0.9889\n",
            "epoch 102; T loss=0.0425 T Accuracy=0.9896\n",
            "epoch 103; T loss=0.0415 T Accuracy=0.9899\n",
            "epoch 104; T loss=0.0428 T Accuracy=0.9893\n",
            "epoch 105; T loss=0.0417 T Accuracy=0.9892\n",
            "epoch 106; T loss=0.0414 T Accuracy=0.9892\n",
            "epoch 107; T loss=0.0406 T Accuracy=0.9899\n",
            "epoch 108; T loss=0.0461 T Accuracy=0.9889\n",
            "epoch 109; T loss=0.0425 T Accuracy=0.9895\n",
            "epoch 110; T loss=0.0410 T Accuracy=0.9898\n",
            "epoch 111; T loss=0.0414 T Accuracy=0.9899\n",
            "epoch 112; T loss=0.0395 T Accuracy=0.9898\n",
            "epoch 113; T loss=0.0435 T Accuracy=0.9899\n",
            "epoch 114; T loss=0.0402 T Accuracy=0.9895\n",
            "epoch 115; T loss=0.0390 T Accuracy=0.9903\n",
            "epoch 116; T loss=0.0410 T Accuracy=0.9902\n",
            "epoch 117; T loss=0.0410 T Accuracy=0.9896\n",
            "epoch 118; T loss=0.0394 T Accuracy=0.9905\n",
            "epoch 119; T loss=0.0404 T Accuracy=0.9899\n",
            "epoch 120; T loss=0.0423 T Accuracy=0.9899\n",
            "epoch 121; T loss=0.0410 T Accuracy=0.9900\n",
            "epoch 122; T loss=0.0398 T Accuracy=0.9903\n",
            "epoch 123; T loss=0.0390 T Accuracy=0.9904\n",
            "epoch 124; T loss=0.0365 T Accuracy=0.9905\n",
            "epoch 125; T loss=0.0382 T Accuracy=0.9910\n",
            "epoch 126; T loss=0.0397 T Accuracy=0.9906\n",
            "epoch 127; T loss=0.0387 T Accuracy=0.9911\n",
            "epoch 128; T loss=0.0385 T Accuracy=0.9912\n",
            "epoch 129; T loss=0.0390 T Accuracy=0.9906\n",
            "epoch 130; T loss=0.0382 T Accuracy=0.9905\n",
            "epoch 131; T loss=0.0397 T Accuracy=0.9906\n",
            "epoch 132; T loss=0.0370 T Accuracy=0.9909\n",
            "epoch 133; T loss=0.0402 T Accuracy=0.9900\n",
            "epoch 134; T loss=0.0423 T Accuracy=0.9911\n",
            "epoch 135; T loss=0.0389 T Accuracy=0.9906\n",
            "epoch 136; T loss=0.0376 T Accuracy=0.9909\n",
            "epoch 137; T loss=0.0375 T Accuracy=0.9908\n",
            "epoch 138; T loss=0.0369 T Accuracy=0.9909\n",
            "epoch 139; T loss=0.0407 T Accuracy=0.9904\n",
            "epoch 140; T loss=0.0349 T Accuracy=0.9915\n",
            "epoch 141; T loss=0.0404 T Accuracy=0.9906\n",
            "epoch 142; T loss=0.0362 T Accuracy=0.9916\n",
            "epoch 143; T loss=0.0381 T Accuracy=0.9902\n",
            "epoch 144; T loss=0.0354 T Accuracy=0.9914\n",
            "epoch 145; T loss=0.0383 T Accuracy=0.9900\n",
            "epoch 146; T loss=0.0377 T Accuracy=0.9909\n",
            "epoch 147; T loss=0.0349 T Accuracy=0.9913\n",
            "epoch 148; T loss=0.0376 T Accuracy=0.9910\n",
            "epoch 149; T loss=0.0349 T Accuracy=0.9915\n",
            "epoch 150; T loss=0.0372 T Accuracy=0.9908\n",
            "epoch 151; T loss=0.0381 T Accuracy=0.9907\n",
            "epoch 152; T loss=0.0362 T Accuracy=0.9914\n",
            "epoch 153; T loss=0.0378 T Accuracy=0.9913\n",
            "epoch 154; T loss=0.0389 T Accuracy=0.9908\n",
            "epoch 155; T loss=0.0358 T Accuracy=0.9911\n",
            "epoch 156; T loss=0.0360 T Accuracy=0.9911\n",
            "epoch 157; T loss=0.0371 T Accuracy=0.9909\n",
            "epoch 158; T loss=0.0361 T Accuracy=0.9911\n",
            "epoch 159; T loss=0.0369 T Accuracy=0.9910\n",
            "epoch 160; T loss=0.0390 T Accuracy=0.9910\n",
            "epoch 161; T loss=0.0366 T Accuracy=0.9912\n",
            "epoch 162; T loss=0.0368 T Accuracy=0.9911\n",
            "epoch 163; T loss=0.0371 T Accuracy=0.9910\n",
            "epoch 164; T loss=0.0351 T Accuracy=0.9913\n",
            "epoch 165; T loss=0.0397 T Accuracy=0.9908\n",
            "epoch 166; T loss=0.0356 T Accuracy=0.9913\n",
            "epoch 167; T loss=0.0356 T Accuracy=0.9921\n",
            "epoch 168; T loss=0.0365 T Accuracy=0.9910\n",
            "epoch 169; T loss=0.0355 T Accuracy=0.9912\n",
            "epoch 170; T loss=0.0369 T Accuracy=0.9910\n",
            "epoch 171; T loss=0.0366 T Accuracy=0.9911\n",
            "epoch 172; T loss=0.0361 T Accuracy=0.9914\n",
            "epoch 173; T loss=0.0321 T Accuracy=0.9920\n",
            "epoch 174; T loss=0.0378 T Accuracy=0.9906\n",
            "epoch 175; T loss=0.0310 T Accuracy=0.9918\n",
            "epoch 176; T loss=0.0361 T Accuracy=0.9909\n",
            "epoch 177; T loss=0.0356 T Accuracy=0.9914\n",
            "epoch 178; T loss=0.0359 T Accuracy=0.9915\n",
            "epoch 179; T loss=0.0353 T Accuracy=0.9920\n",
            "epoch 180; T loss=0.0362 T Accuracy=0.9912\n",
            "epoch 181; T loss=0.0365 T Accuracy=0.9909\n",
            "epoch 182; T loss=0.0344 T Accuracy=0.9915\n",
            "epoch 183; T loss=0.0340 T Accuracy=0.9918\n",
            "epoch 184; T loss=0.0358 T Accuracy=0.9916\n",
            "epoch 185; T loss=0.0326 T Accuracy=0.9918\n",
            "epoch 186; T loss=0.0370 T Accuracy=0.9907\n",
            "epoch 187; T loss=0.0360 T Accuracy=0.9914\n",
            "epoch 188; T loss=0.0358 T Accuracy=0.9913\n",
            "epoch 189; T loss=0.0339 T Accuracy=0.9918\n",
            "epoch 190; T loss=0.0332 T Accuracy=0.9913\n",
            "epoch 191; T loss=0.0334 T Accuracy=0.9918\n",
            "epoch 192; T loss=0.0320 T Accuracy=0.9917\n",
            "epoch 193; T loss=0.0353 T Accuracy=0.9913\n",
            "epoch 194; T loss=0.0378 T Accuracy=0.9909\n",
            "epoch 195; T loss=0.0327 T Accuracy=0.9920\n",
            "epoch 196; T loss=0.0341 T Accuracy=0.9914\n",
            "epoch 197; T loss=0.0343 T Accuracy=0.9917\n",
            "epoch 198; T loss=0.0375 T Accuracy=0.9917\n",
            "epoch 199; T loss=0.0321 T Accuracy=0.9916\n",
            "model saved to models/pretrain_leaf_dnn.\n",
            "done\n",
            "[[6178 1270   10    0    0]\n",
            " [  77 9353  272    8    1]\n",
            " [ 587  585 1249    0    0]\n",
            " [  12 2412    2  327    1]\n",
            " [  12  169    1    6   12]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.83      0.86      7458\n",
            "           1       0.68      0.96      0.80      9711\n",
            "           2       0.81      0.52      0.63      2421\n",
            "           3       0.96      0.12      0.21      2754\n",
            "           4       0.86      0.06      0.11       200\n",
            "\n",
            "    accuracy                           0.76     22544\n",
            "   macro avg       0.84      0.50      0.52     22544\n",
            "weighted avg       0.80      0.76      0.72     22544\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tko3Md4OJQAC"
      },
      "source": [
        "##BlackBox Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQiEsuIPX8NP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aba305e-8abc-4cbd-ac15-27242e631aa0"
      },
      "source": [
        "cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/idsgan18\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He22G2jaI3cj"
      },
      "source": [
        "def black_box_test(x):\n",
        "    clf = pickle.load(file=open('./NIDS-Project/models/tree.pkl', 'rb'))\n",
        "    soft_label_mapping = pickle.load(file=open('./NIDS-Project/models/soft_label_mapping.pkl', 'rb'))\n",
        "    clustering = pickle.load(file=open('./NIDS-Project/models/clustering.pkl', 'rb'))\n",
        "\n",
        "    leaf_nodes = clf.apply(x)\n",
        "    cluster_assignment = clustering.predict(x)\n",
        "\n",
        "    #print(\"done\")\n",
        "\n",
        "    ae_model = AE(total_dataset.get_feature_shape(), 32)\n",
        "    ae_model.load_state_dict(torch.load('./NIDS-Project/models/train_ae'))\n",
        "    ae_model.to(device)\n",
        "    '''\n",
        "    If model does not exist for a particular leaf, use the default pretrained model.\n",
        "    '''\n",
        "\n",
        "    model_dict = dict()\n",
        "    leaf_model_files = os.listdir('./NIDS-Project/models/leaf_models')\n",
        "    for file in leaf_model_files:\n",
        "        spl = str(file).split(\"_\")\n",
        "\n",
        "        leaf_model = leaf_dnn(32, int(max(labels)) + 1)\n",
        "\n",
        "        if not os.path.exists('./NIDS-Project/models/leaf_models/leaf_' + spl[1]):\n",
        "            leaf_model.load_state_dict(torch.load('./NIDS-Project/models/pretrain_leaf_dnn'))\n",
        "        else:\n",
        "            leaf_model.load_state_dict(torch.load('./NIDS-Project/models/leaf_models/leaf_' + spl[1]))\n",
        "\n",
        "        leaf_model.to(device)\n",
        "\n",
        "        model_dict[int(spl[1])] = leaf_model\n",
        "\n",
        "    leaf_pred_dict = dict()\n",
        "    for k, v in model_dict.items():\n",
        "        leaf_model = v\n",
        "        X_emb = ae_model(torch.FloatTensor(x).to(device))[1]\n",
        "        Y = torch.softmax(leaf_model(X_emb), dim=-1)\n",
        "        Y_ = Y.cpu().detach().numpy()\n",
        "        leaf_pred_dict[k] = Y_\n",
        "\n",
        "    test_Y_pred = np.zeros(x.shape[0])\n",
        "\n",
        "    for j in range(len(leaf_nodes)):\n",
        "        y_ = leaf_pred_dict[leaf_nodes[j]][j]\n",
        "        if soft_label_mapping[cluster_assignment[j]] != cat_dict['Normal']:\n",
        "            y_[int(cat_dict['Normal'])] = 0\n",
        "        y_pred = np.argmax(y_)\n",
        "        test_Y_pred[j] = y_pred\n",
        "    return test_Y_pred\n",
        "\n",
        "def getBinPrediction(x):\n",
        "    pred_Y = black_box_test(x)\n",
        "    bin_pred_y = []\n",
        "    for y in pred_Y:\n",
        "        if y == (cat_dict['Normal']-1):\n",
        "            bin_pred_y.append(0)\n",
        "        else:\n",
        "            bin_pred_y.append(1)\n",
        "    return np.array(bin_pred_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56B1DwdoKfpT"
      },
      "source": [
        "import os\n",
        "class BlackBoxWrapper():\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = x.cpu().detach().numpy()\n",
        "        x = getBinPrediction(x)\n",
        "        x = torch.FloatTensor(x).to(device)\n",
        "        return x\n",
        "\n",
        "    def eval(self):\n",
        "        x = 1\n",
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63QD0lJqeWrv"
      },
      "source": [
        "#Traing WGAN and Testing BlackBox IDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NhN3LfdvOZ_L",
        "outputId": "b24b9cce-6ce0-4465-b5a6-b0de4d924096"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "from preprocessing import preprocess4, create_batch2\n",
        "from model.model_class import Generator, Discriminator\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(12345)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "#functional_categories = ['intrinsic', 'time_based']\n",
        "#functional_categories = ['intrinsic', 'time_based', 'host_based']\n",
        "functional_categories = ['intrinsic', 'content']\n",
        "\n",
        "data = pd.read_csv(\"dataset/GAN_train.csv\")\n",
        "train_data, raw_attack, normal, true_label, modification_mask \\\n",
        "    = preprocess4(data, functional_categories=functional_categories)\n",
        "\n",
        "BATCH_SIZE = 256  # Batch size\n",
        "GEN_ITERS = 20\n",
        "CRITIC_ITERS = 1  # For WGAN and WGAN-GP, number of critic iters per gen iter\n",
        "LAMBDA = 10  # Gradient penalty lambda hyperparameter\n",
        "MAX_EPOCH = 1000  # How many generator iterations to train for\n",
        "NOISE_SIZE = 27\n",
        "generator_input_dim = train_data.shape[1] + NOISE_SIZE\n",
        "generator_output_dim = train_data.shape[1]\n",
        "discriminator_output_dim = 1\n",
        "CLAMP = 0.01\n",
        "\n",
        "# read parameters of IDS\n",
        "\n",
        "modification_mask = np.tile(modification_mask, [BATCH_SIZE, 1])\n",
        "modification_mask_t = torch.FloatTensor(modification_mask).to(device)\n",
        "\n",
        "ids_model = BlackBoxWrapper(generator_output_dim, 1)\n",
        "# param = torch.load('model/IDS.pth')\n",
        "# ids_model.load_state_dict(param)\n",
        "# read model\n",
        "\n",
        "generator = Generator(generator_input_dim, generator_output_dim).to(device)\n",
        "discriminator = Discriminator(generator_output_dim, discriminator_output_dim).to(device)\n",
        "\n",
        "\n",
        "optimizer_G = optim.RMSprop(generator.parameters(), lr=0.0001)\n",
        "optimizer_D = optim.RMSprop(discriminator.parameters(), lr=0.0001)\n",
        "\n",
        "batch_attack = create_batch2(raw_attack, BATCH_SIZE)\n",
        "batch_normal = create_batch2(normal, BATCH_SIZE)\n",
        "\n",
        "d_losses, g_losses = [], []\n",
        "ids_model.eval()\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "cnt = -5\n",
        "print(\"IDSGAN start training\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "gen_attack_batch_idx = 0\n",
        "dis_normal_batch_idx = 0\n",
        "\n",
        "for epoch in range(MAX_EPOCH):\n",
        "\n",
        "    #  Train Generator\n",
        "\n",
        "    gen_loss = 0.\n",
        "    for g in range(GEN_ITERS):\n",
        "\n",
        "        for p in generator.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        for p in discriminator.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        ba = batch_attack[gen_attack_batch_idx]\n",
        "        gen_attack_batch_idx = (gen_attack_batch_idx + 1) % len(batch_attack)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        noise = np.random.uniform(0., 1., (BATCH_SIZE, NOISE_SIZE))\n",
        "        gen_x = np.concatenate([ba, noise], axis=1)\n",
        "\n",
        "        gen_x_t = torch.FloatTensor(gen_x).to(device)\n",
        "\n",
        "        adversarial_attack_modification = generator(gen_x_t, modification_mask_t)\n",
        "        adversarial_attack = torch.FloatTensor(ba).to(device) + adversarial_attack_modification\n",
        "\n",
        "        D_pred = discriminator(adversarial_attack)\n",
        "        g_loss = torch.mean(D_pred)\n",
        "        # print(\"G loss\")\n",
        "        # print(g_loss)\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        for p in generator.parameters():\n",
        "            p = torch.clamp(p, -0.01, 0.01)\n",
        "\n",
        "        gen_loss += g_loss.item()\n",
        "\n",
        "    gen_loss = gen_loss / GEN_ITERS\n",
        "\n",
        "    for p in discriminator.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    for p in generator.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    dis_loss = 0.\n",
        "    for c in range(CRITIC_ITERS):\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        np.random.shuffle(batch_normal)\n",
        "\n",
        "        bn = batch_normal[dis_normal_batch_idx]\n",
        "        ba = batch_attack[dis_normal_batch_idx % len(batch_attack)]\n",
        "        dis_normal_batch_idx = (dis_normal_batch_idx + 1) % len(batch_normal)\n",
        "\n",
        "        noise = np.random.uniform(0., 1., (BATCH_SIZE, NOISE_SIZE))\n",
        "        gen_x = np.concatenate([ba, noise], axis=1)\n",
        "\n",
        "        gen_x_t = torch.FloatTensor(gen_x).to(device)\n",
        "\n",
        "        adversarial_attack_modification = generator(gen_x_t, modification_mask_t)\n",
        "        adversarial_attack_t = torch.FloatTensor(ba).to(device) + adversarial_attack_modification\n",
        "        adversarial_attack = adversarial_attack_t.cpu().detach().numpy()\n",
        "\n",
        "        ids_input = np.concatenate([bn, adversarial_attack], axis=0)\n",
        "\n",
        "        l = list(range(len(ids_input)))\n",
        "        np.random.shuffle(l)\n",
        "\n",
        "        ids_input = ids_input[l]\n",
        "\n",
        "        ids_input = torch.FloatTensor(ids_input).to(device)\n",
        "\n",
        "        out = ids_model(ids_input)\n",
        "        out = torch.reshape(out, [len(ids_input)])\n",
        "        out_np = out.cpu().detach().numpy()\n",
        "\n",
        "        ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "\n",
        "        pred_normal = ids_input.cpu().detach().numpy()[ids_pred_label == 0]\n",
        "        pred_attack = ids_input.cpu().detach().numpy()[ids_pred_label == 1]\n",
        "\n",
        "        # print(len(pred_normal))\n",
        "        # print(len(pred_attack))\n",
        "\n",
        "        if len(pred_attack) == 0:\n",
        "            cnt += 1\n",
        "            break\n",
        "\n",
        "        D_normal = discriminator(torch.FloatTensor(pred_normal).to(device))\n",
        "        D_attack = discriminator(torch.FloatTensor(pred_attack).to(device))\n",
        "        D_adv = discriminator(adversarial_attack_t)\n",
        "\n",
        "        loss_normal = torch.mean(D_normal)\n",
        "        loss_attack = torch.mean(D_attack)\n",
        "        loss_adv = torch.mean(D_adv)\n",
        "\n",
        "        # print(loss_normal)\n",
        "        # print(loss_attack)\n",
        "        # print(loss_adv)\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        d_loss = (loss_normal - loss_attack)  # + LAMBDA * gradient_penalty\n",
        "        dis_loss += d_loss.item()\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        for p in discriminator.parameters():\n",
        "            p = torch.clamp(p, -0.01, 0.01)\n",
        "\n",
        "    dis_loss = dis_loss / CRITIC_ITERS\n",
        "\n",
        "    d_losses.append(dis_loss)\n",
        "    g_losses.append(gen_loss)\n",
        "\n",
        "    print(f\"{epoch} : {gen_loss} \\t {dis_loss}\")\n",
        "\n",
        "    torch.save(generator.state_dict(), 'save_model/generator.pth')\n",
        "    torch.save(discriminator.state_dict(), 'save_model/discreminator.pth')\n",
        "\n",
        "    if cnt >= 100:\n",
        "        print(\"Not exist predicted attack traffic\")\n",
        "        break\n",
        "\n",
        "print(\"IDSGAN finish training\")\n",
        "torch.save(generator.state_dict(), 'save_model/generator.pth')\n",
        "torch.save(discriminator.state_dict(), 'save_model/discreminator.pth')\n",
        "plt.plot(d_losses, label=\"D_loss\")\n",
        "plt.plot(g_losses, label=\"G_loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "IDSGAN start training\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 : 0.3960837051272392 \t 0.01558232307434082\n",
            "1 : 0.4070858657360077 \t 0.004598945379257202\n",
            "2 : 0.42050943374633787 \t -0.010844260454177856\n",
            "3 : 0.43061604499816897 \t -0.01413080096244812\n",
            "4 : 0.43962960243225097 \t -0.022183507680892944\n",
            "5 : 0.43891132473945615 \t -0.027121633291244507\n",
            "6 : 0.43896905034780503 \t -0.026742637157440186\n",
            "7 : 0.4376734584569931 \t -0.036143481731414795\n",
            "8 : 0.44026325047016146 \t -0.03982064127922058\n",
            "9 : 0.44062288850545883 \t -0.03897279500961304\n",
            "10 : 0.4411448672413826 \t -0.04919502139091492\n",
            "11 : 0.4411817938089371 \t -0.03113415837287903\n",
            "12 : 0.4431337684392929 \t -0.06448736786842346\n",
            "13 : 0.44442261159420016 \t -0.0451713502407074\n",
            "14 : 0.44500336796045303 \t -0.051665931940078735\n",
            "15 : 0.4475592255592346 \t -0.0550389289855957\n",
            "16 : 0.4492775097489357 \t -0.05990108847618103\n",
            "17 : 0.4509203791618347 \t -0.054251283407211304\n",
            "18 : 0.4532551109790802 \t -0.06989187002182007\n",
            "19 : 0.45288531929254533 \t -0.047024279832839966\n",
            "20 : 0.45534736961126326 \t -0.08499571681022644\n",
            "21 : 0.45710605531930926 \t -0.0844506323337555\n",
            "22 : 0.4596771314740181 \t -0.08917099237442017\n",
            "23 : 0.4603257790207863 \t -0.07769691944122314\n",
            "24 : 0.461796298623085 \t -0.10355174541473389\n",
            "25 : 0.4632031574845314 \t -0.09180983901023865\n",
            "26 : 0.46535158455371856 \t -0.10949081182479858\n",
            "27 : 0.4664743572473526 \t -0.0752997100353241\n",
            "28 : 0.46141469925642015 \t -0.12020957469940186\n",
            "29 : 0.46124830543994905 \t -0.12262067198753357\n",
            "30 : 0.4623449444770813 \t -0.1516287922859192\n",
            "31 : 0.460905610024929 \t -0.13628357648849487\n",
            "32 : 0.46290802508592604 \t -0.14898568391799927\n",
            "33 : 0.4648766666650772 \t -0.1497441828250885\n",
            "34 : 0.4680390939116478 \t -0.1639600396156311\n",
            "35 : 0.4703572511672974 \t -0.1562875211238861\n",
            "36 : 0.4740563154220581 \t -0.14701786637306213\n",
            "37 : 0.4725997716188431 \t -0.14772745966911316\n",
            "38 : 0.47462582439184187 \t -0.16020965576171875\n",
            "39 : 0.4744111478328705 \t -0.1300467848777771\n",
            "40 : 0.47578532695770265 \t -0.1793070137500763\n",
            "41 : 0.4767890080809593 \t -0.132718026638031\n",
            "42 : 0.4780301615595818 \t -0.17188459634780884\n",
            "43 : 0.4798481032252312 \t -0.15674343705177307\n",
            "44 : 0.47988238781690595 \t -0.1894231140613556\n",
            "45 : 0.480257922410965 \t -0.17749854922294617\n",
            "46 : 0.48244968354701995 \t -0.20750939846038818\n",
            "47 : 0.48313250243663786 \t -0.2046913504600525\n",
            "48 : 0.483060921728611 \t -0.23595833778381348\n",
            "49 : 0.4847079664468765 \t -0.2247406542301178\n",
            "50 : 0.4887343212962151 \t -0.23664790391921997\n",
            "51 : 0.49070729613304137 \t -0.23551172018051147\n",
            "52 : 0.49353634715080263 \t -0.24234539270401\n",
            "53 : 0.49685791283845904 \t -0.12181657552719116\n",
            "54 : 0.49614461362361906 \t -0.18607410788536072\n",
            "55 : 0.4970119744539261 \t -0.16460800170898438\n",
            "56 : 0.49571412801742554 \t -0.25966182351112366\n",
            "57 : 0.49347602427005766 \t -0.2120552659034729\n",
            "58 : 0.49040103554725645 \t -0.2736895680427551\n",
            "59 : 0.49102735221385957 \t -0.2511211931705475\n",
            "60 : 0.4929914638400078 \t -0.26851218938827515\n",
            "61 : 0.49394836127758024 \t -0.2758512794971466\n",
            "62 : 0.49352031797170637 \t -0.2704126536846161\n",
            "63 : 0.49382481575012205 \t -0.2296118438243866\n",
            "64 : 0.4948278471827507 \t -0.2959233820438385\n",
            "65 : 0.49342153668403627 \t -0.23750174045562744\n",
            "66 : 0.4943047478795052 \t -0.2969375550746918\n",
            "67 : 0.4928487718105316 \t -0.22828596830368042\n",
            "68 : 0.493368037045002 \t -0.2947244644165039\n",
            "69 : 0.4931610688567162 \t -0.2507829964160919\n",
            "70 : 0.49305757135152817 \t -0.3140029013156891\n",
            "71 : 0.4860862597823143 \t -0.17725041508674622\n",
            "72 : 0.48219566494226457 \t -0.24088269472122192\n",
            "73 : 0.4805727243423462 \t -0.21650108695030212\n",
            "74 : 0.4830320507287979 \t -0.23710176348686218\n",
            "75 : 0.48427716344594957 \t -0.1373814046382904\n",
            "76 : 0.48468702733516694 \t -0.17919334769248962\n",
            "77 : 0.4842586427927017 \t -0.09678217768669128\n",
            "78 : 0.4824387401342392 \t -0.23662716150283813\n",
            "79 : 0.4821596533060074 \t -0.17946457862854004\n",
            "80 : 0.4855931282043457 \t -0.25007402896881104\n",
            "81 : 0.4878061532974243 \t -0.1804339587688446\n",
            "82 : 0.48887875527143476 \t -0.2444605827331543\n",
            "83 : 0.4903070330619812 \t -0.13586336374282837\n",
            "84 : 0.4917369678616524 \t -0.18713998794555664\n",
            "85 : 0.491207680106163 \t -0.10050302743911743\n",
            "86 : 0.4912234514951706 \t -0.22138041257858276\n",
            "87 : 0.4917331486940384 \t -0.2109048068523407\n",
            "88 : 0.4943304806947708 \t -0.27062520384788513\n",
            "89 : 0.4961075484752655 \t -0.12649118900299072\n",
            "90 : 0.4968434080481529 \t -0.19839119911193848\n",
            "91 : 0.497629451751709 \t -0.1430262327194214\n",
            "92 : 0.4963140979409218 \t -0.3057558238506317\n",
            "93 : 0.4896990150213242 \t -0.16656532883644104\n",
            "94 : 0.49032192081213 \t -0.21207541227340698\n",
            "95 : 0.48845518231391905 \t -0.021554917097091675\n",
            "96 : 0.49122980386018755 \t -0.09545725584030151\n",
            "97 : 0.4932640835642815 \t -0.08196213841438293\n",
            "98 : 0.4985570102930069 \t -0.14860287308692932\n",
            "99 : 0.5001629441976547 \t -0.028569161891937256\n",
            "100 : 0.5036519497632981 \t -0.2813141942024231\n",
            "101 : 0.5013282656669616 \t -0.19608372449874878\n",
            "102 : 0.5025403752923012 \t -0.3137570917606354\n",
            "103 : 0.5024173751473426 \t -0.18326187133789062\n",
            "104 : 0.5027791500091553 \t -0.30936726927757263\n",
            "105 : 0.5013249352574348 \t -0.06151130795478821\n",
            "106 : 0.5031468003988266 \t -0.10872679948806763\n",
            "107 : 0.506945326924324 \t -0.12646305561065674\n",
            "108 : 0.5077964723110199 \t -0.33187270164489746\n",
            "109 : 0.506145840883255 \t -0.10199564695358276\n",
            "110 : 0.508420467376709 \t -0.1466332972049713\n",
            "111 : 0.511583685874939 \t -0.06069231033325195\n",
            "112 : 0.5132171034812927 \t -0.10735800862312317\n",
            "113 : 0.5189969837665558 \t -0.0728321373462677\n",
            "114 : 0.5250801056623459 \t -0.3131794333457947\n",
            "115 : 0.5245984882116318 \t -0.30888423323631287\n",
            "116 : 0.5231640219688416 \t -0.3436940610408783\n",
            "117 : 0.5218846797943115 \t -0.28700366616249084\n",
            "118 : 0.5217690259218216 \t -0.3302419185638428\n",
            "119 : 0.5254107117652893 \t -0.2660885751247406\n",
            "120 : 0.5277779966592788 \t -0.3483428955078125\n",
            "121 : 0.5247605532407761 \t -0.0735439658164978\n",
            "122 : 0.5283259153366089 \t -0.28345346450805664\n",
            "123 : 0.5282659560441971 \t -0.1279553771018982\n",
            "124 : 0.5308555334806442 \t -0.2936811149120331\n",
            "125 : 0.5286587774753571 \t -0.0866529643535614\n",
            "126 : 0.5306571751832962 \t -0.3115295469760895\n",
            "127 : 0.5288410693407058 \t -0.281943678855896\n",
            "128 : 0.5282270580530166 \t -0.289971262216568\n",
            "129 : 0.5289292424917221 \t -0.24844923615455627\n",
            "130 : 0.5298248142004013 \t -0.3322232961654663\n",
            "131 : 0.5281473189592362 \t -0.18398845195770264\n",
            "132 : 0.5304049730300904 \t -0.35694029927253723\n",
            "133 : 0.528658002614975 \t -0.344668447971344\n",
            "134 : 0.5268950164318085 \t -0.3551459312438965\n",
            "135 : 0.5262331694364548 \t -0.40263834595680237\n",
            "136 : 0.5239949107170105 \t -0.3545944392681122\n",
            "137 : 0.5223462402820587 \t -0.3829755187034607\n",
            "138 : 0.516738498210907 \t -0.17685922980308533\n",
            "139 : 0.5286625325679779 \t -0.20531365275382996\n",
            "140 : 0.5389857858419418 \t -0.1965942084789276\n",
            "141 : 0.5491836935281753 \t -0.21400028467178345\n",
            "142 : 0.5608983188867569 \t -0.23974695801734924\n",
            "143 : 0.5726951628923416 \t -0.28735002875328064\n",
            "144 : 0.58504319190979 \t -0.2718204855918884\n",
            "145 : 0.5966537922620774 \t -0.2712286412715912\n",
            "146 : 0.6032094448804856 \t -0.17851310968399048\n",
            "147 : 0.6097081869840622 \t -0.21416667103767395\n",
            "148 : 0.6164807260036469 \t -0.19692453742027283\n",
            "149 : 0.6271619379520417 \t -0.26794764399528503\n",
            "150 : 0.6335806548595428 \t -0.20253044366836548\n",
            "151 : 0.6435459077358245 \t -0.2836032211780548\n",
            "152 : 0.6477856755256652 \t -0.25602638721466064\n",
            "153 : 0.656557047367096 \t -0.3035053014755249\n",
            "154 : 0.66404729783535 \t -0.3208351731300354\n",
            "155 : 0.6695695221424103 \t -0.30462831258773804\n",
            "156 : 0.6692751914262771 \t -0.3266855776309967\n",
            "157 : 0.6671852856874466 \t -0.3389943540096283\n",
            "158 : 0.668274137377739 \t -0.3219134509563446\n",
            "159 : 0.6694346904754639 \t -0.35979029536247253\n",
            "160 : 0.6681615442037583 \t -0.31501126289367676\n",
            "161 : 0.6720623135566711 \t -0.3612777888774872\n",
            "162 : 0.6662134915590286 \t -0.4275318384170532\n",
            "163 : 0.6555204957723617 \t -0.44544604420661926\n",
            "164 : 0.6486904680728912 \t -0.2913423776626587\n",
            "165 : 0.6502689301967621 \t -0.4643537402153015\n",
            "166 : 0.64500992000103 \t -0.3255622088909149\n",
            "167 : 0.6466062605381012 \t -0.33307042717933655\n",
            "168 : 0.6463160991668702 \t -0.32400205731391907\n",
            "169 : 0.6501191258430481 \t -0.34279897809028625\n",
            "170 : 0.6516634047031402 \t -0.3452942371368408\n",
            "171 : 0.6521996170282364 \t -0.3989870548248291\n",
            "172 : 0.6512321144342422 \t -0.37581273913383484\n",
            "173 : 0.6490957409143447 \t -0.42829954624176025\n",
            "174 : 0.6458698064088821 \t -0.38418614864349365\n",
            "175 : 0.6457947462797164 \t -0.41731125116348267\n",
            "176 : 0.6440607935190201 \t -0.2630956768989563\n",
            "177 : 0.6419103562831878 \t -0.4404071867465973\n",
            "178 : 0.6383072316646576 \t -0.3842606246471405\n",
            "179 : 0.6389411687850952 \t -0.46377745270729065\n",
            "180 : 0.635362109541893 \t -0.34839680790901184\n",
            "181 : 0.6346964001655578 \t -0.39640483260154724\n",
            "182 : 0.6308754175901413 \t -0.3216177523136139\n",
            "183 : 0.6327032893896103 \t -0.34466734528541565\n",
            "184 : 0.6332483291625977 \t -0.3183964490890503\n",
            "185 : 0.6376180529594422 \t -0.3481750190258026\n",
            "186 : 0.6380013793706893 \t -0.3411455452442169\n",
            "187 : 0.6416229456663132 \t -0.34865108132362366\n",
            "188 : 0.6433051854372025 \t -0.3171108067035675\n",
            "189 : 0.6475363731384277 \t -0.49293652176856995\n",
            "190 : 0.6424505293369294 \t -0.4181985557079315\n",
            "191 : 0.6418900430202484 \t -0.40182119607925415\n",
            "192 : 0.636598989367485 \t -0.36272555589675903\n",
            "193 : 0.6353708297014237 \t -0.3674660921096802\n",
            "194 : 0.6306950390338898 \t -0.36375361680984497\n",
            "195 : 0.6322028279304505 \t -0.4764191806316376\n",
            "196 : 0.6283950060606003 \t -0.3280288577079773\n",
            "197 : 0.6270705759525299 \t -0.3633970618247986\n",
            "198 : 0.6243147790431977 \t -0.3722456097602844\n",
            "199 : 0.6220637947320938 \t -0.4826200604438782\n",
            "200 : 0.6191047638654709 \t -0.4499896764755249\n",
            "201 : 0.6184047043323517 \t -0.42141225934028625\n",
            "202 : 0.6116408348083496 \t -0.43579772114753723\n",
            "203 : 0.6132732152938842 \t -0.5082440972328186\n",
            "204 : 0.6072900980710983 \t -0.44080981612205505\n",
            "205 : 0.6071697026491165 \t -0.5460753440856934\n",
            "206 : 0.6005411744117737 \t -0.4833284318447113\n",
            "207 : 0.5984550744295121 \t -0.5120800733566284\n",
            "208 : 0.5915095329284668 \t -0.4767603278160095\n",
            "209 : 0.5875066995620728 \t -0.3386870324611664\n",
            "210 : 0.5925894111394883 \t -0.3439100682735443\n",
            "211 : 0.5940842777490616 \t -0.5510432720184326\n",
            "212 : 0.5814473539590835 \t -0.5275630354881287\n",
            "213 : 0.5732275366783142 \t -0.5527852773666382\n",
            "214 : 0.5665302693843841 \t -0.5331717729568481\n",
            "215 : 0.5597368896007537 \t -0.5550240874290466\n",
            "216 : 0.5501004427671432 \t -0.5316190123558044\n",
            "217 : 0.5470557391643525 \t -0.5985890626907349\n",
            "218 : 0.5378904402256012 \t -0.5538557767868042\n",
            "219 : 0.5351936489343643 \t -0.5773620009422302\n",
            "220 : 0.5254071533679963 \t -0.5505093336105347\n",
            "221 : 0.5209665358066559 \t -0.5901526212692261\n",
            "222 : 0.5134876310825348 \t -0.5459953546524048\n",
            "223 : 0.5130368053913117 \t -0.6066247820854187\n",
            "224 : 0.5046512573957443 \t -0.4911210536956787\n",
            "225 : 0.49959238320589067 \t -0.5719151496887207\n",
            "226 : 0.4925815314054489 \t -0.5716503858566284\n",
            "227 : 0.48743338882923126 \t -0.5936006307601929\n",
            "228 : 0.47826059311628344 \t -0.6322457790374756\n",
            "229 : 0.469494092464447 \t -0.658444881439209\n",
            "230 : 0.46250550299882887 \t -0.5747225284576416\n",
            "231 : 0.4505304515361786 \t -0.6517055630683899\n",
            "232 : 0.44833447635173795 \t -0.6574615240097046\n",
            "233 : 0.4379573181271553 \t -0.6637461185455322\n",
            "234 : 0.43056495785713195 \t -0.6588619947433472\n",
            "235 : 0.42535185515880586 \t -0.6614441871643066\n",
            "236 : 0.41780479103326795 \t -0.643998384475708\n",
            "237 : 0.40874893218278885 \t -0.6168173551559448\n",
            "238 : 0.4008816257119179 \t -0.43731221556663513\n",
            "239 : 0.4034424602985382 \t -0.4483252465724945\n",
            "240 : 0.4031013011932373 \t -0.4408431649208069\n",
            "241 : 0.40611001700162885 \t -0.47548365592956543\n",
            "242 : 0.4056205913424492 \t -0.428504079580307\n",
            "243 : 0.4078139752149582 \t -0.4561517834663391\n",
            "244 : 0.40483753979206083 \t -0.4099408984184265\n",
            "245 : 0.4107345253229141 \t -0.4699264168739319\n",
            "246 : 0.41078062951564787 \t -0.39869609475135803\n",
            "247 : 0.4148329347372055 \t -0.6618846654891968\n",
            "248 : 0.4107027769088745 \t -0.5973338484764099\n",
            "249 : 0.40142149925231935 \t -0.6775665283203125\n",
            "250 : 0.39537647664546965 \t -0.6574641466140747\n",
            "251 : 0.3884859621524811 \t -0.6707677841186523\n",
            "252 : 0.38312809318304064 \t -0.6869271993637085\n",
            "253 : 0.3764896348118782 \t -0.6547785997390747\n",
            "254 : 0.37022527903318403 \t -0.6984298229217529\n",
            "255 : 0.36497595012187956 \t -0.6690564751625061\n",
            "256 : 0.3610182359814644 \t -0.7181473970413208\n",
            "257 : 0.3529440686106682 \t -0.7197272777557373\n",
            "258 : 0.3473551064729691 \t -0.6916210055351257\n",
            "259 : 0.34366481602191923 \t -0.7146618366241455\n",
            "260 : 0.33898506313562393 \t -0.7218656539916992\n",
            "261 : 0.3338930293917656 \t -0.7161668539047241\n",
            "262 : 0.32846812456846236 \t -0.740678608417511\n",
            "263 : 0.32266885191202166 \t -0.7421145439147949\n",
            "264 : 0.3204297974705696 \t -0.7192873954772949\n",
            "265 : 0.31348375529050826 \t -0.7245833277702332\n",
            "266 : 0.3096057683229446 \t -0.7449144721031189\n",
            "267 : 0.3062723740935326 \t -0.7588133811950684\n",
            "268 : 0.30272858291864396 \t -0.6989850997924805\n",
            "269 : 0.29898459017276763 \t -0.7255783081054688\n",
            "270 : 0.296310992538929 \t -0.46777623891830444\n",
            "271 : 0.29810685515403745 \t -0.3826807737350464\n",
            "272 : 0.302265502512455 \t -0.4685944616794586\n",
            "273 : 0.30406007915735245 \t -0.3829731047153473\n",
            "274 : 0.30923735052347184 \t -0.744149386882782\n",
            "275 : 0.3033242583274841 \t -0.706083357334137\n",
            "276 : 0.29958837330341337 \t -0.7478580474853516\n",
            "277 : 0.29346629679203035 \t -0.7798091173171997\n",
            "278 : 0.2912444472312927 \t -0.7113533020019531\n",
            "279 : 0.28676446080207824 \t -0.7774361371994019\n",
            "280 : 0.28451581448316576 \t -0.7382550239562988\n",
            "281 : 0.28124195486307146 \t -0.7728109955787659\n",
            "282 : 0.27688207924366 \t -0.7513389587402344\n",
            "283 : 0.27497847378253937 \t -0.7307291626930237\n",
            "284 : 0.2720989540219307 \t -0.7692134976387024\n",
            "285 : 0.26937578469514845 \t -0.7510159015655518\n",
            "286 : 0.2648676671087742 \t -0.7823559045791626\n",
            "287 : 0.26512019634246825 \t -0.6913143992424011\n",
            "288 : 0.2614294707775116 \t -0.7925364971160889\n",
            "289 : 0.258956640958786 \t -0.7023121118545532\n",
            "290 : 0.25813751518726347 \t -0.7729882001876831\n",
            "291 : 0.2555460795760155 \t -0.8093647956848145\n",
            "292 : 0.25265499129891394 \t -0.7864604592323303\n",
            "293 : 0.24930868074297904 \t -0.793039083480835\n",
            "294 : 0.24755536615848542 \t -0.7556812763214111\n",
            "295 : 0.24512219280004502 \t -0.7480207681655884\n",
            "296 : 0.24170615002512932 \t -0.7791497111320496\n",
            "297 : 0.23850843459367752 \t -0.6961103677749634\n",
            "298 : 0.23756530359387398 \t -0.8206595182418823\n",
            "299 : 0.23505380898714065 \t -0.7740161418914795\n",
            "300 : 0.23261375427246095 \t -0.8170244693756104\n",
            "301 : 0.23139145746827125 \t -0.8013896346092224\n",
            "302 : 0.22946272194385528 \t -0.8058387637138367\n",
            "303 : 0.22599126398563385 \t -0.7831795811653137\n",
            "304 : 0.22674109488725663 \t -0.7693035006523132\n",
            "305 : 0.2237456738948822 \t -0.7622839212417603\n",
            "306 : 0.22400892078876494 \t -0.786780834197998\n",
            "307 : 0.2213895872235298 \t -0.7793699502944946\n",
            "308 : 0.2218959577381611 \t -0.80182945728302\n",
            "309 : 0.2199118949472904 \t -0.7902067303657532\n",
            "310 : 0.21797674596309663 \t -0.8005499243736267\n",
            "311 : 0.2143045850098133 \t -0.6346704959869385\n",
            "312 : 0.21503706350922586 \t -0.43516960740089417\n",
            "313 : 0.21648059040308 \t -0.5514169931411743\n",
            "314 : 0.21805072650313378 \t -0.8238178491592407\n",
            "315 : 0.2161284066736698 \t -0.8673480153083801\n",
            "316 : 0.21322237625718116 \t -0.7885153293609619\n",
            "317 : 0.2092660002410412 \t -0.5579135417938232\n",
            "318 : 0.2070618599653244 \t -0.8107715249061584\n",
            "319 : 0.2065744623541832 \t -0.8192606568336487\n",
            "320 : 0.2048078879714012 \t -0.8321568369865417\n",
            "321 : 0.2040904052555561 \t -0.7539252638816833\n",
            "322 : 0.20154666975140573 \t -0.797218918800354\n",
            "323 : 0.20249657481908798 \t -0.8366724848747253\n",
            "324 : 0.20170022323727607 \t -0.8427594304084778\n",
            "325 : 0.20056554451584815 \t -0.8274884819984436\n",
            "326 : 0.19857715144753457 \t -0.8357508778572083\n",
            "327 : 0.19626138135790824 \t -0.7854886054992676\n",
            "328 : 0.19390689581632614 \t -0.8615779876708984\n",
            "329 : 0.19270494803786278 \t -0.8524386286735535\n",
            "330 : 0.19197083190083503 \t -0.7918635010719299\n",
            "331 : 0.19253871962428093 \t -0.8740361928939819\n",
            "332 : 0.19060007482767105 \t -0.8542197942733765\n",
            "333 : 0.19039923325181007 \t -0.8751665353775024\n",
            "334 : 0.18991137593984603 \t -0.8373936414718628\n",
            "335 : 0.1889518044888973 \t -0.7632945775985718\n",
            "336 : 0.1883825972676277 \t -0.8436363339424133\n",
            "337 : 0.18820913136005402 \t -0.8037489056587219\n",
            "338 : 0.18684947937726976 \t -0.8253248929977417\n",
            "339 : 0.18598462715744973 \t -0.8121138215065002\n",
            "340 : 0.18700647205114365 \t -0.8459056615829468\n",
            "341 : 0.1853003166615963 \t -0.8432493209838867\n",
            "342 : 0.18371281623840333 \t -0.8665604591369629\n",
            "343 : 0.1839569963514805 \t -0.8969414830207825\n",
            "344 : 0.18254157826304435 \t -0.8324497938156128\n",
            "345 : 0.18145351931452752 \t -0.8962427973747253\n",
            "346 : 0.17947833240032196 \t -0.8490850329399109\n",
            "347 : 0.18017296344041825 \t -0.8924403190612793\n",
            "348 : 0.1787463143467903 \t -0.8437689542770386\n",
            "349 : 0.17783511132001878 \t -0.8893165588378906\n",
            "350 : 0.1778251364827156 \t -0.8080830574035645\n",
            "351 : 0.17762300223112107 \t -0.8889178037643433\n",
            "352 : 0.17548926621675492 \t -0.8221547603607178\n",
            "353 : 0.17539335489273072 \t -0.8356334567070007\n",
            "354 : 0.17318888157606124 \t -0.8845010995864868\n",
            "355 : 0.17310755029320718 \t -0.9054062366485596\n",
            "356 : 0.1705281525850296 \t -0.8730960488319397\n",
            "357 : 0.17207712233066558 \t -0.824205219745636\n",
            "358 : 0.17108274027705192 \t -0.8705214262008667\n",
            "359 : 0.17144739478826523 \t -0.9002530574798584\n",
            "360 : 0.17068130597472192 \t -0.8656301498413086\n",
            "361 : 0.17033803537487985 \t -0.9079583287239075\n",
            "362 : 0.16864603608846665 \t -0.9007678031921387\n",
            "363 : 0.16704705953598023 \t -0.5514855980873108\n",
            "364 : 0.16710673570632933 \t -0.5588940382003784\n",
            "365 : 0.16636897996068 \t -0.4335213303565979\n",
            "366 : 0.16739280745387078 \t -0.5149907469749451\n",
            "367 : 0.1681400515139103 \t -0.400588721036911\n",
            "368 : 0.16835474967956543 \t -0.8896782398223877\n",
            "369 : 0.16754791662096977 \t -0.8290342688560486\n",
            "370 : 0.16779540181159974 \t -0.8876399397850037\n",
            "371 : 0.1671241842210293 \t -0.8973971605300903\n",
            "372 : 0.16649343594908714 \t -0.8780044317245483\n",
            "373 : 0.16657619401812554 \t -0.900070071220398\n",
            "374 : 0.1659776024520397 \t -0.8567131757736206\n",
            "375 : 0.1652693383395672 \t -0.9083946943283081\n",
            "376 : 0.16462163478136063 \t -0.8727490305900574\n",
            "377 : 0.16561670675873758 \t -0.909726619720459\n",
            "378 : 0.16374992057681084 \t -0.8376281261444092\n",
            "379 : 0.16235923171043395 \t -0.9043884873390198\n",
            "380 : 0.16168524473905563 \t -0.8514960408210754\n",
            "381 : 0.16328932121396064 \t -0.9142807722091675\n",
            "382 : 0.16072747930884362 \t -0.5245431065559387\n",
            "383 : 0.16120799332857133 \t -0.41831380128860474\n",
            "384 : 0.1605999007821083 \t -0.49714839458465576\n",
            "385 : 0.16315532326698304 \t -0.42286384105682373\n",
            "386 : 0.1625464990735054 \t -0.5261948108673096\n",
            "387 : 0.16117824614048004 \t -0.4230284094810486\n",
            "388 : 0.16193860694766044 \t -0.5233656764030457\n",
            "389 : 0.16327485665678979 \t -0.4201553761959076\n",
            "390 : 0.16291645243763925 \t -0.5179964303970337\n",
            "391 : 0.16362853422760965 \t -0.4411509037017822\n",
            "392 : 0.1637017086148262 \t -0.8243785500526428\n",
            "393 : 0.16353349462151529 \t -0.9181203246116638\n",
            "394 : 0.16018274053931236 \t -0.9111955761909485\n",
            "395 : 0.15993270203471183 \t -0.8449194431304932\n",
            "396 : 0.15760991871356964 \t -0.9133648872375488\n",
            "397 : 0.15728232711553575 \t -0.8738992810249329\n",
            "398 : 0.15510253310203553 \t -0.8828611969947815\n",
            "399 : 0.15469803437590599 \t -0.8436176180839539\n",
            "400 : 0.1523246981203556 \t -0.9379383325576782\n",
            "401 : 0.15055350661277772 \t -0.9020271301269531\n",
            "402 : 0.1497481346130371 \t -0.8989165425300598\n",
            "403 : 0.15090019777417182 \t -0.9192327857017517\n",
            "404 : 0.1489562749862671 \t -0.9116250872612\n",
            "405 : 0.14897462651133536 \t -0.8950096964836121\n",
            "406 : 0.14714473113417625 \t -0.930046558380127\n",
            "407 : 0.14626859351992608 \t -0.9206627011299133\n",
            "408 : 0.1459263727068901 \t -0.9179967045783997\n",
            "409 : 0.1445552758872509 \t -0.876024067401886\n",
            "410 : 0.14463253542780877 \t -0.9375542402267456\n",
            "411 : 0.14461423456668854 \t -0.9205987453460693\n",
            "412 : 0.14342878460884095 \t -0.8680068850517273\n",
            "413 : 0.14358458667993546 \t -0.9221224784851074\n",
            "414 : 0.14200111776590346 \t -0.9342033863067627\n",
            "415 : 0.14185986518859864 \t -0.9324440360069275\n",
            "416 : 0.14048229902982712 \t -0.9205880761146545\n",
            "417 : 0.14115888699889184 \t -0.9188001155853271\n",
            "418 : 0.14072555899620057 \t -0.9342281222343445\n",
            "419 : 0.13881191611289978 \t -0.8376981616020203\n",
            "420 : 0.13902080208063125 \t -0.8594964742660522\n",
            "421 : 0.13986177891492843 \t -0.8884178996086121\n",
            "422 : 0.1401255950331688 \t -0.9357174634933472\n",
            "423 : 0.13814362287521362 \t -0.9235435128211975\n",
            "424 : 0.13809814378619195 \t -0.9348162412643433\n",
            "425 : 0.13774774670600892 \t -0.9261075258255005\n",
            "426 : 0.13684344813227653 \t -0.8414555788040161\n",
            "427 : 0.13649300970137118 \t -0.9209432601928711\n",
            "428 : 0.13520708344876767 \t -0.9330518841743469\n",
            "429 : 0.13547155782580375 \t -0.889784038066864\n",
            "430 : 0.1355339664965868 \t -0.9264556169509888\n",
            "431 : 0.1353219762444496 \t -0.9245447516441345\n",
            "432 : 0.13507956005632876 \t -0.8935949206352234\n",
            "433 : 0.13510761745274066 \t -0.8692287802696228\n",
            "434 : 0.1353421952575445 \t -0.9376780986785889\n",
            "435 : 0.1354404129087925 \t -0.9047189354896545\n",
            "436 : 0.1365532547235489 \t -0.9289737939834595\n",
            "437 : 0.13564411848783492 \t -0.8057584166526794\n",
            "438 : 0.13603201881051064 \t -0.8263919949531555\n",
            "439 : 0.1363912720233202 \t -0.8651133179664612\n",
            "440 : 0.13725142925977707 \t -0.9203469157218933\n",
            "441 : 0.13660454526543617 \t -0.8596524000167847\n",
            "442 : 0.13776096515357494 \t -0.9347822070121765\n",
            "443 : 0.13700951933860778 \t -0.5309082269668579\n",
            "444 : 0.13789435774087905 \t -0.41576024889945984\n",
            "445 : 0.1380184419453144 \t -0.5440889000892639\n",
            "446 : 0.13721712231636046 \t -0.4259999990463257\n",
            "447 : 0.13844636529684068 \t -0.5422171354293823\n",
            "448 : 0.13937772810459137 \t -0.8865686058998108\n",
            "449 : 0.13747883960604668 \t -0.8355687260627747\n",
            "450 : 0.13746211417019366 \t -0.8807210326194763\n",
            "451 : 0.13663697056472301 \t -0.9056742191314697\n",
            "452 : 0.1390797823667526 \t -0.9440478682518005\n",
            "453 : 0.13698635958135127 \t -0.8696823120117188\n",
            "454 : 0.137650416046381 \t -0.8679136037826538\n",
            "455 : 0.13677389211952687 \t -0.8458406329154968\n",
            "456 : 0.13855335749685765 \t -0.8752949833869934\n",
            "457 : 0.13787982203066348 \t -0.8690896034240723\n",
            "458 : 0.13831980228424073 \t -0.8680774569511414\n",
            "459 : 0.13730528131127356 \t -0.8877708911895752\n",
            "460 : 0.13870040997862815 \t -0.9503780603408813\n",
            "461 : 0.13686037622392178 \t -0.9379051923751831\n",
            "462 : 0.137136060744524 \t -0.9499125480651855\n",
            "463 : 0.13653393946588038 \t -0.9416125416755676\n",
            "464 : 0.13626438304781913 \t -0.8329772353172302\n",
            "465 : 0.13532947152853012 \t -0.9106793999671936\n",
            "466 : 0.1357333704829216 \t -0.9497788548469543\n",
            "467 : 0.1348380085080862 \t -0.5320532917976379\n",
            "468 : 0.134093014895916 \t -0.41539764404296875\n",
            "469 : 0.13389644660055638 \t -0.5310948491096497\n",
            "470 : 0.1359236303716898 \t -0.4156884253025055\n",
            "471 : 0.13570110388100148 \t -0.5280188322067261\n",
            "472 : 0.13524273596704006 \t -0.40225228667259216\n",
            "473 : 0.1350158177316189 \t -0.5331145524978638\n",
            "474 : 0.13588864728808403 \t -0.4150446653366089\n",
            "475 : 0.13638029843568802 \t -0.5398628115653992\n",
            "476 : 0.13684299178421497 \t -0.9379677176475525\n",
            "477 : 0.1355502113699913 \t -0.9216269850730896\n",
            "478 : 0.136726588383317 \t -0.8859252333641052\n",
            "479 : 0.13513900190591813 \t -0.9192416667938232\n",
            "480 : 0.13565907031297683 \t -0.7857171893119812\n",
            "481 : 0.13407422080636025 \t -0.9206730127334595\n",
            "482 : 0.13521191142499447 \t -0.941981315612793\n",
            "483 : 0.13392225056886672 \t -0.877916693687439\n",
            "484 : 0.13459998555481434 \t -0.8609397411346436\n",
            "485 : 0.1337923403829336 \t -0.9295486807823181\n",
            "486 : 0.1337934322655201 \t -0.9095414876937866\n",
            "487 : 0.1331438824534416 \t -0.8543378114700317\n",
            "488 : 0.1359532091766596 \t -0.8899171352386475\n",
            "489 : 0.1340354848653078 \t -0.8627902865409851\n",
            "490 : 0.1347841639071703 \t -0.9534161686897278\n",
            "491 : 0.13386491276323795 \t -0.9285771250724792\n",
            "492 : 0.1333034783601761 \t -0.8840144276618958\n",
            "493 : 0.1339654315263033 \t -0.9139198064804077\n",
            "494 : 0.13470576144754887 \t -0.948906421661377\n",
            "495 : 0.13419993855059148 \t -0.8847836256027222\n",
            "496 : 0.1339169278740883 \t -0.9477707743644714\n",
            "497 : 0.13240718916058541 \t -0.8871673941612244\n",
            "498 : 0.13312047496438026 \t -0.9449330568313599\n",
            "499 : 0.13240463361144067 \t -0.8840891718864441\n",
            "500 : 0.13312002643942833 \t -0.9528710246086121\n",
            "501 : 0.13237470313906669 \t -0.9339258670806885\n",
            "502 : 0.13295036740601063 \t -0.9425003528594971\n",
            "503 : 0.13246973417699337 \t -0.906684160232544\n",
            "504 : 0.132361613586545 \t -0.8965591788291931\n",
            "505 : 0.13109763525426388 \t -0.8749606013298035\n",
            "506 : 0.13115429021418096 \t -0.9492070078849792\n",
            "507 : 0.13006306886672975 \t -0.9185835123062134\n",
            "508 : 0.13256371170282363 \t -0.9538211822509766\n",
            "509 : 0.13089665845036508 \t -0.9396560788154602\n",
            "510 : 0.13198479861021042 \t -0.8548241853713989\n",
            "511 : 0.13073913119733332 \t -0.9207260012626648\n",
            "512 : 0.13114162012934685 \t -0.953521728515625\n",
            "513 : 0.1302634183317423 \t -0.9184429049491882\n",
            "514 : 0.13190851211547852 \t -0.8900400996208191\n",
            "515 : 0.13091678395867348 \t -0.5422708988189697\n",
            "516 : 0.13100058436393738 \t -0.40979164838790894\n",
            "517 : 0.12968433685600758 \t -0.5369473099708557\n",
            "518 : 0.13105326555669308 \t -0.4083349108695984\n",
            "519 : 0.1305391348898411 \t -0.8147778511047363\n",
            "520 : 0.13028907626867295 \t -0.8258156776428223\n",
            "521 : 0.12932871095836163 \t -0.9000886678695679\n",
            "522 : 0.1301917266100645 \t -0.8893194198608398\n",
            "523 : 0.12855137325823307 \t -0.8760008811950684\n",
            "524 : 0.12976377494633198 \t -0.8895183801651001\n",
            "525 : 0.1292387902736664 \t -0.8952181935310364\n",
            "526 : 0.12927716486155988 \t -0.8877399563789368\n",
            "527 : 0.12918547615408899 \t -0.4300853908061981\n",
            "528 : 0.12972244434058666 \t -0.5397984981536865\n",
            "529 : 0.13060454837977886 \t -0.4148940443992615\n",
            "530 : 0.12999984212219715 \t -0.5427814722061157\n",
            "531 : 0.13025389425456524 \t -0.4250204265117645\n",
            "532 : 0.13007646650075913 \t -0.5210990309715271\n",
            "533 : 0.12969544678926467 \t -0.4128228425979614\n",
            "534 : 0.1299966037273407 \t -0.5325708389282227\n",
            "535 : 0.13074786886572837 \t -0.4127580225467682\n",
            "536 : 0.13110303394496442 \t -0.5399274230003357\n",
            "537 : 0.13135364428162574 \t -0.40288347005844116\n",
            "538 : 0.13210328295826912 \t -0.5526496171951294\n",
            "539 : 0.13235774524509908 \t -0.4016171395778656\n",
            "540 : 0.1318241387605667 \t -0.8965680599212646\n",
            "541 : 0.1316662948578596 \t -0.9495376348495483\n",
            "542 : 0.130832215026021 \t -0.9396895170211792\n",
            "543 : 0.13038148283958434 \t -0.8900232911109924\n",
            "544 : 0.12900367826223375 \t -0.9001132249832153\n",
            "545 : 0.12852882035076618 \t -0.9507638216018677\n",
            "546 : 0.12799510322511196 \t -0.8706705570220947\n",
            "547 : 0.12727926559746267 \t -0.9568896293640137\n",
            "548 : 0.12646761909127235 \t -0.916947603225708\n",
            "549 : 0.1253611661493778 \t -0.9320468902587891\n",
            "550 : 0.12568699680268763 \t -0.9373014569282532\n",
            "551 : 0.12481249570846557 \t -0.8315157890319824\n",
            "552 : 0.12446035034954547 \t -0.899185299873352\n",
            "553 : 0.12405285760760307 \t -0.954203724861145\n",
            "554 : 0.1234231848269701 \t -0.8966761231422424\n",
            "555 : 0.12264683954417706 \t -0.9589925408363342\n",
            "556 : 0.12287873513996601 \t -0.9035699367523193\n",
            "557 : 0.12251027897000313 \t -0.8949864506721497\n",
            "558 : 0.12187772393226623 \t -0.956729531288147\n",
            "559 : 0.12227969877421856 \t -0.9488620758056641\n",
            "560 : 0.12096229083836078 \t -0.9480146169662476\n",
            "561 : 0.12122689187526703 \t -0.902656078338623\n",
            "562 : 0.12024166472256184 \t -0.9431211352348328\n",
            "563 : 0.12066893205046654 \t -0.8981198072433472\n",
            "564 : 0.1199038177728653 \t -0.951334536075592\n",
            "565 : 0.12069116570055485 \t -0.9609962701797485\n",
            "566 : 0.11938041411340236 \t -0.938340425491333\n",
            "567 : 0.11953972354531288 \t -0.8924394845962524\n",
            "568 : 0.12014333792030811 \t -0.9527461528778076\n",
            "569 : 0.11994217298924922 \t -0.8450397253036499\n",
            "570 : 0.11941562853753566 \t -0.9329278469085693\n",
            "571 : 0.12006228789687157 \t -0.959156334400177\n",
            "572 : 0.11869627870619297 \t -0.9063484072685242\n",
            "573 : 0.11987445503473282 \t -0.9626128673553467\n",
            "574 : 0.11886511780321599 \t -0.9079736471176147\n",
            "575 : 0.1191967971622944 \t -0.9566968679428101\n",
            "576 : 0.11821075268089772 \t -0.901252806186676\n",
            "577 : 0.11873624548316002 \t -0.9048480987548828\n",
            "578 : 0.11830925159156322 \t -0.9533942341804504\n",
            "579 : 0.1181913398206234 \t -0.9542301893234253\n",
            "580 : 0.11742018535733223 \t -0.9465987682342529\n",
            "581 : 0.11798539459705353 \t -0.9558380842208862\n",
            "582 : 0.11727514900267125 \t -0.8967962861061096\n",
            "583 : 0.11778525821864605 \t -0.9548631310462952\n",
            "584 : 0.11762370094656945 \t -0.9593863487243652\n",
            "585 : 0.11706895232200623 \t -0.8998162150382996\n",
            "586 : 0.11690100133419037 \t -0.9132020473480225\n",
            "587 : 0.11706224195659161 \t -0.9270482063293457\n",
            "588 : 0.11648431271314622 \t -0.9616743922233582\n",
            "589 : 0.1171704575419426 \t -0.8692625761032104\n",
            "590 : 0.1164758786559105 \t -0.8960525989532471\n",
            "591 : 0.11708253771066665 \t -0.9554882049560547\n",
            "592 : 0.11753910519182682 \t -0.950343132019043\n",
            "593 : 0.11637717224657536 \t -0.8948951959609985\n",
            "594 : 0.1167992401868105 \t -0.9538969397544861\n",
            "595 : 0.1168505098670721 \t -0.9611225128173828\n",
            "596 : 0.11677044741809368 \t -0.9586396217346191\n",
            "597 : 0.1168970923870802 \t -0.9570066928863525\n",
            "598 : 0.11563485451042652 \t -0.9523449540138245\n",
            "599 : 0.11603474617004395 \t -0.9675967693328857\n",
            "600 : 0.11604632362723351 \t -0.9521641731262207\n",
            "601 : 0.1162609476596117 \t -0.934262752532959\n",
            "602 : 0.11638766601681709 \t -0.9635481834411621\n",
            "603 : 0.11618337854743004 \t -0.9684428572654724\n",
            "604 : 0.11595711261034011 \t -0.9406823515892029\n",
            "605 : 0.11636591777205467 \t -0.9110410213470459\n",
            "606 : 0.11547545902431011 \t -0.9491071701049805\n",
            "607 : 0.11589587703347207 \t -0.9643946886062622\n",
            "608 : 0.11531030759215355 \t -0.9684409499168396\n",
            "609 : 0.1148013148456812 \t -0.9605057239532471\n",
            "610 : 0.11539966389536857 \t -0.9118266701698303\n",
            "611 : 0.11448641233146191 \t -0.9613611102104187\n",
            "612 : 0.11425287686288357 \t -0.9551844596862793\n",
            "613 : 0.11569503508508205 \t -0.9628675580024719\n",
            "614 : 0.11450841426849365 \t -0.9135494232177734\n",
            "615 : 0.11467728205025196 \t -0.9622614979743958\n",
            "616 : 0.11517291739583016 \t -0.869678258895874\n",
            "617 : 0.11461331658065319 \t -0.9682344198226929\n",
            "618 : 0.11507312469184398 \t -0.9000278115272522\n",
            "619 : 0.11588002368807793 \t -0.5467087626457214\n",
            "620 : 0.11543252766132354 \t -0.5370591878890991\n",
            "621 : 0.11546485126018524 \t -0.41199439764022827\n",
            "622 : 0.11582602113485337 \t -0.5243066549301147\n",
            "623 : 0.11586811132729054 \t -0.4228835105895996\n",
            "624 : 0.11613609492778779 \t -0.4979410767555237\n",
            "625 : 0.11724303029477597 \t -0.4247990846633911\n",
            "626 : 0.11716173514723778 \t -0.5361149907112122\n",
            "627 : 0.11809122711420059 \t -0.43317118287086487\n",
            "628 : 0.11782242767512799 \t -0.9538952708244324\n",
            "629 : 0.11824100911617279 \t -0.831606388092041\n",
            "630 : 0.11808946914970875 \t -0.9520272612571716\n",
            "631 : 0.11940042600035668 \t -0.9027615189552307\n",
            "632 : 0.11899441033601761 \t -0.9081272482872009\n",
            "633 : 0.11883995197713375 \t -0.9063722491264343\n",
            "634 : 0.11913209892809391 \t -0.9384418725967407\n",
            "635 : 0.11965958252549172 \t -0.9342424273490906\n",
            "636 : 0.11901558637619018 \t -0.8776668310165405\n",
            "637 : 0.12101013585925102 \t -0.8998113870620728\n",
            "638 : 0.12074772007763386 \t -0.9504541754722595\n",
            "639 : 0.12011504359543324 \t -0.9036068916320801\n",
            "640 : 0.11970713920891285 \t -0.9439453482627869\n",
            "641 : 0.12031705230474472 \t -0.961938202381134\n",
            "642 : 0.11983935534954071 \t -0.9306225776672363\n",
            "643 : 0.12075552754104138 \t -0.8958637714385986\n",
            "644 : 0.120269038900733 \t -0.9417608380317688\n",
            "645 : 0.12097032181918621 \t -0.9596017003059387\n",
            "646 : 0.12017111964523793 \t -0.9087978005409241\n",
            "647 : 0.12084422595798969 \t -0.9601152539253235\n",
            "648 : 0.1204165305942297 \t -0.9553795456886292\n",
            "649 : 0.11999851688742638 \t -0.8411935567855835\n",
            "650 : 0.11991353034973144 \t -0.954504132270813\n",
            "651 : 0.12081353329122066 \t -0.9622067809104919\n",
            "652 : 0.12011495120823383 \t -0.9546299576759338\n",
            "653 : 0.11993376091122628 \t -0.968299388885498\n",
            "654 : 0.11956190280616283 \t -0.9524486660957336\n",
            "655 : 0.12008439302444458 \t -0.9115166664123535\n",
            "656 : 0.12009963504970074 \t -0.7961590886116028\n",
            "657 : 0.12061481773853303 \t -0.9141813516616821\n",
            "658 : 0.1191718976944685 \t -0.8841603994369507\n",
            "659 : 0.12020236887037754 \t -0.9550384283065796\n",
            "660 : 0.12026955783367158 \t -0.9668264389038086\n",
            "661 : 0.11947520263493061 \t -0.9512606263160706\n",
            "662 : 0.12052665799856185 \t -0.9017676115036011\n",
            "663 : 0.11975125223398209 \t -0.9565324187278748\n",
            "664 : 0.11988002471625805 \t -0.9602927565574646\n",
            "665 : 0.11951322443783283 \t -0.9641919136047363\n",
            "666 : 0.11845602318644524 \t -0.9628292918205261\n",
            "667 : 0.1189314492046833 \t -0.9471971392631531\n",
            "668 : 0.11973503455519677 \t -0.9646112322807312\n",
            "669 : 0.11864059753715991 \t -0.8612791895866394\n",
            "670 : 0.11897018253803253 \t -0.9045162796974182\n",
            "671 : 0.11872647777199745 \t -0.9050173163414001\n",
            "672 : 0.11929230988025666 \t -0.8988713026046753\n",
            "673 : 0.11874239705502987 \t -0.8739665150642395\n",
            "674 : 0.11875872053205967 \t -0.80999356508255\n",
            "675 : 0.11861848756670952 \t -0.9126823544502258\n",
            "676 : 0.11970912553369999 \t -0.9700616002082825\n",
            "677 : 0.11872937642037869 \t -0.9177085161209106\n",
            "678 : 0.1189836673438549 \t -0.9611981511116028\n",
            "679 : 0.11843368671834469 \t -0.929901659488678\n",
            "680 : 0.11929266862571239 \t -0.9611703157424927\n",
            "681 : 0.11881211400032043 \t -0.9383237361907959\n",
            "682 : 0.11980580762028695 \t -0.8996790051460266\n",
            "683 : 0.11878813989460468 \t -0.9446021318435669\n",
            "684 : 0.11909274496138096 \t -0.966736912727356\n",
            "685 : 0.1186027005314827 \t -0.9329817891120911\n",
            "686 : 0.11896020211279393 \t -0.9170083403587341\n",
            "687 : 0.11927907913923264 \t -0.9565407037734985\n",
            "688 : 0.11886182501912117 \t -0.960204541683197\n",
            "689 : 0.11841198094189168 \t -0.9572269916534424\n",
            "690 : 0.11873129606246949 \t -0.9674302339553833\n",
            "691 : 0.1179522480815649 \t -0.9576266407966614\n",
            "692 : 0.11891220547258854 \t -0.9441426992416382\n",
            "693 : 0.1177895687520504 \t -0.9553531408309937\n",
            "694 : 0.11939298771321774 \t -0.8500731587409973\n",
            "695 : 0.11784118004143238 \t -0.9521415829658508\n",
            "696 : 0.11876411363482475 \t -0.9651037454605103\n",
            "697 : 0.11801197193562984 \t -0.9077255725860596\n",
            "698 : 0.11805791668593883 \t -0.9050361514091492\n",
            "699 : 0.11861088909208775 \t -0.9087483882904053\n",
            "700 : 0.1183248445391655 \t -0.9360924959182739\n",
            "701 : 0.11762124821543693 \t -0.9164741635322571\n",
            "702 : 0.11779550015926361 \t -0.9709447622299194\n",
            "703 : 0.11745873875916005 \t -0.9585329294204712\n",
            "704 : 0.11762657761573792 \t -0.971549391746521\n",
            "705 : 0.11766819693148137 \t -0.9054135084152222\n",
            "706 : 0.11838454380631447 \t -0.9672046303749084\n",
            "707 : 0.1168865755200386 \t -0.9635063409805298\n",
            "708 : 0.11785653531551361 \t -0.9660901427268982\n",
            "709 : 0.11775736846029758 \t -0.9511308670043945\n",
            "710 : 0.11821644641458988 \t -0.9559565186500549\n",
            "711 : 0.11793169267475605 \t -0.9269384145736694\n",
            "712 : 0.11862591691315175 \t -0.964400053024292\n",
            "713 : 0.118328270688653 \t -0.9611493349075317\n",
            "714 : 0.11834363788366317 \t -0.945195734500885\n",
            "715 : 0.11737956069409847 \t -0.9572156071662903\n",
            "716 : 0.11763215251266956 \t -0.9567894339561462\n",
            "717 : 0.11760920025408268 \t -0.9634321928024292\n",
            "718 : 0.11783920526504517 \t -0.9550385475158691\n",
            "719 : 0.11680406220257282 \t -0.9563039541244507\n",
            "720 : 0.11693025939166546 \t -0.9087640643119812\n",
            "721 : 0.11640655510127544 \t -0.9665994644165039\n",
            "722 : 0.11633922643959523 \t -0.968157172203064\n",
            "723 : 0.11587857156991958 \t -0.8783683180809021\n",
            "724 : 0.11582306921482086 \t -0.9185208678245544\n",
            "725 : 0.11513170301914215 \t -0.958100438117981\n",
            "726 : 0.11505713388323784 \t -0.9741830229759216\n",
            "727 : 0.11508016027510166 \t -0.9686490297317505\n",
            "728 : 0.1147349551320076 \t -0.9732377529144287\n",
            "729 : 0.11351126842200757 \t -0.9126772284507751\n",
            "730 : 0.11499668322503567 \t -0.8835508227348328\n",
            "731 : 0.11394750662147998 \t -0.9207661151885986\n",
            "732 : 0.11530737392604351 \t -0.9092453122138977\n",
            "733 : 0.11481166183948517 \t -0.9677137136459351\n",
            "734 : 0.11428363472223282 \t -0.8984314799308777\n",
            "735 : 0.11346875764429569 \t -0.8715910315513611\n",
            "736 : 0.1132029078900814 \t -0.9581813812255859\n",
            "737 : 0.11406759694218635 \t -0.9678976535797119\n",
            "738 : 0.11348260641098022 \t -0.8525640964508057\n",
            "739 : 0.11321361772716046 \t -0.9610243439674377\n",
            "740 : 0.1131638515740633 \t -0.9043076038360596\n",
            "741 : 0.11334255151450634 \t -0.9653804898262024\n",
            "742 : 0.11288128718733788 \t -0.8580102920532227\n",
            "743 : 0.11291981525719166 \t -0.920930802822113\n",
            "744 : 0.11268995851278304 \t -0.9633740782737732\n",
            "745 : 0.11237332932651042 \t -0.9697532057762146\n",
            "746 : 0.11214694045484067 \t -0.9067013263702393\n",
            "747 : 0.1125147718936205 \t -0.9710627794265747\n",
            "748 : 0.11164539642632007 \t -0.968796968460083\n",
            "749 : 0.11224603280425072 \t -0.9541651606559753\n",
            "750 : 0.11337012015283107 \t -0.907961905002594\n",
            "751 : 0.11276673041284084 \t -0.9471539258956909\n",
            "752 : 0.11282597258687019 \t -0.9719486832618713\n",
            "753 : 0.11268821246922016 \t -0.9696869850158691\n",
            "754 : 0.11205635666847229 \t -0.9671944975852966\n",
            "755 : 0.11295773871243 \t -0.9603902697563171\n",
            "756 : 0.11212991438806057 \t -0.9737673401832581\n",
            "757 : 0.11200732849538327 \t -0.9660366773605347\n",
            "758 : 0.11176529675722122 \t -0.850227415561676\n",
            "759 : 0.11181402429938317 \t -0.968776285648346\n",
            "760 : 0.11171709783375264 \t -0.9739126563072205\n",
            "761 : 0.11139917150139808 \t -0.9219933152198792\n",
            "762 : 0.11126759052276611 \t -0.9763993620872498\n",
            "763 : 0.11125677973031997 \t -0.964919924736023\n",
            "764 : 0.11177504286170006 \t -0.9669545292854309\n",
            "765 : 0.11109620444476605 \t -0.9155310988426208\n",
            "766 : 0.11129740700125694 \t -0.9093042016029358\n",
            "767 : 0.11189827807247639 \t -0.9204760193824768\n",
            "768 : 0.11078106500208378 \t -0.9674681425094604\n",
            "769 : 0.1110578503459692 \t -0.9226952195167542\n",
            "770 : 0.11134935021400452 \t -0.919890820980072\n",
            "771 : 0.11113886795938015 \t -0.9456610083580017\n",
            "772 : 0.11207718923687934 \t -0.9739144444465637\n",
            "773 : 0.11100031435489655 \t -0.9638264179229736\n",
            "774 : 0.11126833893358708 \t -0.9708892107009888\n",
            "775 : 0.11048955321311951 \t -0.9031731486320496\n",
            "776 : 0.11087332032620907 \t -0.9634962677955627\n",
            "777 : 0.11086481921374798 \t -0.9571981430053711\n",
            "778 : 0.11057702004909516 \t -0.9677436947822571\n",
            "779 : 0.11121306456625461 \t -0.9625901579856873\n",
            "780 : 0.11090829595923424 \t -0.9692894220352173\n",
            "781 : 0.11137987300753593 \t -0.9611756801605225\n",
            "782 : 0.11099794544279576 \t -0.9132230877876282\n",
            "783 : 0.11071044690907002 \t -0.9597501158714294\n",
            "784 : 0.11066581718623639 \t -0.9594138860702515\n",
            "785 : 0.11146693341434002 \t -0.9660491943359375\n",
            "786 : 0.11098440289497376 \t -0.9348981380462646\n",
            "787 : 0.11109020449221134 \t -0.9650207757949829\n",
            "788 : 0.11148647107183933 \t -0.9624897837638855\n",
            "789 : 0.11136548705399037 \t -0.9711019992828369\n",
            "790 : 0.11003870069980622 \t -0.9032281041145325\n",
            "791 : 0.1110712181776762 \t -0.9698015451431274\n",
            "792 : 0.11101864948868752 \t -0.9180075526237488\n",
            "793 : 0.11062377654016017 \t -0.8527138233184814\n",
            "794 : 0.11118503771722317 \t -0.9673876762390137\n",
            "795 : 0.1111394263803959 \t -0.9712619781494141\n",
            "796 : 0.1106189839541912 \t -0.9725210070610046\n",
            "797 : 0.11111891120672227 \t -0.9724206924438477\n",
            "798 : 0.11094855405390262 \t -0.8829215168952942\n",
            "799 : 0.110810412093997 \t -0.9713867902755737\n",
            "800 : 0.11050154976546764 \t -0.8819907307624817\n",
            "801 : 0.11067375093698502 \t -0.9657835960388184\n",
            "802 : 0.11055846288800239 \t -0.9060831069946289\n",
            "803 : 0.11019263751804828 \t -0.9710849523544312\n",
            "804 : 0.10983275584876537 \t -0.8945353627204895\n",
            "805 : 0.10998646728694439 \t -0.9650796055793762\n",
            "806 : 0.11011902876198292 \t -0.7826806306838989\n",
            "807 : 0.10969100594520569 \t -0.9620237350463867\n",
            "808 : 0.11000802181661129 \t -0.8973631262779236\n",
            "809 : 0.10893266126513482 \t -0.9461090564727783\n",
            "810 : 0.1093351799994707 \t -0.8715178966522217\n",
            "811 : 0.10995660237967968 \t -0.9608479142189026\n",
            "812 : 0.10984470993280411 \t -0.8351123332977295\n",
            "813 : 0.10942336730659008 \t -0.969150185585022\n",
            "814 : 0.10921027511358261 \t -0.8967339396476746\n",
            "815 : 0.1089098297059536 \t -0.9664523005485535\n",
            "816 : 0.1088484600186348 \t -0.8895778059959412\n",
            "817 : 0.10886284783482551 \t -0.9712028503417969\n",
            "818 : 0.10894418619573117 \t -0.8850648999214172\n",
            "819 : 0.10849620476365089 \t -0.8492838740348816\n",
            "820 : 0.10953293330967426 \t -0.7814203500747681\n",
            "821 : 0.10909250788390637 \t -0.9689779877662659\n",
            "822 : 0.10836800709366798 \t -0.826144814491272\n",
            "823 : 0.10883349403738976 \t -0.904304563999176\n",
            "824 : 0.10904650911688804 \t -0.8467512726783752\n",
            "825 : 0.10971836522221565 \t -0.9711008667945862\n",
            "826 : 0.10913977958261967 \t -0.8844146728515625\n",
            "827 : 0.10932436920702457 \t -0.9713809490203857\n",
            "828 : 0.10947084911167622 \t -0.8065072894096375\n",
            "829 : 0.10974009297788143 \t -0.8945356011390686\n",
            "830 : 0.11010128036141395 \t -0.8308737874031067\n",
            "831 : 0.10910964384675026 \t -0.9506863951683044\n",
            "832 : 0.10930415950715541 \t -0.8875101208686829\n",
            "833 : 0.10988055840134621 \t -0.9618610143661499\n",
            "834 : 0.10889847427606583 \t -0.8953297734260559\n",
            "835 : 0.10922663174569607 \t -0.9596113562583923\n",
            "836 : 0.10867811404168606 \t -0.8913382291793823\n",
            "837 : 0.10925241149961948 \t -0.903544008731842\n",
            "838 : 0.10837192125618458 \t -0.8376814126968384\n",
            "839 : 0.10881347879767418 \t -0.8996445536613464\n",
            "840 : 0.10897647477686405 \t -0.8376914262771606\n",
            "841 : 0.10905539132654667 \t -0.9651798605918884\n",
            "842 : 0.10894212052226067 \t -0.8880163431167603\n",
            "843 : 0.10861248597502708 \t -0.9667360186576843\n",
            "844 : 0.10865374878048897 \t -0.8927087783813477\n",
            "845 : 0.10845538154244423 \t -0.9741853475570679\n",
            "846 : 0.10887879468500614 \t -0.8876450657844543\n",
            "847 : 0.10892729088664055 \t -0.903960108757019\n",
            "848 : 0.10857180766761303 \t -0.8976590633392334\n",
            "849 : 0.1087634202092886 \t -0.9726749658584595\n",
            "850 : 0.10820862539112568 \t -0.8939517736434937\n",
            "851 : 0.10800504684448242 \t -0.96954745054245\n",
            "852 : 0.10846786238253117 \t -0.8827136754989624\n",
            "853 : 0.10893890112638474 \t -0.9038925170898438\n",
            "854 : 0.10814813561737538 \t -0.8984915018081665\n",
            "855 : 0.10757180042564869 \t -0.907442569732666\n",
            "856 : 0.10807659924030304 \t -0.9031765460968018\n",
            "857 : 0.10836688689887523 \t -0.9707855582237244\n",
            "858 : 0.10799585245549678 \t -0.8444746136665344\n",
            "859 : 0.10726561062037945 \t -0.9681442379951477\n",
            "860 : 0.10720761641860008 \t -0.8462585806846619\n",
            "861 : 0.10806242786347867 \t -0.9006385207176208\n",
            "862 : 0.10826485604047775 \t -0.8959349393844604\n",
            "863 : 0.1066095557063818 \t -0.9427934288978577\n",
            "864 : 0.10762435160577297 \t -0.9010401368141174\n",
            "865 : 0.10763285234570504 \t -0.9279812574386597\n",
            "866 : 0.10717477835714817 \t -0.8978809714317322\n",
            "867 : 0.10763295590877534 \t -0.9725170731544495\n",
            "868 : 0.10731794387102127 \t -0.797630250453949\n",
            "869 : 0.107236947119236 \t -0.9665927886962891\n",
            "870 : 0.10831871777772903 \t -0.8994715809822083\n",
            "871 : 0.10751018933951854 \t -0.9695258736610413\n",
            "872 : 0.10704905018210412 \t -0.844998836517334\n",
            "873 : 0.10725183673202991 \t -0.966346025466919\n",
            "874 : 0.10731158852577209 \t -0.8468248844146729\n",
            "875 : 0.1076097048819065 \t -0.8959345817565918\n",
            "876 : 0.10797102712094783 \t -0.9013861417770386\n",
            "877 : 0.1067100640386343 \t -0.9041609764099121\n",
            "878 : 0.10631413497030735 \t -0.9023034572601318\n",
            "879 : 0.10744654573500156 \t -0.9028962254524231\n",
            "880 : 0.10730415247380734 \t -0.8939427733421326\n",
            "881 : 0.10741124711930752 \t -0.9694988131523132\n",
            "882 : 0.10706774443387986 \t -0.8904691934585571\n",
            "883 : 0.10693539045751095 \t -0.9512105584144592\n",
            "884 : 0.10768799409270287 \t -0.8972679972648621\n",
            "885 : 0.10766801983118057 \t -0.9648922681808472\n",
            "886 : 0.10765150561928749 \t -0.9043264389038086\n",
            "887 : 0.10664360336959362 \t -0.9703115224838257\n",
            "888 : 0.10704231522977352 \t -0.9017041325569153\n",
            "889 : 0.10656018704175949 \t -0.9714881777763367\n",
            "890 : 0.10732017382979393 \t -0.9016111493110657\n",
            "891 : 0.10719793066382408 \t -0.9135787487030029\n",
            "892 : 0.1075743205845356 \t -0.9062902927398682\n",
            "893 : 0.10740107335150242 \t -0.9058386087417603\n",
            "894 : 0.10751200430095195 \t -0.8989827036857605\n",
            "895 : 0.10692810006439686 \t -0.8974190950393677\n",
            "896 : 0.10715789422392845 \t -0.9048978090286255\n",
            "897 : 0.10758871324360371 \t -0.9720262289047241\n",
            "898 : 0.10759886726737022 \t -0.8970502614974976\n",
            "899 : 0.10781629830598831 \t -0.9728090167045593\n",
            "900 : 0.10705510601401329 \t -0.9026268720626831\n",
            "901 : 0.1072337806224823 \t -0.9715239405632019\n",
            "902 : 0.10689728893339634 \t -0.8434668183326721\n",
            "903 : 0.1072866216301918 \t -0.9665019512176514\n",
            "904 : 0.10652631260454655 \t -0.8510950207710266\n",
            "905 : 0.10709516294300556 \t -0.9038367867469788\n",
            "906 : 0.10729614794254302 \t -0.8992270827293396\n",
            "907 : 0.1065522488206625 \t -0.9753121733665466\n",
            "908 : 0.10662199631333351 \t -0.8987459540367126\n",
            "909 : 0.1067513782531023 \t -0.9223152995109558\n",
            "910 : 0.1071652639657259 \t -0.8416143655776978\n",
            "911 : 0.10705635771155357 \t -0.9728192090988159\n",
            "912 : 0.10728925317525864 \t -0.8493426442146301\n",
            "913 : 0.10761284902691841 \t -0.9699022173881531\n",
            "914 : 0.10696199499070644 \t -0.8963718414306641\n",
            "915 : 0.10733118504285813 \t -0.9028660655021667\n",
            "916 : 0.10681858770549298 \t -0.8484278321266174\n",
            "917 : 0.1073833804577589 \t -0.8941279053688049\n",
            "918 : 0.10664595514535904 \t -0.954659104347229\n",
            "919 : 0.10665694959461688 \t -0.8997477293014526\n",
            "920 : 0.10689727291464805 \t -0.9642966985702515\n",
            "921 : 0.10706096775829792 \t -0.8426189422607422\n",
            "922 : 0.10700066462159157 \t -0.9748655557632446\n",
            "923 : 0.10717094764113426 \t -0.8975872993469238\n",
            "924 : 0.10670313462615014 \t -0.9645968079566956\n",
            "925 : 0.10621084198355675 \t -0.898780345916748\n",
            "926 : 0.10616164840757847 \t -0.973554790019989\n",
            "927 : 0.10689157880842685 \t -0.8971740007400513\n",
            "928 : 0.1065319873392582 \t -0.9646778106689453\n",
            "929 : 0.10683049038052558 \t -0.8673707842826843\n",
            "930 : 0.10640550181269645 \t -0.9736685156822205\n",
            "931 : 0.10630523040890694 \t -0.8334826231002808\n",
            "932 : 0.1065462116152048 \t -0.9727598428726196\n",
            "933 : 0.10639942549169064 \t -0.9058335423469543\n",
            "934 : 0.10754798017442227 \t -0.9052302837371826\n",
            "935 : 0.10606281533837318 \t -0.8959544897079468\n",
            "936 : 0.106971637904644 \t -0.969437301158905\n",
            "937 : 0.10675037764012814 \t -0.8441780805587769\n",
            "938 : 0.10585406385362148 \t -0.8524602055549622\n",
            "939 : 0.10628789961338043 \t -0.8491100668907166\n",
            "940 : 0.10617105476558208 \t -0.9031674861907959\n",
            "941 : 0.10643322952091694 \t -0.8958649635314941\n",
            "942 : 0.10692529864609242 \t -0.8935669660568237\n",
            "943 : 0.10669769495725631 \t -0.8963094353675842\n",
            "944 : 0.10602820925414562 \t -0.9721131920814514\n",
            "945 : 0.1060959231108427 \t -0.9050592184066772\n",
            "946 : 0.10661805048584938 \t -0.8947705626487732\n",
            "947 : 0.10613039098680019 \t -0.9001383781433105\n",
            "948 : 0.10589310266077519 \t -0.9627829194068909\n",
            "949 : 0.10617536418139935 \t -0.8053917288780212\n",
            "950 : 0.10593833327293396 \t -0.9006259441375732\n",
            "951 : 0.10589651092886924 \t -0.9005153179168701\n",
            "952 : 0.10648020543158054 \t -0.9680035710334778\n",
            "953 : 0.10665803141891957 \t -0.8997191786766052\n",
            "954 : 0.10628528781235218 \t -0.9057245850563049\n",
            "955 : 0.10673942901194096 \t -0.9049243927001953\n",
            "956 : 0.10574721284210682 \t -0.9685818552970886\n",
            "957 : 0.10568334981799125 \t -0.8978651762008667\n",
            "958 : 0.10621823593974114 \t -0.9712079167366028\n",
            "959 : 0.10614031851291657 \t -0.8512088060379028\n",
            "960 : 0.10600451529026031 \t -0.975513756275177\n",
            "961 : 0.10630515888333321 \t -0.8402810096740723\n",
            "962 : 0.10617725141346454 \t -0.9649735689163208\n",
            "963 : 0.10526419952511787 \t -0.9013722538948059\n",
            "964 : 0.10575489178299904 \t -0.9718235731124878\n",
            "965 : 0.10629216469824314 \t -0.8981061577796936\n",
            "966 : 0.1057162918150425 \t -0.9026457667350769\n",
            "967 : 0.1055015966296196 \t -0.8974730372428894\n",
            "968 : 0.10575384646654129 \t -0.973157525062561\n",
            "969 : 0.10569445751607418 \t -0.8956084847450256\n",
            "970 : 0.10558187514543534 \t -0.9060346484184265\n",
            "971 : 0.10589786060154438 \t -0.8563278913497925\n",
            "972 : 0.1058152973651886 \t -0.9675841331481934\n",
            "973 : 0.10550766251981258 \t -0.892470121383667\n",
            "974 : 0.10579802803695201 \t -0.9076371788978577\n",
            "975 : 0.10607091784477234 \t -0.8433030843734741\n",
            "976 : 0.10608921013772488 \t -0.9706296920776367\n",
            "977 : 0.10592188090085983 \t -0.8999475836753845\n",
            "978 : 0.10570013150572777 \t -0.9700455069541931\n",
            "979 : 0.10521324649453163 \t -0.8460736274719238\n",
            "980 : 0.10495901070535182 \t -0.9747363924980164\n",
            "981 : 0.10582998506724835 \t -0.8410331010818481\n",
            "982 : 0.10577005855739116 \t -0.9767674803733826\n",
            "983 : 0.1057514812797308 \t -0.8444921374320984\n",
            "984 : 0.10564249604940415 \t -0.9597325325012207\n",
            "985 : 0.10597998537123203 \t -0.9039987921714783\n",
            "986 : 0.10625931769609451 \t -0.970265805721283\n",
            "987 : 0.10545601323246956 \t -0.9067023396492004\n",
            "988 : 0.10544550195336341 \t -0.9697499871253967\n",
            "989 : 0.1049691952764988 \t -0.9053639769554138\n",
            "990 : 0.10545304156839848 \t -0.9632558226585388\n",
            "991 : 0.10534666627645492 \t -0.8940467238426208\n",
            "992 : 0.10560007207095623 \t -0.9762367010116577\n",
            "993 : 0.10597760826349259 \t -0.9033111929893494\n",
            "994 : 0.10584391467273235 \t -0.9632735848426819\n",
            "995 : 0.10583797581493855 \t -0.9030991196632385\n",
            "996 : 0.10576443858444691 \t -0.9717350602149963\n",
            "997 : 0.10487394109368324 \t -0.9020283222198486\n",
            "998 : 0.10605920776724816 \t -0.970329999923706\n",
            "999 : 0.10535824336111546 \t -0.9058049321174622\n",
            "IDSGAN finish training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "boSeuM3IQ3Tk",
        "outputId": "0819940d-99a4-40ad-e210-75a05a12b618"
      },
      "source": [
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from preprocessing import preprocess4, create_batch2\n",
        "from model.model_class import Generator\n",
        "from torch import nn\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(12345)\n",
        "\n",
        "test = pd.read_csv(\"dataset/generator_test_combined.csv\")\n",
        "\n",
        "#functional_categories = ['intrinsic', 'time_based']\n",
        "#functional_categories = ['intrinsic', 'time_based', 'host_based']\n",
        "functional_categories = ['intrinsic', 'content']\n",
        "\n",
        "test, raw_attack, _, _, modification_mask = preprocess4(test, functional_categories)\n",
        "\n",
        "raw_attack = np.array(raw_attack).astype(float)\n",
        "\n",
        "modification_mask = np.tile(modification_mask, [len(raw_attack), 1])\n",
        "modification_mask_t = torch.FloatTensor(modification_mask)\n",
        "\n",
        "NOISE_SIZE = 27\n",
        "generator_input_dim = test.shape[1] + NOISE_SIZE\n",
        "generator_output_dim = test.shape[1]\n",
        "discriminator_output_dim = 1\n",
        "\n",
        "random_g = Generator(generator_input_dim, generator_output_dim)\n",
        "learned_g = Generator(generator_input_dim, generator_output_dim)\n",
        "\n",
        "# ids_model = Blackbox_IDS(generator_output_dim, 1)\n",
        "# param = torch.load('model/IDS.pth')\n",
        "# ids_model.load_state_dict(param)\n",
        "ids_model = BlackBoxWrapper(generator_output_dim, 1)\n",
        "\n",
        "g_param = torch.load('save_model/generator.pth')\n",
        "learned_g.load_state_dict(g_param)\n",
        "\n",
        "# original detection rate\n",
        "\n",
        "x_t = torch.FloatTensor(raw_attack).to(device)\n",
        "\n",
        "out = ids_model(x_t)\n",
        "out = torch.reshape(out, [len(raw_attack)])\n",
        "out_np = out.cpu().detach().numpy()\n",
        "\n",
        "ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "corr = np.sum(ids_pred_label)\n",
        "\n",
        "original_det_rate = corr / len(raw_attack)\n",
        "\n",
        "print(\"original detection rate : {}\".format(original_det_rate))\n",
        "\n",
        "# adv. detection rate : random generator\n",
        "\n",
        "noise = np.random.uniform(0., 1., (len(raw_attack), NOISE_SIZE))\n",
        "gen_x = np.concatenate([raw_attack, noise], axis=1)\n",
        "\n",
        "gen_x_t = torch.FloatTensor(gen_x)\n",
        "\n",
        "adversarial_attack_modification = random_g(gen_x_t, modification_mask_t)\n",
        "adversarial_attack = torch.FloatTensor(raw_attack) + adversarial_attack_modification\n",
        "\n",
        "out = ids_model(adversarial_attack)\n",
        "out = torch.reshape(out, [len(adversarial_attack)])\n",
        "out_np = out.cpu().detach().numpy()\n",
        "\n",
        "# print(adversarial_attack_modification[0])\n",
        "# print(adversarial_attack_modification[1])\n",
        "\n",
        "ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "corr = np.sum(ids_pred_label)\n",
        "\n",
        "adv_det_rate_rand = corr / len(raw_attack)\n",
        "\n",
        "print(\"adversarial detection rate (random): {}\".format(adv_det_rate_rand))\n",
        "\n",
        "# adv. detection rate : learned generator\n",
        "\n",
        "adversarial_attack_modification_l = learned_g(gen_x_t, modification_mask_t)\n",
        "adversarial_attack_l = torch.FloatTensor(raw_attack) + adversarial_attack_modification_l\n",
        "\n",
        "#print(adversarial_attack_modification_l[0])\n",
        "# print(adversarial_attack_modification[1])\n",
        "\n",
        "out = ids_model(adversarial_attack_l)\n",
        "out = torch.reshape(out, [len(adversarial_attack_l)])\n",
        "out_np = out.cpu().detach().numpy()\n",
        "\n",
        "print(np.min(adversarial_attack_modification_l.cpu().detach().numpy(), axis=0))\n",
        "print(np.max(adversarial_attack_modification_l.cpu().detach().numpy(), axis=0))\n",
        "\n",
        "\n",
        "# print(np.sum(np.abs(adversarial_attack_l.detach().numpy() - raw_attack) < 0.001)/len(raw_attack))\n",
        "ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "corr = np.sum(ids_pred_label)\n",
        "\n",
        "adv_det_rate_learned = corr / len(raw_attack)\n",
        "\n",
        "print(\"adversarial detection rate (learned): {}\".format(adv_det_rate_learned))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original detection rate : 0.03656059580230196\n",
            "adversarial detection rate (random): 0.037237643872714964\n",
            "[ 0.          0.         -0.         -0.         -0.         -0.\n",
            "  0.         -0.         -0.         -0.         -0.          0.\n",
            " -0.         -0.          0.         -0.         -0.         -0.\n",
            "  0.          0.         -0.         -0.          0.          0.\n",
            "  0.         -0.          0.          0.         -0.         -0.\n",
            " -0.         -0.         -0.          0.          0.          0.\n",
            "  0.          0.         -0.          0.         -0.         -0.\n",
            "  0.         -0.          0.         -0.         -0.         -0.\n",
            " -0.         -0.          0.          0.         -0.         -0.\n",
            " -0.         -0.          0.         -0.          0.          0.\n",
            "  0.         -0.         -0.          0.         -0.          0.\n",
            " -0.         -0.         -0.         -0.          0.         -0.\n",
            " -0.         -0.          0.          0.          0.          0.\n",
            " -0.          0.         -0.         -0.         -0.          0.\n",
            " -0.         -0.         -0.         -0.          0.         -0.\n",
            " -0.          0.          0.         -0.          0.         -0.\n",
            "  0.          0.         -0.          0.         -0.         -0.\n",
            " -0.         -0.5        -0.5        -0.5        -0.5        -0.5\n",
            " -0.5        -0.02006928 -0.21364625 -0.5        -0.5         0.21518049\n",
            " -0.13206878 -0.16104    -0.5        -0.5        -0.5        -0.5\n",
            " -0.5        -0.5       ]\n",
            "[ 0.          0.         -0.         -0.         -0.         -0.\n",
            "  0.         -0.         -0.         -0.         -0.          0.\n",
            " -0.         -0.          0.         -0.         -0.         -0.\n",
            "  0.          0.         -0.         -0.          0.          0.\n",
            "  0.         -0.          0.          0.         -0.         -0.\n",
            " -0.         -0.         -0.          0.          0.          0.\n",
            "  0.          0.         -0.          0.         -0.         -0.\n",
            "  0.         -0.          0.         -0.         -0.         -0.\n",
            " -0.         -0.          0.          0.         -0.         -0.\n",
            " -0.         -0.          0.         -0.          0.          0.\n",
            "  0.         -0.         -0.          0.         -0.          0.\n",
            " -0.         -0.         -0.         -0.          0.         -0.\n",
            " -0.         -0.          0.          0.          0.          0.\n",
            " -0.          0.         -0.         -0.         -0.          0.\n",
            " -0.         -0.         -0.         -0.          0.         -0.\n",
            " -0.          0.          0.         -0.          0.         -0.\n",
            "  0.          0.         -0.          0.         -0.         -0.\n",
            " -0.          0.32107615  0.5         0.21617237 -0.19648881 -0.08392335\n",
            "  0.10324409  0.5         0.5         0.22585677  0.17373215  0.5\n",
            "  0.5         0.5        -0.1620095   0.01969292  0.00714048  0.22085492\n",
            "  0.04752804  0.37982655]\n",
            "adversarial detection rate (learned): 0.012186865267433988\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QO-q73wVkEH_"
      },
      "source": [
        "#Fuzzyness IDS\n",
        "## 100% Labeled\n",
        "original detection rate : 0.8531777956556718  \n",
        "adversarial detection rate (random): 0.8323947438991687  \n",
        "adversarial detection rate (learned): 0.6423974255832663  \n",
        "\n",
        "## 90% Label\n",
        "original detection rate : 0.8393671225529633  \n",
        "adversarial detection rate (random): 0.8356127648163046  \n",
        "adversarial detection rate (learned): 0.6158487530168946  \n",
        "\n",
        "## 50% Label\n",
        "original detection rate : 0.8454009117725931  \n",
        "adversarial detection rate (random): 0.8472780906409225    \n",
        "adversarial detection rate (learned): 0.5922499329578975\n",
        "\n",
        "## 10% Label\n",
        "original detection rate : 0.8581389112362564  \n",
        "adversarial detection rate (random): 0.8565299007776884    \n",
        "adversarial detection rate (learned): 0.6773934030571199  \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDdcAmJkRpTc"
      },
      "source": [
        "#Vanilla NN as BlackBoxIDS\n",
        "original detection rate : 0.8175113971574148   \n",
        "adversarial detection rate (random): 0.8205953338696702   \n",
        "adversarial detection rate (learned): 0.27715205148833466"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ztYSjXtQUkaA"
      },
      "source": [
        "#RNN as BlackBoxIDS\n",
        "## 100% Data\n",
        "original detection rate : 0.8765084473049075  \n",
        "adversarial detection rate (random): 0.8679270581925449  \n",
        "adversarial detection rate (learned): 0.6790024135156878    \n",
        "\n",
        "## 90% Data\n",
        "original detection rate : 0.8207294180745508  \n",
        "adversarial detection rate (random): 0.8172432287476535    \n",
        "adversarial detection rate (learned): 0.6533923303834809  \n",
        "\n",
        "## 50% Data\n",
        "original detection rate : 0.8576025744167337  \n",
        "adversarial detection rate (random): 0.857870742826495    \n",
        "adversarial detection rate (learned): 0.6310002681684097    \n",
        "\n",
        "## 10% Data\n",
        "original detection rate : 0.8527755430410298  \n",
        "adversarial detection rate (random): 0.8487530168946098    \n",
        "adversarial detection rate (learned): 0.6924108340037544    \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SW3zpqJIbiBT"
      },
      "source": [
        "#Our IDS\n",
        "\n",
        "##Trained with 100% Labelled Data\n",
        "original detection rate : 0.8425851434700993  \n",
        "adversarial detection rate (random): 0.8411102172164119  \n",
        "adversarial detection rate (learned): 0.7139983909895414\n",
        "\n",
        "##Trained with 90% Labelled Data\n",
        "original detection rate : 0.8228747653526415  \n",
        "adversarial detection rate (random): 0.8169750603378922  \n",
        "adversarial detection rate (learned): 0.7294180745508179\n",
        "\n",
        "##Trained with 50% Labelled Data\n",
        "original detection rate : 0.8472780906409225  \n",
        "adversarial detection rate (random): 0.8472780906409225  \n",
        "adversarial detection rate (learned): 0.7687047465808527  \n",
        "\n",
        "##Trained with 10% Labelled Data\n",
        "original detection rate : 0.8336015017430947  \n",
        "adversarial detection rate (random): 0.8148297130598016  \n",
        "adversarial detection rate (learned): 0.7959238401716278"
      ]
    }
  ]
}