{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imIS9ti1bsfN"
      },
      "source": [
        "#Steps:\n",
        "\n",
        "\n",
        "1. Copy paste variant to \"Blackbox NIDS Trainer\" Section.\n",
        "\n",
        "2. Set label ratio variable (around line 13)\n",
        "\n",
        "3. Check out epoch no.s (at around line 27-29)\n",
        "\n",
        "4. Delete import from nids_blackbox_data_generator (UPDATED: DO NOT)\n",
        "\n",
        "5. Uncomment train_model() if you want to train\n",
        "\n",
        "6. Update attack type at \"Black Box Wrapper\" section\n",
        "\n",
        "7. Update attack type at \"IDSGAN Data Loader\" section (line 48)\n",
        "\n",
        "8. Update functional_categories for attack type in \"IDSGAN Train\" section\n",
        "\n",
        "9. Update functional_categories for attack type in \"IDSGAN Result Generation\" section.\n",
        "\n",
        "10. Repeat steps (5-8) for different attack types.\n",
        "\n",
        "-- Special Tip: Also check out the generated result after training at the end of the blackbox NIDS trainer section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sJCxDptFaP6"
      },
      "source": [
        "# Primary Loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXIN8sL58J-D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75b9d40a-9bdd-4ef2-e923-dc11c59118f0"
      },
      "source": [
        "! git clone https://github.com/tzpranto/NIDS_NSYSS_23"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'idsgan18'...\n",
            "remote: Enumerating objects: 668, done.\u001b[K\n",
            "remote: Counting objects: 100% (668/668), done.\u001b[K\n",
            "remote: Compressing objects: 100% (592/592), done.\u001b[K\n",
            "remote: Total 668 (delta 78), reused 635 (delta 63), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (668/668), 38.46 MiB | 8.16 MiB/s, done.\n",
            "Resolving deltas: 100% (78/78), done.\n",
            "Checking out files: 100% (347/347), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEqbPzhV9B8t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08a87ae9-3c43-4281-e13b-cbae1c765b1c"
      },
      "source": [
        "cd https://github.com/tzpranto/NIDS_NSYSS_23/idsgan18/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/idsgan18\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZXD9Ivpg9EqV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "88cd3689-84ec-4e41-a6a6-ec012225623d"
      },
      "source": [
        "!ls -l"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 96\n",
            "-rw-r--r--  1 root root  3083 Oct  1 10:14 BlackBox_IDS.py\n",
            "-rw-r--r--  1 root root   887 Oct  1 10:14 blackbox_wrapper.py\n",
            "drwxr-xr-x  2 root root  4096 Oct  1 10:14 clustering\n",
            "drwxr-xr-x  2 root root  4096 Oct  1 10:14 dataset\n",
            "-rw-r--r--  1 root root  5346 Oct  1 10:14 idsgan_data_loader.py\n",
            "-rw-r--r--  1 root root  7032 Oct  1 10:14 idsgan_preprocessor.py\n",
            "-rw-r--r--  1 root root  2543 Oct  1 10:14 idsgan_result_generation.py\n",
            "-rw-r--r--  1 root root  7324 Oct  1 10:14 idsgan_train.py\n",
            "drwxr-xr-x  3 root root  4096 Oct  1 10:14 model\n",
            "drwxr-xr-x 30 root root  4096 Oct  1 10:14 models\n",
            "-rw-r--r--  1 root root  6867 Oct  1 10:14 nids_blackbox_data_generator.py\n",
            "-rw-r--r--  1 root root 28321 Oct  1 10:14 nids_loader.py\n",
            "drwxr-xr-x  2 root root  4096 Oct  1 10:14 __pycache__\n",
            "drwxr-xr-x  2 root root  4096 Oct  1 10:14 save_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Enl_la3qxb02",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5a353a6-b7a9-45a9-e104-e1bf5c9deddb"
      },
      "source": [
        "! pip install scikit-learn==0.23.1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting scikit-learn==0.23.1\n",
            "  Downloading scikit_learn-0.23.1-cp37-cp37m-manylinux1_x86_64.whl (6.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8 MB 2.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1) (1.19.5)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-2.2.0-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.23.1) (1.0.1)\n",
            "Installing collected packages: threadpoolctl, scikit-learn\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 0.22.2.post1\n",
            "    Uninstalling scikit-learn-0.22.2.post1:\n",
            "      Successfully uninstalled scikit-learn-0.22.2.post1\n",
            "Successfully installed scikit-learn-0.23.1 threadpoolctl-2.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_jwRGOoEsXu"
      },
      "source": [
        "# NIDS Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgQRFDve9jbt"
      },
      "source": [
        "import os\n",
        "import pickle\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import KMeans\n",
        "import seaborn as sns\n",
        "import matplotlib.patheffects as PathEffects\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from sklearn.metrics import pairwise_distances_argmin_min, pairwise_distances\n",
        "\n",
        "from nids_blackbox_data_generator import get_training_data, dataset_test\n",
        "\n",
        "debug = False\n",
        "method = 'km'\n",
        "label_ratio = 0.50\n",
        "cluster_centroid_ratio = 100\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "random.seed(12345)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "total_dataset, labeled_dataset, unlabeled_dataset = get_training_data(no_samples=125973, label_ratio=label_ratio)\n",
        "cat_dict = total_dataset.get_cat_dict()\n",
        "print(cat_dict)\n",
        "test_dataset = dataset_test(cat_dict)\n",
        "\n",
        "num_data = total_dataset.get_x()\n",
        "labels = total_dataset.get_y()\n",
        "\n",
        "# pca = PCA(n_components=2)\n",
        "# pca_result = pca.fit_transform(num_data)\n",
        "# pca_result_test = pca.transform(test_dataset.get_x())\n",
        "# print(\"Pca done!\")\n",
        "#\n",
        "#\n",
        "# def get_scatter(x, colors):\n",
        "#     # choose a color palette with seaborn.\n",
        "#     fig, ax = plt.subplots()\n",
        "#     classes = np.unique(colors)\n",
        "#     for c in classes:\n",
        "#         x_, y_ = x[colors == c], c\n",
        "#         ax.scatter(x_[:, 0], x_[:, 1], label=y_, lw=0, s=40)\n",
        "#\n",
        "#     ax.legend()\n",
        "#     plt.show()\n",
        "#\n",
        "#\n",
        "# get_scatter(pca_result, labels)\n",
        "\n",
        "total_original_label_counts = dict()\n",
        "distinct_labels, distinct_label_counts = np.unique(labels, return_counts=True)\n",
        "\n",
        "for i in range(len(distinct_labels)):\n",
        "    if distinct_labels[i] != -1:\n",
        "        total_original_label_counts[distinct_labels[i]] = distinct_label_counts[i]\n",
        "\n",
        "\n",
        "def tree_work(load_cluster_from_file=False):\n",
        "    if load_cluster_from_file:\n",
        "        clustering = pickle.load(\n",
        "            file=open(os.path.join('clustering', 'nslkdd' + \"_\" + str(cluster_centroid_ratio) + \".pkl\"), 'rb'))\n",
        "    else:\n",
        "        clustering = KMeans(n_clusters=int(total_dataset.__len__() / cluster_centroid_ratio), random_state=0)\n",
        "        print(\"Clustering Started.\")\n",
        "        clustering.fit(num_data)\n",
        "        print(\"Clustering ended.\")\n",
        "\n",
        "    all_cluster_centers = clustering.cluster_centers_\n",
        "    closest, _ = pairwise_distances_argmin_min(num_data, all_cluster_centers)\n",
        "\n",
        "    print(closest.shape)\n",
        "\n",
        "    cluster_to_labels_dict = dict()\n",
        "    for j in range(len(closest)):\n",
        "        if labels[j] >= 0:\n",
        "            cluster_to_labels_dict.setdefault(closest[j], []).append(labels[j])\n",
        "\n",
        "    cluster_to_label_dict = dict()\n",
        "    for k, v in cluster_to_labels_dict.items():\n",
        "        un, cnt = np.unique(v, return_counts=True)\n",
        "        un_idx = np.argmax(cnt)\n",
        "        cluster_to_label_dict[k] = un[un_idx]\n",
        "\n",
        "    print(cluster_to_label_dict)\n",
        "\n",
        "    file = open(os.path.join('models', 'models_' +\n",
        "                             str(method) + \"_\" + str(label_ratio), 'cluster_to_label_dict.pkl'), 'wb')\n",
        "    pickle.dump(cluster_to_label_dict, file)\n",
        "    file.close()\n",
        "\n",
        "    file = open(os.path.join('models', 'models_' +\n",
        "                             str(method) + \"_\" + str(label_ratio), 'all_cluster_centers.pkl'), 'wb')\n",
        "    pickle.dump(all_cluster_centers, file)\n",
        "    file.close()\n",
        "\n",
        "    if not load_cluster_from_file:\n",
        "        file = open(os.path.join('clustering', 'nslkdd' + \"_\" + str(cluster_centroid_ratio) + \".pkl\"), 'wb')\n",
        "        pickle.dump(clustering, file)\n",
        "        file.close()\n",
        "\n",
        "\n",
        "def generate_result(test_X, label_ratio_, test_Y=None, path_to_model='models'):\n",
        "\n",
        "    cluster_to_label_dict = pickle.load(\n",
        "        file=open(os.path.join(path_to_model, 'models_' + str(method) + \"_\" +\n",
        "                               str(label_ratio_), 'cluster_to_label_dict.pkl'), 'rb'))\n",
        "\n",
        "    cluster_to_label_dict = dict(cluster_to_label_dict)\n",
        "    all_cluster_centers = pickle.load(\n",
        "        file=open(os.path.join(path_to_model, 'models_' + str(method) + \"_\" +\n",
        "                               str(label_ratio_), 'all_cluster_centers.pkl'), 'rb'))\n",
        "\n",
        "    pd = pairwise_distances(X=test_X, Y=all_cluster_centers)\n",
        "    pd_s = np.argsort(pd, axis=1)\n",
        "    # print(pd_s.shape)\n",
        "\n",
        "    test_Y_pred = np.zeros(test_X.shape[0])\n",
        "    for i in range(len(pd_s)):\n",
        "        for j in range(len(pd_s[i])):\n",
        "            if pd_s[i][j] in cluster_to_label_dict.keys():\n",
        "                test_Y_pred[i] = cluster_to_label_dict[pd_s[i][j]]\n",
        "                break\n",
        "\n",
        "    test_Y_pred = np.array(test_Y_pred)\n",
        "\n",
        "    if test_Y is not None:\n",
        "        test_Y_binary = np.zeros(test_Y.shape)\n",
        "        test_Y_pred_binary = np.zeros(test_Y_pred.shape)\n",
        "\n",
        "        for i in range(len(test_Y_pred)):\n",
        "            if test_Y[i] != cat_dict['Normal']:\n",
        "                test_Y_binary[i] = 1\n",
        "            if test_Y_pred[i] != cat_dict['Normal']:\n",
        "                test_Y_pred_binary[i] = 1\n",
        "\n",
        "        print(confusion_matrix(test_Y, test_Y_pred))\n",
        "        print(classification_report(test_Y, test_Y_pred))\n",
        "\n",
        "        # print(confusion_matrix(test_Y_binary, test_Y_pred_binary))\n",
        "        # print(classification_report(test_Y_binary, test_Y_pred_binary))\n",
        "\n",
        "    return test_Y_pred\n",
        "\n",
        "\n",
        "def train_model():\n",
        "    if not os.path.exists('models'):\n",
        "        os.mkdir('models')\n",
        "\n",
        "    if not os.path.exists(os.path.join('models', 'models_' + str(method) + \"_\" + str(label_ratio))):\n",
        "        os.mkdir(os.path.join('models', 'models_' + str(method) + \"_\" + str(label_ratio)))\n",
        "\n",
        "    tree_work(load_cluster_from_file=True)\n",
        "\n",
        "\n",
        "train_model()\n",
        "total_dataset, labeled_dataset, unlabeled_dataset = get_training_data(no_samples=125973, label_ratio=1.0)\n",
        "generate_result(total_dataset.get_x(), test_Y=total_dataset.get_y(), label_ratio_=label_ratio)\n",
        "generate_result(test_dataset.get_x(), test_Y=test_dataset.get_y(), label_ratio_=label_ratio)\n",
        "\n",
        "# total_dataset_s, labeled_dataset_s, unlabeled_dataset_s = get_training_data(no_samples=175341,\n",
        "#                                                                             label_ratio=1.0, stratified=True)\n",
        "#\n",
        "# test_dataset_s = dataset_test(cat_dict, stratified_test=True)\n",
        "#\n",
        "# generate_result(total_dataset_s.get_x(), test_Y=total_dataset_s.get_y(), label_ratio_=label_ratio)\n",
        "# generate_result(test_dataset_s.get_x(), test_Y=test_dataset_s.get_y(), label_ratio_=label_ratio)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sV8h6m8pFACR"
      },
      "source": [
        "# Black Box Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJibtu5c-ljP"
      },
      "source": [
        "class BlackBoxWrapper:\n",
        "    def __init__(self, input_dim, output_dim, path_to_model='models'):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.path_to_model = path_to_model\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = x.cpu().detach().numpy()\n",
        "        x = x.astype(float)\n",
        "        y = self.getBinPrediction(x)\n",
        "        y = torch.FloatTensor(y).to(device)\n",
        "        return y\n",
        "\n",
        "    def eval(self):\n",
        "\n",
        "        return\n",
        "\n",
        "    def getBinPrediction(self, x):\n",
        "        pred_Y = generate_result(x, label_ratio_=label_ratio, path_to_model=self.path_to_model)\n",
        "        bin_pred_y = []\n",
        "        for y in pred_Y:\n",
        "            if y == cat_dict[\"DoS\"]:\n",
        "                bin_pred_y.append(1)\n",
        "            else:\n",
        "                bin_pred_y.append(0)\n",
        "        return np.array(bin_pred_y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9YQ6FBTjFD5z"
      },
      "source": [
        "#DNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ltLVmnd9FFlZ"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "class Blackbox_IDS(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Linear(input_dim, input_dim // 2),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.LeakyReLU(True),\n",
        "            nn.Linear(input_dim // 2, input_dim // 4),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.LeakyReLU(True),\n",
        "            nn.Linear(input_dim // 4, input_dim // 8),\n",
        "            nn.LeakyReLU(True),\n",
        "            nn.Linear(input_dim // 8, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer(x)\n",
        "        x = torch.nn.Sigmoid()(x)\n",
        "        return x\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2G9f6yMGiik"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from idsgan_preprocessor import preprocess3, create_batch1\n",
        "# from model.model_class import Blackbox_IDS\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(12345)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train = pd.read_csv('dataset/KDDTrain+.csv')\n",
        "test = pd.read_csv('dataset/KDDTest+.csv')\n",
        "\n",
        "trainx, trainy, testx, testy = preprocess3(train, test)\n",
        "# trainx, _, trainy, _ = train_test_split(trainx, trainy, test_size=0.5, random_state=12345)\n",
        "\n",
        "input_dim = trainx.shape[1]\n",
        "output_dim = 1\n",
        "batch_size = 64\n",
        "tr_N = len(trainx)\n",
        "te_N = len(testx)\n",
        "ids_model = Blackbox_IDS(input_dim, output_dim).to(device)\n",
        "optimizer = torch.optim.Adam(ids_model.parameters(), lr=0.001)\n",
        "loss_f = torch.nn.BCELoss()\n",
        "max_epoch = 50\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "\n",
        "def train(x, y):\n",
        "    ids_model.train()\n",
        "    batch_x, batch_y = create_batch1(x, y, batch_size)\n",
        "    run_loss = 0\n",
        "    for x, y in zip(batch_x, batch_y):\n",
        "        ids_model.zero_grad()\n",
        "        x_t = torch.FloatTensor(x).to(device)\n",
        "        y_t = torch.FloatTensor(y).to(device)\n",
        "        y_t = torch.reshape(y_t, [batch_size])\n",
        "\n",
        "        out = ids_model(x_t)\n",
        "        out = torch.reshape(out, [batch_size])\n",
        "\n",
        "        loss = loss_f(out, y_t)\n",
        "\n",
        "        run_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return run_loss / tr_N\n",
        "\n",
        "\n",
        "def test(x, y):\n",
        "    ids_model.eval()\n",
        "    batch_x, batch_y = create_batch1(x, y, batch_size)\n",
        "    run_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in zip(batch_x, batch_y):\n",
        "            x_t = torch.FloatTensor(x).to(device)\n",
        "            y_t = torch.FloatTensor(y).to(device)\n",
        "            y_t = torch.reshape(y_t, [batch_size])\n",
        "\n",
        "            out = ids_model(x_t)\n",
        "            out = torch.reshape(out, [batch_size])\n",
        "\n",
        "            loss = loss_f(out, y_t)\n",
        "\n",
        "            run_loss += loss.item()\n",
        "    return run_loss / te_N\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"IDS start training\")\n",
        "    print(\"-\" * 100)\n",
        "    for epoch in range(max_epoch):\n",
        "        train_loss = train(trainx, trainy)\n",
        "        test_loss = test(testx, testy)\n",
        "\n",
        "        x_t = torch.FloatTensor(testx).to(device)\n",
        "\n",
        "        out = ids_model(x_t)\n",
        "        out = torch.reshape(out, [len(testy)])\n",
        "        out_np = out.cpu().detach().numpy()\n",
        "\n",
        "        ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "\n",
        "        correct = np.sum(np.equal(ids_pred_label, testy))\n",
        "        acc = correct / len(testy)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(f\"{epoch} : {train_loss} \\t {test_loss} \\t {acc}\")\n",
        "\n",
        "    print(\"IDS finished training\")\n",
        "\n",
        "    x_t = torch.FloatTensor(testx).to(device)\n",
        "\n",
        "    out = ids_model(x_t)\n",
        "    out = torch.reshape(out, [len(testy)])\n",
        "    out_np = out.cpu().detach().numpy()\n",
        "\n",
        "    ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "\n",
        "    conf_mat = confusion_matrix(testy, ids_pred_label)\n",
        "    print(conf_mat)\n",
        "\n",
        "    torch.save(ids_model.state_dict(), 'model/IDS.pth')\n",
        "    plt.plot(train_losses, label=\"train\")\n",
        "    plt.plot(test_losses, label=\"test\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rYRRSNLEGvJo",
        "outputId": "98f04b5b-1ddb-41eb-eabb-0bc0aca3b3e8"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDS start training\n",
            "----------------------------------------------------------------------------------------------------\n",
            "0 : 0.001603667558888593 \t 0.024088641138174924 \t 0.7650816181689141\n",
            "1 : 0.0014328183404067516 \t 0.0228069831635023 \t 0.7435681334279631\n",
            "2 : 0.0014315951755499916 \t 0.025635523051340755 \t 0.7538591199432222\n",
            "3 : 0.001429098769125913 \t 0.025978903188994797 \t 0.7536816891412349\n",
            "4 : 0.0014194524444937889 \t 0.027638522159860776 \t 0.7587384669978708\n",
            "5 : 0.0014180715454657337 \t 0.026347325029442547 \t 0.7461408800567778\n",
            "6 : 0.0014339814711241676 \t 0.023876370678373563 \t 0.7498225691980128\n",
            "7 : 0.001403328682065855 \t 0.02633300822590411 \t 0.7565205819730305\n",
            "8 : 0.0014218083764960668 \t 0.024046106368430208 \t 0.7490684882895671\n",
            "9 : 0.0014232932399244884 \t 0.026063164229444616 \t 0.747028034066714\n",
            "10 : 0.0014230929557677778 \t 0.02653842670029998 \t 0.7485361958836054\n",
            "11 : 0.0014215941970705664 \t 0.02678609240041336 \t 0.747028034066714\n",
            "12 : 0.0014085875609459277 \t 0.026161419078337547 \t 0.7554559971611071\n",
            "13 : 0.0014158811544708327 \t 0.029578567225409365 \t 0.7519073811213627\n",
            "14 : 0.0013998772682505842 \t 0.027457994090973527 \t 0.7456085876508162\n",
            "15 : 0.0013982022966544788 \t 0.029648304937408696 \t 0.7444109297374024\n",
            "16 : 0.00141728372385253 \t 0.030586680078798358 \t 0.7475603264726757\n",
            "17 : 0.0014213169137778589 \t 0.025855339191076687 \t 0.7552342086586231\n",
            "18 : 0.0014032489720043801 \t 0.030502543782695037 \t 0.7498225691980128\n",
            "19 : 0.0014316055017506678 \t 0.027133782006234376 \t 0.747028034066714\n",
            "20 : 0.001386857833950018 \t 0.02497536738352246 \t 0.7480482611781405\n",
            "21 : 0.0014114193805600075 \t 0.030272113356271307 \t 0.745874733853797\n",
            "22 : 0.001414917874475072 \t 0.03152451892115699 \t 0.7484918381831086\n",
            "23 : 0.0014043518011671989 \t 0.022861199875179126 \t 0.7543914123491838\n",
            "24 : 0.0013758755870698313 \t 0.03269845738509938 \t 0.7496894960965224\n",
            "25 : 0.0014114420769945793 \t 0.029926805858116273 \t 0.7495564229950319\n",
            "26 : 0.0014098375124550714 \t 0.022886186557893977 \t 0.7533711852377573\n",
            "27 : 0.0013971166055906411 \t 0.029308237479120557 \t 0.7482256919801278\n",
            "28 : 0.0014035591488481274 \t 0.029592427222165256 \t 0.7461408800567778\n",
            "29 : 0.0014168422454615374 \t 0.028522709639699346 \t 0.7488910574875799\n",
            "30 : 0.0013992988046537523 \t 0.029680859910904557 \t 0.7524840312278211\n",
            "31 : 0.0014180809684891129 \t 0.027177043831553588 \t 0.7470723917672107\n",
            "32 : 0.0013913310300786954 \t 0.03392216952146157 \t 0.7495564229950319\n",
            "33 : 0.0014162937165168431 \t 0.02716474249467222 \t 0.7504435770049681\n",
            "34 : 0.0013974786178200217 \t 0.03181307369974 \t 0.7466288147622427\n",
            "35 : 0.0014020124183089994 \t 0.030101571070801036 \t 0.7492015613910575\n",
            "36 : 0.001383511086636693 \t 0.027171385111624338 \t 0.7580731014904187\n",
            "37 : 0.0014019538309973472 \t 0.031208737426145945 \t 0.751375088715401\n",
            "38 : 0.0014036678656954085 \t 0.03097594767079191 \t 0.7472054648687012\n",
            "39 : 0.001385273780712127 \t 0.030940164639393093 \t 0.7489797728885734\n",
            "40 : 0.0013848355961835856 \t 0.028013218656933164 \t 0.749778211497516\n",
            "41 : 0.001380959202299842 \t 0.03345145294118892 \t 0.7491572036905607\n",
            "42 : 0.0013962606739935596 \t 0.03129466101157065 \t 0.7473828956706884\n",
            "43 : 0.0013847951039343766 \t 0.027501150844070062 \t 0.7472941802696949\n",
            "44 : 0.001395974690719819 \t 0.03111102853982985 \t 0.7556334279630944\n",
            "45 : 0.0013920941106234238 \t 0.029235703136964924 \t 0.7575408090844571\n",
            "46 : 0.0013957278948027439 \t 0.0307220681864607 \t 0.7482700496806246\n",
            "47 : 0.0013896471069942632 \t 0.030056287404321626 \t 0.7507984386089425\n",
            "48 : 0.0013811955838617124 \t 0.031172558268686826 \t 0.7503992193044713\n",
            "49 : 0.001398228909537313 \t 0.028682907981133106 \t 0.7524840312278211\n",
            "IDS finished training\n",
            "[[8989  722]\n",
            " [4858 7975]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD5CAYAAAAuneICAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2deZxU1ZX4v6cXQBBbaRaVRlldWBSlQY2aiEZFY8RE3DUmMWImMctk4i86E83o/JLoz4wmmZhMTCAxJq6oCYkLasSIcYEGkQYVukWUZm1almbtpc7vj/Ned3V1Vder3uk63w/9qXr33ffq3qrHPfee7Yqq4jiO42QfOV3dAMdxHKdrcAHgOI6TpbgAcBzHyVJcADiO42QpLgAcx3GyFBcAjuM4WUpelEoiMg34GZAL/FZV70w43xv4AzAJqAIuU9U1IjIFuD+sBvynqj4VXLMGqAbqgTpVLU7XjoEDB+rw4cOjNNlxHMcJWLx48RZVHZRYnlYAiEgucB9wNlABLBKRuar6Tly164CtqjpaRC4H7gIuA5YDxapaJyKHAW+LyF9VtS64bqqqbonaieHDh1NSUhK1uuM4jgOIyIfJyqOogKYA5aq6WlVrgEeA6Ql1pgMPBO/nAGeJiKjq7rjBvg/gUWeO4zjdhCgCYCiwNu64IihLWicY8LcDhQAicpKIrABKga/GCQQFnheRxSIys/VdcBzHcVpDJBtAW1DVN4FxInIs8ICIPKuqe4HTVHWdiAwGXhCR91T1lcTrA+EwE+CII47o6OY6juNkDVEEwDpgWNxxUVCWrE6FiOQBBZgxuAFVfVdEdgLjgRJVXReUbxaRpzBVUzMBoKr3ExiSi4uLm6mQamtrqaioYO/evRG6sv/Sp08fioqKyM/P7+qmOI7TQ4giABYBY0RkBDbQXw5cmVBnLnAt8DowA3hJVTW4Zm1gBD4SOAZYIyL9gBxVrQ7enwPc0ZoOVFRU0L9/f4YPH46ItOYW3R5VpaqqioqKCkaMGNHVzXEcp4eQVgAEg/eNwDzMDXS2qq4QkTuwmfxcYBbwoIiUAx9jQgLgNOBmEakFYsDXVHWLiIwEngoG7DzgIVV9rjUd2Lt3b48e/AFEhMLCQiorK7u6KY7j9CAi2QBU9RngmYSy2+Le7wUuSXLdg8CDScpXA8dn2thU9OTBPyQb+ug4TufikcCOk82UzoFdVenrOT0SFwBtZNu2bfzyl7/M+Lrzzz+fbdu2dUCLHCci29fBE9fB4tld3RKni3AB0EZSCYC6uroktRt55plnOPjggzuqWY6Tni2rgteyrm2H02W4AGgjN998M++//z4TJ05k8uTJnH766Vx44YWMHTsWgIsuuohJkyYxbtw47r///obrhg8fzpYtW1izZg3HHnss119/PePGjeOcc85hz549XdUdJ5uoKrfXnioA6mrghdtgx4aubkm3pcMDwTqT2/+6gnfW72jXe449/CB+8NlxKc/feeedLF++nKVLl/Lyyy/zmc98huXLlze4a86ePZsBAwawZ88eJk+ezMUXX0xhYWGTe5SVlfHwww/zm9/8hksvvZQnnniCq6++ul374TjNCAf+qvdBFXqao8HaN+GfP4M9W+HC/+nq1nRLfAXQzkyZMqWJr/7Pf/5zjj/+eE4++WTWrl1LWVnz2daIESOYOHEiAJMmTWLNmjWd1VwnmwlXAPu2w64e6GK8sdRelz5s9g6nGT1qBdDSTL2z6NevX8P7l19+mRdffJHXX3+dvn37csYZZySNWO7du3fD+9zcXFcBOZ1DVRn0HQi7t9hq4MDBXd2i9mXjMuh9ENTsgtfvg2k/6uoWdTt8BdBG+vfvT3V1ddJz27dv55BDDqFv37689957vPHGG53cOqfDUIVYrKtb0Xpq98C2tXDUuXZc1QPtABtLYdgUOO5SWPw72P1xV7eo2+ECoI0UFhZy6qmnMn78eG666aYm56ZNm0ZdXR3HHnssN998MyeffHIXtdJpd56cCY9f29WtaD0frwYURp4Bub0b1UE9hbp9UPkeHDoBTv021O6GN/+3q1vV7ehRKqCu4qGHHkpa3rt3b5599tmk50I9/8CBA1m+fHlD+Xe/+912b5/TAaxfYsbT7eugIDE7+n5AaAAeeBQUjoItPUwAVL4HsToTAIOPgWMugDd/DZ/4BvTu39Wt6zb4CsBxWkP1RkCh9LGubknrCFU+haPtr6epgEID8KFBxpnTvgN7t0HJ77quTd0QFwCOkyl7d0DNTnu/9GGzB+xvVL0P/Q+H3geaANi6Bupru7pV7cfGUsjvBwMCj7yiSTDiU2YMru3ZqeMzwQWA42RKdRBYNPx02LIS1r/Vte1pDVvKYOBoez9wjKlLtibdNnb/ZGMpDBkHObmNZad/B3ZuhLeTq2yzERcAjpMpoQA4+V/MgPr2w13bnkxRNZVP4Rg7Dl+jqIFWPmfBVd2ZWMwEwKETmpaP+BQMnWTtr285VUu24ALAcTIlTC0w6Bg45nzLqFlX07VtyoRdW2DvdlP9gBmBIVpKiH/+1NIrfNBs877uw7YPYd+O5gJAxGwBW9fAiqe6pGndDRcAjrPhbfjF5EbDYTqq19tr/8Pg+Cthz8dQ9nzHta+9CWf6A4OZf98B0LcwvStofW2juuuZm7qvzaDBAHxc83NHn2+C+9V79u84jnbCBUAbaW06aICf/vSn7N69u51b5GTEzkp4+ErLjLl2YbRrqjdCnwLo1RdGnQn9Bu9faqBwoA9XAGBqoHQCYGMp1O2FCZeYm+Wbv25bOzrKeL6xFCQHBh/b/FxODpz2r7D5HSib1zGfvx/hAqCNuADYj6mrgceugd1VILmwvSLadTvWmwcNQG6eRZqumrf/RJpuKTPbxcFHNJYNHJ1eBVRRYq9n/QBGnw0v3xm4w7aCuhr42fFw7wSYcx0s/I2txNpDN7+x1OIbevVNfn78xXDgEFj2aPR7Vm+E9+e3vW0t8epPYf6PO/YzEvBAsDYSnw767LPPZvDgwTz22GPs27ePz33uc9x+++3s2rWLSy+9lIqKCurr67n11lvZtGkT69evZ+rUqQwcOJD58zv44XKaogrPfBc+eh1mzIYXb4fta6NdW70R+h/aeHz85fD6L2D5EzDl+o5pb3tSVQ4DRjb1kCkcA7v+aLaBPgXJr6tYZGqvgiI47y745cnw/K1w8W8yb8P6JaarH3YSfPhPWD7HyvP7mcvmgJGQdwDk92n6euh4OPITLd97YykceUrq87n5tnJbNc/UQDkR5sHzfwhLHoQbXoHDkqiW2sqrP4UXfwAInHBVU+HcgfQsAfDszdH1uFE5dAKcd2fK0/HpoJ9//nnmzJnDwoULUVUuvPBCXnnlFSorKzn88MN5+umnAcsRVFBQwD333MP8+fMZOHBg+7bZSc+i38KSB8woOP5iWDQ7+gqgegMMOrrx+NAJMGQCLH1o/xAAW8osOjaeUB1UVW6eMsmoWARFxWZMLRwFn/gmLPgJTPoiDD81szZ8sMBeL3/YbBDbKyx9c/j33jOmbqrdA7E4W0PeAXBTWepo3t0fw46K5gbgREZONbXdxrfh8BNarqsKZS8CaoP0NRENyLH6pkI2FUsetPuOOQfKX4TFv4ezbkt7WXvgKqB25Pnnn+f555/nhBNO4MQTT+S9996jrKyMCRMm8MILL/C9732PBQsWUFCQYoaVDdTXmgqlK/ngFXj2e3DUNDjzVis7eFg0ARCrb74CAFsFrF8ClSvbv73tSX0tbP2g0fUzJDQIp0oJsbPSriua3Fh2+r9BwbDAIJyh6mbNKzBkPPQrNIFy8DCYMAPOv9tm2TeVwS1r4bYtcNvHcMs6uPpJqNtjwiEVDQbgdALgDHuNotbZ/I4Z/g8/Ad5/yf7SsekduGs4PP5F2Lk5db13/wZ//aatSC77E4w5F5b8odO8ynrWCqCFmXpnoKrccsst3HDDDc3OLVmyhGeeeYbvf//7nHXWWdx2W+dI+G7F3u3w8BWwbgl85x2b+aVD1Qbr4y9LPTPNhK1r4LFrbcb7+d80Lv8Likww1deZXj8Vu7aA1psqJJ4Jl5h75NuPwKd/0PZ2dhRbP7Sgr3gDMMAhI8xwmioWYF2g/y+a0ljWqy+c+yOzoyz6jcVFRKFunxncJ30pWv2cXItYHjkVCo6A0sfteUjGxmX2OiSNAOg/BAaPg9XzLUCsJcpftNdLfg8PfNZ+5xFnpFYd1dXAUzMBgfeehtUvw7S7zFYUv+nOmldhzpfh8BPh0gchrxdMvg5WPQvvzjWB2MFEWgGIyDQRWSki5SJyc5LzvUXk0eD8myIyPCifIiJLg7+3ReRzUe+5vxCfDvrcc89l9uzZ7NxpaQLWrVvH5s2bWb9+PX379uXqq6/mpptuYsmSJc2u7fHs3Ay//4zpe+v2wLrF0a6rKoeFv7YQ/rayb6d5/Gg9XPEw9Dmo8VxBkZXvTGPUDF1ADzq8aXn/ITD6LDMsxurb3taOIvT0GZiwAsjrBQcfmdoQXLEIcvLgsOOblh/7WZu9zv8RVG+K1oaKRabeGXF6Zm3PyYHxn7cZ+K4tyetsLDXhfOCg9PcbNRU+egNq0jhilL1gq5VDhsOZt9lnlD6euv4/7rI6n/sVfPVVW209NRMeurRxlblhmU2GDhkOVz1uAg5g1Fn2O5TMTt/+diCtABCRXOA+4DxgLHCFiIxNqHYdsFVVRwP3AncF5cuBYlWdCEwDfi0ieRHvuV8Qnw76hRde4Morr+SUU05hwoQJzJgxg+rqakpLS5kyZQoTJ07k9ttv5/vf/z4AM2fOZNq0aUydOrWLe9HBfPwBzDrH8s9c+gdAGj1K0hG6Zpa92PZl8dsPw+YVZvQNg59CCorsdVsaQ3AYBJaoAgI4/grYsQ7WLGhbOzuS+CRwiQwcY79RMtYutEEw0bNGBM6723T1L0Zc+ax5FZD0xtxkTLjEBPU7f05+fmNpcv//ZIycCvU18NFrqevsqzYhMfosOx5/sQnBl/4reU6htYssxmDi1XDMZ8xW9OXnYNqd1u/7ToYF/w1/vNg2q7nmyaYr4ZwcKP6yTZQ2vxutH20gygpgClCuqqtVtQZ4BJieUGc68EDwfg5wloiIqu5W1VA52AcIHX+j3HO/4aGHHmL58uXcfffdfOtb36K0tJTS0lJef/11Ro0axbnnnsuyZctYunQpixYtori4GIBvfOMbrFy5smd7AG1cDrPPtUyM1/4Vxk6HwWMbVQrpqFhkr/u2t/wfNQrrl0K/QTbLSqRgmL2mswOEaSD6H9783NHnQ+8CSxDXXdlSZkFfydRvYSxAYoBUrN7UdsOmNL8GzIX0lK+bgP14dfo2fLDAPGkOOCTz9g8ZZ4FcpU80P1e712ww6fT/IUd+AnJ7tWwH+OAVM0KPPtuOc3Lg7DvMY2xRgvdTzS546gY4qAimxblz5uSaeuxrr8PQE+Hvd5ga7pqnGice8ZxwtbWrE1YBUQTAUCB+WlQRlCWtEwz424FCABE5SURWAKXAV4PzUe5JcP1MESkRkZLKyh64b2lP5sPX4Xfnm+rgS8+ZBwmYm19FSbRAoIoSGHYy5PWxPDRtYePbNjtMtvl5+B8xnSto9QbTlSfbPjG/D4y9EN77W/eNMq0qb24ADhk42tRzOxL2z938LtTuamoATmTyV+x1eZKBOZ7aPVCx0BLptQYR041/9FpzYV35rq0OogqAXn3NDXX1y6nrlL0AvQ60eiEjz4DRn4ZX7m4a+/HCD0wAXvTLpurFkEOGwxf+Yvr+Lz0Lg45K/pn9BsLYi8yetG9ntL60kg73AlLVN1V1HDAZuEVE+mR4/f2qWqyqxYMGRdDrOR1Dza7M6pf/HR68yAbKL89r6nY4tNhWBKnUDSH7qk1lM/JT9p9u5TOtjx6t2web32uuww7p1Q8OGJB+BbBjgwURpXLvO3yipYoOVwqtRdXakol3Tfnf4blbWhY+8VlAE2lwBU2wA4SrsFCAJ+PgYXDEKZYXqaXfaO1CU7uM+GTqOukYf7G9JgqbqB5A8YyaCpuWJ/fUUbXvdOQZZiOJ59O3W1rwV++x4/K/24rglK+3bNsQsUlCohtuIpOvs3xGLdka2oEoAmAdMCzuuCgoS1pHRPKAAqAqvoKqvgvsBMZHvGdkdH/Mx54hXdrHnZVw9xjznY9CfR387ds24/nyczY4xBPOJNOpgda/BRqz+kefZ4FDrdWLbn7XlvItBfEUFEVbASR6AMUzILAtfJxGuCVj747ALfDb8LPj4N5xMOtsK0/H2oXwyJXwxi/NsyXp/bfDrs3J9f8QlxU0oe0Vi2zz+ENGtNyGCTMsRcSmFanrrFlgK6gj2rA96oCR5hFWOqdp+cZSm62na2c8IwP7W7JVwJZVsP2jRv1/PIeON5vPm/ebQfcvXzfVVOhW3FaGnWQ2l5JZHbrfRBQBsAgYIyIjRKQXcDkwN6HOXCDcIHUG8JKqanBNHoCIHAkcA6yJeM9I9OnTh6qqqh4tBFSVqqoq+vTJaPHUfpTNMxXAKz+xmXQ63vkzbPvIgln6JQlyG3S0/UcNZ5apCM8PnWQ++2CrgNYQuge2ZCAsiBALkE4AhMbldKubEFV47Rcw+zz4fyPg0ats1jdkPJz+XWv3Q5e17Kmypdzq9D/MBupUuuOGHEApVED9D7XfJdETqGKRCeFkqrN4xl5kKTWWz0ld54MFcNjE1NHGUZlwiX03lasayzYss+8tSmRvyGHHmy0imR0gdP8c/enk1575H/Y6exrsqoTP/a+pAdsDETMGbyyN7jDRCtLGAahqnYjcCMwDcoHZqrpCRO4ASlR1LjALeFBEyoGPsQEd4DTgZhGpBWLA11R1C0Cye7amA0VFRVRUVNDT7QN9+vShqCiJwagzWPms6eCrN8DSP9mDmQpVC2sfeDQcdV7yOjm5FlST7sFeu8gGq9BgefiJsOo5+GQr9k3e8LZ5XbQ0Ozx4WHoPnuoNLXuvHFRkeXairgAq34Pn/8Nmj5/4ps02i6Y0qhyGjIUnvmKC4YpHIK930+t3VsKfLrYB4+on4K0HLd99sr2Kt6RwAQ0Rab495J6tNhM+LoXffTz9BppKpfQJyxeUKDBqdpn77ylfS3+vdIz7HMz7dxM2U//d1F6bltusPBNycm2fgNXz7dmNb3PZC/Ycp0rLUFAEJ3/Vvu+p/5E+ojhTjrvUYg5KZsGwFuwvbSBSIJiqPgM8k1B2W9z7vcAlSa57EHgw6j1bQ35+PiNGZLDkczKjdq/NjiZeaTOsV++FE66xfCrJKP87bCqF6b9seSZWNBle+7kZBfMPaH5e1WaeY85pLDv6fMvJUr3J/O4zYcMy0w231KaCItO7psqHU7vHBsSWVgA5OaaiqIrgDQMmAAA+f39y+8T4i+03+MvXLKr00j80fvc1u8y3vHoTfPFvtvqY9EUTwEv+AFNvaXqvqjKbobckBAtHN82KWhHEa6TyAEpkwiXmCbN2IRxxUtNza980NdzwNuj/Q/ofaobk0sfhjFssSrlmZ+vy9IyaaqvWypWNuvmaXeaKOWVmy9eecYs9y6kmO22hd38TvG/90QLuogROZoingnBa5sNXTf1z1Hk28972UXPdazyv3gsHDbWBoCWKis0VbsPbyc9vXQO7tzSd+Rw9DdDM0/jG6m12mMoAHNLgCZRCDdTgAtqCAAAbiNOlVg6pXAVIarUMWHKw839i6q8nZ1p/6ussinTDUotrCA20hww3lcWSB5rn699SBocc2dygGc/AMWYHqd1jxxWLTGcfdXZ7zGdstZhMDfTBAvMIa4v+P54JM8zrZv1brTMAhzTYAeLUQGteNWN1Mv1/PPkHWDBcS9HjbWHydVC/z1beHYALAKdlVj5nCbhGnG56+CETLJAlWbTr2kUmME75esuDDJgnEKRWAzV4nsQJgCHjTU+/8tnM+lBVDrW70wcIhbEAqYLBwtTHB6URAANG2ow0SkTwllWmekqVujhkyvXmebLiSZj7Tctkuuo5y51zzPlN6xZ/2YTVqgS32ZZcQEMKRwPa6M9fscjiNlIlX0ukd3846lzbcSvRg2nNAlPjhVGvbeXYz5q/fOmcYA+AXBiUZA+AdBxypP1m8XaA8hchvy8c0YpgtfZkyDjzrlo0q0Nci10AOKlRtUFk1FSb6YhY3pSqMstVksg/fwp9DoYTr21+LpH+QyyvSypPoIpFlho4/j+0iHkDvT+/cYYahQ2BATjyCiCFAAiT2CULAouncJTNHqMkl9uy0vTMUTjt2/Cp78HSP8Li39nGJqH/fTxHnWu2iEWzGstiMTNMp/IACmlICldm16wradn9MxkTLjGj6Af/aCzbV23BZJmmf2iJAw6xAK0VT9pKaNDRrTfCjpxqs/4w2rzsBVMxtZdRty0UX2fqn13tb+d0AeCkZvM7NhiGHjhgkbyFY+CV/27qnla50gKgpsyMPsMLA8KSUbHIoiYTl9ZHn2fBSqv/kfy6ZGxYamqJgSkCb0L6DbYZZVoVUJI0EPFEdQWNxcwwm65d8Zxxi3lXnfoty0uTjJxcmHStqTRCb6Qd6+x7SxUDkNj2qnIT9Hu3N00AF4XRZ5vBPd5P/6M3LEirtQFgqZgww36X919qnfonZNRUU3VWLLLvbOsHMObs9mtnW5gwA65/KXO7VwRcADipCVUtR53bWJaTa6uATaW2oUbIP39uqqKTmmdCTUnRZBMwiUnEavfYkj5Z5OmRp0Gv/pm5g25cZmqMdHranByzX6QUABtNLZDOhbEhoCqNANj+kQ3KqSJCkyFiaZjPvqNlg/aJXzCVyOLfBW0JcwClUQH1PtBWOFXlydVwUcjvY+qZd//amC9nzQLIyW8aUdseHDXNXFc11jYBMPx0s3Wsnp/e/bOzSed+2wZcADipWTXPjH+JM94Jl5hr3Ct3BxGr6ywL5onXJPf7T0VoB0hUA61fagbiZANPXi8zzIW7OaVD1QzN6dQ/IQVFqQXAjvX2XaT7D9n/UFNfpRMAob99VBVQJvQ/1Ayyb/3JBuF0LqDxhNtDViwyYZdObZSMCTPMo6rseTv+YIGpktLZOjKlV1/rJ7RNABxwsMWbvD/f1D8DRsGAnu9d6ALASc7OShsA4tU/Ibn5cOq3beD+4B8WfaoxOOXGzD7jsOPMKyQxICzdzPPo8y1t84a30n/Gto9MjRHVPbBgWGobQPXG9Pp/MAExYGR6FVC4ecygDhAAYB4kez6Gd/5iK4Be/S2NRTrCWIC1QQBYJoFVIcM/aYn3ls+x73/D0vZX/4ScdAMceWrb94sYOdU29VmzoPuofzoYFwDtxaYVZujqKZQ9D2hyAQAw8So48FD4+3/ZFnbjLzZvikzIP8BmbYl2gIqF5s6YKqf7mLNNvRHFGyh0M81kBVC9obkLJdheAOk8gEIKR0ZYAaxMnZmzPRj+SZvJlsxqzAEURZ1QOMYG7c0rMlf/hOTmWbDWyudsRq2x9jUAxzN0EnzpmeieSqkYNdXaWbe3+6h/OhgXAO3Bhrfhf0+zHN+dtJVbh7PqOfN3TzVw5veBU79pq4CanWaUbA1Di82PO3SZVG2ceaai7wDzJY8iADYuM2ExeFy09hw8zAaBxGRuqpYILp0BOGTAKMtd1FIyt8pVHaP+CQlzy4f77EZV5cSriTL1AIpnwiXmw/7if1p0dKbG5M6maLLZE/L6wPDTuro1nYILgLYSq7fkXfn97D/Zc/vt5maN1O0zr4qjzm15xjjpi7bMP2qaJcdqDUXFJkDCiNgd60y9k26wOPo8C+7a9lHL9TYsszQLUd35UgWD7dlqg1kUFRDYYBurMyGQii2rMjMAt4aJV9rgW7s7vQE4JF5QDG2DACiabLai7Wstkrg7uFS2RG6+Rd5OuCR5dHoPxAVAW1n8O9MbXnCv5XIpmWWh2/sza161QTldeHuvfnDDAttbt7WEM/1QDRSmIUg38zw6CH5Kt0fAhrczSw+QKhgsqgtoSLqkcLu2mH6+I1cAYKul8Z+39+lcQEMOPsLcYQcebcbR1iIC44N9bTtK/9/eXHAPTP9FV7ei03AB0BZ2boYX77BkUhNmWAKsEZ+Cv33Hgl72V1bNs2VwlJztBx2WfPOLqAwYaQE9oSdQRYl99pA0K4rCUTZALXs0dbrc6k22moi6RSCYGyg0NwSHW0Em7gWcinSxAKEBOJMYgNZyytetPVFdMMMEaaF3TVuYeJV9p8de0PZ7Oe2OC4C2MO8/zI/7M/9ts53cPJjxO/O0ePSa1BtXdzW1e5ruZBSPKqx61jbBaG+XvWSImBEvXAFULDLX03SpJABOmmmCY82ryc9vjBgBHE+vvpZSOVEFFDUPUEi/gRYMlWoFsCX0AOoEAXDoBPjmkuTbD6bi6jnw6Yh7/LbEwNHwnXcspYHT7XAB0FpWvwylj1k4frzRrF8hXPagJTKb86XMdnTqKPbuME+MF//TNmf/8TD4yRg7Ttzpa/O7pldP5f3TERRNts/d/bG5C0Y1PE682qJ3F/x38vOhB1Cm/uHJYgEyVQGlcwXdUmZBZQd1UYpvx8EFQOuo3WtqnkNGwGnfaX7+8IlmE/jgFfj7f3Z68wAbTP9xN/z6k3DXkfCnGfDa/wT++l8zQ9er98J9J8F7TzdeFyYQi4/+7WiGFgNq7qT1NdFdD/P7WF9WzzdPokQ2vG2DcKYqqlQCoG9h83z8LVE4KvUKoHKlTRxa42PvOO1EB+Uw7eH882c2s7v6idSeDROvNDvAa/9jGRBDQ1w6avfagHbUtNaFgG/7CF6/z/LB1+62bIafvMk2MSmabIbbkBO/AE//m20leNR5cN5dJgAOOz66rrs9GHqivS4MjMmZuAsWXwcL7oUF99jKK56Ny2z3qUwpGGYrvPgNQnZsiO4BFFI42rJi1u1rLji2rLIsj47Thfj0I1Oq3jeVw7jPpw8WOfdHNvi/cFu01MBgAuPhyxtD6KOyaYXliv/ZRNu7d+x0+JfX4cvP2o5JI89oOviDCYUbXoGz/8tWK/edZF44nan+AfNUKRwdBFoVRQ+2ApvdT7ne8s7Ebw+4Z5vtKZCJ/j+koMi8oPZuayyrXh9d/RMyYJStuLauaVq+b6cZmUHT+W4AABo8SURBVDvDAOw4LeACIBPqamzGnNvLBvd05PUyG8H2tdEG9Pq6xuRdr0d0RVOFp74Kv/qEbSh+0g3wzaW2P+mQsemvz823gK4bF8KYT5sHyNjp0T67PQn9zVsTeHTyv5jn0D9/2lgWbhDSmh2iwk3s49VA1RszE0yQ2hU03CymMwzAjtMCLgDSUV8LZS/Cn78OPxlt6pmzbo0+GBx9vnmOLPpt+rqrnrVAqOGn24w8zGPfEmUvwNsPWxrmf10O037cOIBlQkERXPZHuKWiazw2woE/6taD8fQbaOmPlz3a6L/fsAl8K1cA0CgA6mvN5TeqB1DIgJH2mmgI3hKsVDo6BsBx0uACIBmqFgn7lxvNW+ZPF9sGKEefD1c+nn6f0Hhy82DSlyzFbLjLUioW/sb0z5f+wSKLX78vfTtf/pEF7pzzw/bJKdNVEZCjz7K+j25lEq4wEd1r/2OvG942nX2qfEItUZCwAti5CdDMBUDfARbjkLgCqFxp6SlCAeE4XYQLgGQs/j08+DlY8WfblPyKR+CmclOrHHVO5sbZE79gWS9LZqeuU7nKMmtO+qINHCdeY5kUw12okrHqOfN++eT/ieY3350ZMNJWMK1Vixw8zML4l/zBMpluWNY69Q9YHEBu78Y0Ew1bQbbCMF44OskKYKWlGt7ffzNnvyeSABCRaSKyUkTKRaRZshsR6S0ijwbn3xSR4UH52SKyWERKg9cz4655Objn0uBvcHt1qs0secD2vr2pHD5/v+WdycT9L5GDDoNjLrAUEam2MiyZZRtmhNspnvRVMyAuvD95fVWY/0NzRT3+8ta3rSdx6rctk+Or99og2xoDMJhrZrwraMNWkBkagcEMwYkrgC1lrv5xugVpBYCI5AL3AecBY4ErRCTRungdsFVVRwP3AncF5VuAz6rqBOBaIMFPj6tUdWLwt7kN/Wg/KlfZrHriFe2bvGryVyyh2Iqnmp/btxOWPgTjLmpUWQwYYUKjZLadT+S9v5mh81PfM0OuY6uHYz8Lb/4q2CGqlSsAaCoAwhVApm6gYIbgHeugZrcd19eZQHADsNMNiLICmAKUq+pqVa0BHgES3USmAw8E7+cAZ4mIqOpbqhrqMFYAB4hIG6bSncCyR2xruDCJVXsx/DSb9SUzBpc+brsnTb6+afknvmF52Zc+1LQ8FoP5Pzb1woRL2red+zunf8cGf2j9CgCCjWFCAbDeVmd9CzO/T6jn3/pB42us1lcATrcgigAYCsRnxqoIypLWUdU6YDuQ+L/lYmCJqu6LK/tdoP65VSS5Yl1EZopIiYiUVFZWRmhuG4jFYNnjMOrM9t+AWcRWAesWN00Up2pCYciE5h4ww6ZY8NYb9zWNI3h3rm3W8amb0+9zm20cfoL9fv0GZ5b7JpFwY5i6msZ9AFoTtZvoClrZiTmAHCcNnWIEFpFxmFoofsfwqwLV0OnB3zXJrlXV+1W1WFWLBw1qhUdHJnz0um3UfdxlHXP/4y8z756SWXGf+YbltZ/yleTG5VNutECicBP0WD28/GObQUaNLs42Lp4FX3y6bZtpFxQBarP/6g2ZewCFJGYFDV1Ao+bmd5wOJIoAWAfEO5YXBWVJ64hIHlAAVAXHRcBTwBdUtcEapqrrgtdq4CFM1dS1LHvUBuj2SIObjD4FcNylUDrH7AFgs//eBalVOcdcYG6erwWBYSuess1TzvieBW05zek7oO0z7PhgsOoMdgJLpM9BthoJg7+2rDJbQltSaDtOOxFFACwCxojICBHpBVwOzE2oMxcz8gLMAF5SVRWRg4GngZtV9Z9hZRHJE5GBwft84AJgedu60kZq95rb57GfbZ4yoT2ZfJ15qix9yIKL3vmL5Q1K9Zm5eXDy12DtG/DRm/DynTDoWBj7uY5ro9M0FmDHhrblRiocBVVBDEjlSlf/ON2GtAIg0OnfCMwD3gUeU9UVInKHiFwYVJsFFIpIOfAdIHQVvREYDdyW4O7ZG5gnIsuApdgKog3bSqXthOn3W6JsHuzbbmqajuTQCTDsZFg0y+INYrUmFFrihKttlfD4F6GqDKbe4lkkO5pwwN/8LtRUt14FBKYG+vh9ew7dBdTpRkSyIKrqM8AzCWW3xb3fCzTTYajq/wX+b4rbTorezDZQXwtPXm//6abekrre24/CgYfaTkgdzeSvwJNfsaRyI89oup9AMnr3t1QHr/3cjMXHfLbj25jt5B9g+x2HG9W0RQAUjoSlm0wNVFOd/vd2nE6i508jc/Js441/3Nk07308uz+2ZG0TZnSOXn3shRZtWre3uetnKk7+Fzj4SDj7dp/9dxYFw8xrCzJPBBdPaAgODfmDfAXgdA96/kgiAp+5x9wDn7yhacrgkBVPmiqmo7x/EsnrbT7+hx4XPfXyQYfDt5dZzhyncygosi0/oY0rgGAz9pXP2qurgJxuQs8XAGARvZf90QbeR660LRLjeftRGDw2860D28Jp34avLnA//u5MQZzzW5tsAEEw2No3zRPswO6T9cTJbrJDAIDN5i59wDJyPvXVRqPwx6uhYqG5Z7bFb9zpeYSBZL0Pgt4Htv4+vfqa66fGbBMYf86cbkL2CACwdAzn/ghWPg2v3G1lyx4DxFMqOM0JBUBrYwDiCSOCXf3jdCOyT/9w0g2wYanl0T/sOAv+Gn5a29IGOD2TMBisLeqfkAEjYc0CjwFwuhXZJwBE4IJ7YfM78Ni1UL8PTv+3rm6V0x0JbQBtCQIL8RWA0w3JLhVQSP4BcNmfTK+b1weOvTD9NU720bfQZv+Dj237vY48ze419MS238tx2glR1a5uQ2SKi4u1pKSk/W5YudJyvY/shOAvZ/9k7w6LI3FvLWc/RkQWq2pxYnl2P9WDjvagHKdlPGmb04PJThWQ4ziO4wLAcRwnW3EB4DiOk6W4AHAcx8lSXAA4juNkKS4AHMdxshQXAI7jOFmKCwDHcZwsxQWA4zhOluICwHEcJ0txAeA4jpOlRBIAIjJNRFaKSLmI3JzkfG8ReTQ4/6aIDA/KzxaRxSJSGryeGXfNpKC8XER+LuLbJDmO43QmaQWAiOQC9wHnAWOBK0RkbEK164CtqjoauBe4KyjfAnxWVScA1wIPxl3zK+B6YEzwF3F3dMdxHKc9iLICmAKUq+pqVa0BHgGmJ9SZDjwQvJ8DnCUioqpvqer6oHwFcECwWjgMOEhV31DLR/0H4KI298ZxHMeJTBQBMBRYG3dcEZQlraOqdcB2oDChzsXAElXdF9SvSHNPx3EcpwPplP0ARGQcphY6pxXXzgRmAhxxxBHt3DLHcZzsJcoKYB0wLO64KChLWkdE8oACoCo4LgKeAr6gqu/H1Y/fhT3ZPQFQ1ftVtVhViwcNGhShuY7jOE4UogiARcAYERkhIr2Ay4G5CXXmYkZegBnAS6qqInIw8DRws6r+M6ysqhuAHSJycuD98wXgL23si+M4jpMBaQVAoNO/EZgHvAs8pqorROQOEQl3U58FFIpIOfAdIHQVvREYDdwmIkuDv8HBua8BvwXKgfeBZ9urU47jOE56sntTeMdxnCwg1abwHgnsOI6TpbgAcBzHyVJcADiO42QpLgAcx3GyFBcAjuM4WYoLAMdxnCzFBYDjOE6W4gLAcRwnS3EB4DiOk6W4AHAcx8lSXAA4juNkKS4AHMdxshQXAI7jOFmKCwDHcZwsxQWA4zhOluICwHEcJ0txAeA4jpOluABwHMfJUlwAOI7jZCkuABzHcbIUFwCO4zhZigsAx3GcLCWSABCRaSKyUkTKReTmJOd7i8ijwfk3RWR4UF4oIvNFZKeI/CLhmpeDey4N/ga3R4ccx3GcaOSlqyAiucB9wNlABbBIROaq6jtx1a4DtqrqaBG5HLgLuAzYC9wKjA/+ErlKVUva2AfHcRynFURZAUwBylV1tarWAI8A0xPqTAceCN7PAc4SEVHVXar6KiYIHMdxnG5EFAEwFFgbd1wRlCWto6p1wHagMMK9fxeof24VEUlWQURmikiJiJRUVlZGuKXjOI4Tha40Al+lqhOA04O/a5JVUtX7VbVYVYsHDRrUqQ10HMfpyUQRAOuAYXHHRUFZ0joikgcUAFUt3VRV1wWv1cBDmKrJcRzH6SSiCIBFwBgRGSEivYDLgbkJdeYC1wbvZwAvqaqmuqGI5InIwOB9PnABsDzTxjuO4zitJ60XkKrWiciNwDwgF5itqitE5A6gRFXnArOAB0WkHPgYExIAiMga4CCgl4hcBJwDfAjMCwb/XOBF4Dft2jPHcRynRaSFiXq3o7i4WEtK3GvUcRwnE0RksaoWJ5Z7JLDjOE6W4gLAcRwnS3EB4DiOk6W4AHAcx8lSXAA4juNkKS4AHMdxshQXAI7jOFmKCwDHcZwsxQWA4zhOluICwHEcJ0txAeA4jpOluABwHMfJUlwAOI7jZCkuABzHcbIUFwCO4zhZigsAx3GcLMUFgOM4TpbiAsBxHCdLcQHgOI6TpbgAcBzHyVJcADiO42QpkQSAiEwTkZUiUi4iNyc531tEHg3Ovykiw4PyQhGZLyI7ReQXCddMEpHS4Jqfi4i0R4ccx3GcaKQVACKSC9wHnAeMBa4QkbEJ1a4DtqrqaOBe4K6gfC9wK/DdJLf+FXA9MCb4m9aaDjiO4zitI8oKYApQrqqrVbUGeASYnlBnOvBA8H4OcJaIiKruUtVXMUHQgIgcBhykqm+oqgJ/AC5qS0ccx3GczIgiAIYCa+OOK4KypHVUtQ7YDhSmuWdFmns6juM4HUi3NwKLyEwRKRGRksrKyq5ujuM4To8higBYBwyLOy4KypLWEZE8oACoSnPPojT3BEBV71fVYlUtHjRoUITmOo7jOFGIIgAWAWNEZISI9AIuB+Ym1JkLXBu8nwG8FOj2k6KqG4AdInJy4P3zBeAvGbfecRzHaTV56Sqoap2I3AjMA3KB2aq6QkTuAEpUdS4wC3hQRMqBjzEhAYCIrAEOAnqJyEXAOar6DvA14PfAAcCzwZ/jOI7TSUgLE/VuR3FxsZaUlHR1MxzHcfYrRGSxqhYnlnd7I7DjOI7TMbgAcBzHyVJcADiO42QpLgAcx3GyFBcAjuM4WYoLAMdxnCzFBYDjOE6W4gLAcRwnS3EB4DiOk6W4AHAcx8lSXAA4juNkKS4AHMdxshQXAI7jOFmKCwDHcZwsxQWA4zhOluICwHEcJ0txAeA4jpOluABwHMfJUlwAOI7jZCkuABzHcbIUFwCO4zhZSiQBICLTRGSliJSLyM1JzvcWkUeD82+KyPC4c7cE5StF5Ny48jUiUioiS0WkpD064ziO40QnL10FEckF7gPOBiqARSIyV1Xfiat2HbBVVUeLyOXAXcBlIjIWuBwYBxwOvCgiR6lqfXDdVFXd0o79cRzHcSISZQUwBShX1dWqWgM8AkxPqDMdeCB4Pwc4S0QkKH9EVfep6gdAeXA/x3Ecp4uJIgCGAmvjjiuCsqR1VLUO2A4UprlWgedFZLGIzMy86Y7jOE5bSKsC6kBOU9V1IjIYeEFE3lPVVxIrBcJhJsARRxzR2W10HMfpsURZAawDhsUdFwVlSeuISB5QAFS1dK2qhq+bgadIoRpS1ftVtVhViwcNGhShuY7jOE4UogiARcAYERkhIr0wo+7chDpzgWuD9zOAl1RVg/LLAy+hEcAYYKGI9BOR/gAi0g84B1je9u44juM4UUmrAlLVOhG5EZgH5AKzVXWFiNwBlKjqXGAW8KCIlAMfY0KCoN5jwDtAHfB1Va0XkSHAU2YnJg94SFWf64D+OY7jOCkQm6jvHxQXF2tJiYcMOI7jZIKILFbV4sRyjwR2HMfJUlwAOI7jZCkuABzHcbIUFwCO4zhZigsAx3GcLMUFgOM4TpbiAsBxHCdLcQHgOI6TpbgAcBzHyVJcADiO42QpLgAcx3GyFBcAjuM4WYoLAMdxnCzFBYDjOE6WkhUCYFnFNj6s2kUstv+kvnYcx+lounJP4E7jXx9dyvuVu+iTn8PowQdy1OD+jBnSn6OGHMhhBQdg+9IYTd4jDccSdz5HpPEvx45zc4S4S4k/UIW6mFJfr9TFYtTHlLpAGPXKy6F3Xg6983LpnZ9Dr1z7q4tZ3bqYUlev1NXbe224Z+bCLNiABwn6kStCTo6Ql2OvuUE/YqqoQkyVmNpnxdTq5+Za/dzgOhFp8TNVra/1MaW2PmZ9iSmKEvxDFcKe2X1zyAs+Jy8nh7wcoS6m7Kurp6YuRk19zF7rYuTkCH175dI3P4++vXPJz00+p4nFlJrgOwz7Zw1s/GwRIUesDY2/sR2n62fY1311sYbfNvyuCb5vwZ6X8HuO8t3FFOqD769e7RmqDxqfF/db5OfkkJMjTb7zunqlNhZruCY/J4deeTnk5wp5Sb4nVaW23n6n2vpY8Bk5we8Qrc1tIfGZ7sjPcoysEAA/ueR4Vm6spmzzTlZtqua196t48q3EbY2d1pATCERoLjxjqk0Gw84gP1c4IN8EQU0wkNXW2wDaWkQwwRwI6165OfTOz0VV2VsbY29dPXtr69lXFyMTuRwvhMEGwHjB25p25ohE6muOQH4w2ahXbfie0hFOFgjaqWG7MWGXl5NjQjxXyM/NaZgohJOe2vpgAhQIp5a+r7zwPjk5wcQjh9wce7ZS9d8mN43PowjEYjSZeMUL0fD7D4V+KBfrY9a/+pgSC4Rv2Nb4zxGCjgeTGeK+j8bvWhomXeEkI0ekyfscaWx3YlvDCVTp7efQOy837W+UCVkhAE444hBOOOKQJmXb99RSvnknldV7G8riH0aNO1Y07n04M9OGhyR8QFLdJ1fiZs25jbNnVaipj7GvNsa++hj7gkGktj4WPPyNs6/wfU7TUTY6cX0BGmaW4UMe/sU0HEiaPqQSd409kI2rk/jvJ+6jbEDIzSE/vi/BrJX4/xRxKy0bHIJ7x60cwsGqV17wF7yPqbJrXz27a+rYU1PP7tp6du+rozamDXXyc4Veubnk50mT7zB+RRS2OxZ+J+HvGgweNXUx9gV/9r6eHBH65OfQJz/X/vJMMOTnCoI0fNfhdxNTmnzfDe9VG76Dhu/dGmjPTq61uXGlZvcLv5/wd6iPxahXJS/H+pwbvIbX1dYrNcHzFb+Sys2R4HvKoVeuvc/LyWn4PWpjjSu3unr7jJwkv58q1KvVCYVuXXBt43PftG2Jv0H4HMSUxucgWDnX1mtKNW74DDZZUQbH4f+38DVc7QKNv3Pw/zkUwiYU4gRE8Bqej/8c1ebPcfjdhHXDVWe4qlPiVtexuNU2tlJrXN3lNHx3Tf7vtxNZIQCSUXBAPpOOPCR9RcdxnB5KVhiBHcdxnOa4AHAcx8lSXAA4juNkKZEEgIhME5GVIlIuIjcnOd9bRB4Nzr8pIsPjzt0SlK8UkXOj3tNxHMfpWNIKABHJBe4DzgPGAleIyNiEatcBW1V1NHAvcFdw7VjgcmAcMA34pYjkRryn4ziO04FEWQFMAcpVdbWq1gCPANMT6kwHHgjezwHOEvPvmg48oqr7VPUDoDy4X5R7Oo7jOB1IFAEwFFgbd1wRlCWto6p1wHagsIVro9wTABGZKSIlIlJSWVkZobmO4zhOFLq9EVhV71fVYlUtHjRoUFc3x3Ecp8cQJRBsHTAs7rgoKEtWp0JE8oACoCrNtenu2YzFixdvEZEPI7Q5GQOBLa28dn/G+51deL+zi6j9PjJZYRQBsAgYIyIjsEH6cuDKhDpzgWuB14EZwEuqqiIyF3hIRO4BDgfGAAuxSOl092yGqrZ6CSAiJapa3Nrr91e839mF9zu7aGu/0woAVa0TkRuBeUAuMFtVV4jIHUCJqs4FZgEPikg58DE2oBPUewx4B6gDvq6q9UHDm92ztZ1wHMdxMkdak1Z4f8RnCNmF9zu78H63jm5vBG5H7u/qBnQR3u/swvudXbSp31mzAnAcx3Gakk0rAMdxHCeOHi8AsinnkIjMFpHNIrI8rmyAiLwgImXBa4/bBEFEhonIfBF5R0RWiMi3gvIe3XcR6SMiC0Xk7aDftwflI4KcXOVBjq5eXd3WjiBIK/OWiPwtOO7x/RaRNSJSKiJLRaQkKGv1c96jBUAW5hz6PZZzKZ6bgb+r6hjg78FxT6MO+DdVHQucDHw9+J17et/3AWeq6vHARGCaiJyM5eK6N8jNtRXL1dUT+RbwbtxxtvR7qqpOjDP+tvo579ECgCzLOaSqr2BuuPHE52l6ALioUxvVCajqBlVdEryvxgaFofTwvquxMzjMD/4UOBPLyQU9sN8AIlIEfAb4bXAsZEG/U9Dq57ynC4DIOYd6MENUdUPwfiMwpCsb09EEqchPAN4kC/oeqEGWApuBF4D3gW1BTi7ouc/8T4H/A8SC40Kyo98KPC8ii0VkZlDW6uc8a/cEzkaC6Owe6/YlIgcCTwDfVtUdEreJdk/texBYOVFEDgaeAo7p4iZ1OCJyAbBZVReLyBld3Z5O5jRVXScig4EXROS9+JOZPuc9fQUQJY9RT2eTiBwGELxu7uL2dAgiko8N/n9S1SeD4qzoO4CqbgPmA6cABwc5uaBnPvOnAheKyBpMrXsm8DN6fr9R1XXB62ZM4E+hDc95TxcADXmMAo+Ay7G8RdlEmKeJ4PUvXdiWDiHQ/84C3lXVe+JO9ei+i8igYOaPiBwAnI3ZP+ZjObmgB/ZbVW9R1SJVHY79n35JVa+ih/dbRPqJSP/wPXAOsJw2POc9PhBMRM7H9IVhzqEfdnGTOgwReRg4A8sQuAn4AfBn4DHgCOBD4FJVTTQU79eIyGnAAqCURp3wv2N2gB7bdxE5DjP65WKTucdU9Q4RGYnNjAcAbwFXq+q+rmtpxxGogL6rqhf09H4H/XsqOMwDHlLVH4pIIa18znu8AHAcx3GS09NVQI7jOE4KXAA4juNkKS4AHMdxshQXAI7jOFmKCwDHcZwsxQWA4zhOluICwHEcJ0txAeA4jpOl/H+jVd0BOcTltQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOfhqHd1Ha_e"
      },
      "source": [
        "import os\n",
        "class BlackBoxWrapper():\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.model = Blackbox_IDS(input_dim, output_dim).to(device)\n",
        "        if os.path.exists('model/IDS.pth'):\n",
        "            param = torch.load('model/IDS.pth')\n",
        "            self.model.load_state_dict(param)\n",
        "        else:\n",
        "            print('BlackBox Model not saved yet')\n",
        "\n",
        "    def __call__(self, x):\n",
        "        # x = torch.reshape(x, [x.shape[0],1,-1])\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "    def eval(self):\n",
        "        self.model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzwarypvA1vJ"
      },
      "source": [
        "#RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnSKuzmbA7e5"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "\n",
        "class Blackbox_IDS(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "        self.lstm = nn.LSTM(input_dim, 80, 1, batch_first=True)\n",
        "        self.layer = nn.Sequential(\n",
        "            nn.Dropout(0.1),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(80, 40),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(40, 20),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(20, 5),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(5, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lstm(x)[0]\n",
        "        x = torch.squeeze(x)\n",
        "        x = self.layer(x)\n",
        "        x = torch.nn.Sigmoid()(x)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXCJUXXuBF3t"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from idsgan_preprocessor import preprocess3, create_batch1\n",
        "#from model.model_class import Blackbox_IDS\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(12345)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train = pd.read_csv('dataset/KDDTrain+.csv')\n",
        "test = pd.read_csv('dataset/KDDTest+.csv')\n",
        "\n",
        "trainx, trainy, testx, testy = preprocess3(train, test)\n",
        "trainx, _, trainy, _ = train_test_split(trainx, trainy, test_size=0.5, random_state=12345)\n",
        "print(trainx.shape)\n",
        "\n",
        "input_dim = trainx.shape[1]\n",
        "output_dim = 1\n",
        "batch_size = 64\n",
        "tr_N = len(trainx)\n",
        "te_N = len(testx)\n",
        "ids_model = Blackbox_IDS(input_dim, output_dim).to(device)\n",
        "optimizer = torch.optim.Adam(ids_model.parameters(), lr=0.001)\n",
        "loss_f = torch.nn.BCELoss()\n",
        "max_epoch = 50\n",
        "train_losses, test_losses = [], []\n",
        "\n",
        "\n",
        "def train(x, y):\n",
        "    ids_model.train()\n",
        "    batch_x, batch_y = create_batch1(x, y, batch_size)\n",
        "    run_loss = 0\n",
        "    for x, y in zip(batch_x, batch_y):\n",
        "        ids_model.zero_grad()\n",
        "        x_t = torch.FloatTensor(x).to(device)\n",
        "        x_t = torch.reshape(x_t, [batch_size,1,-1])\n",
        "        y_t = torch.FloatTensor(y).to(device)\n",
        "        y_t = torch.reshape(y_t, [batch_size])\n",
        "\n",
        "        out = ids_model(x_t)\n",
        "        out = torch.reshape(out, [batch_size])\n",
        "\n",
        "        loss = loss_f(out, y_t)\n",
        "\n",
        "        run_loss += loss.item()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return run_loss / tr_N\n",
        "\n",
        "\n",
        "def test(x, y):\n",
        "    ids_model.eval()\n",
        "    batch_x, batch_y = create_batch1(x, y, batch_size)\n",
        "    run_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in zip(batch_x, batch_y):\n",
        "            x_t = torch.FloatTensor(x).to(device)\n",
        "            x_t = torch.reshape(x_t, [batch_size,1,-1])\n",
        "            y_t = torch.FloatTensor(y).to(device)\n",
        "            y_t = torch.reshape(y_t, [batch_size])\n",
        "\n",
        "            out = ids_model(x_t)\n",
        "            out = torch.reshape(out, [batch_size])\n",
        "\n",
        "            loss = loss_f(out, y_t)\n",
        "\n",
        "            run_loss += loss.item()\n",
        "    return run_loss / te_N\n",
        "\n",
        "\n",
        "def main():\n",
        "    print(\"IDS start training\")\n",
        "    print(\"-\" * 100)\n",
        "    for epoch in range(max_epoch):\n",
        "        train_loss = train(trainx, trainy)\n",
        "        test_loss = test(testx, testy)\n",
        "\n",
        "        x_t = torch.FloatTensor(testx).to(device)\n",
        "        x_t = torch.reshape(x_t, [x_t.shape[0],1,-1])\n",
        "\n",
        "        out = ids_model(x_t)\n",
        "        out = torch.reshape(out, [len(testy)])\n",
        "        out_np = out.cpu().detach().numpy()\n",
        "\n",
        "        ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "\n",
        "        correct = np.sum(np.equal(ids_pred_label, testy))\n",
        "        acc = correct / len(testy)\n",
        "\n",
        "        train_losses.append(train_loss)\n",
        "        test_losses.append(test_loss)\n",
        "\n",
        "        print(f\"{epoch} : {train_loss} \\t {test_loss} \\t {acc}\")\n",
        "\n",
        "    print(\"IDS finished training\")\n",
        "\n",
        "    x_t = torch.FloatTensor(testx).to(device)\n",
        "    x_t = torch.reshape(x_t, [x_t.shape[0],1,-1])\n",
        "    out = ids_model(x_t)\n",
        "    out = torch.reshape(out, [len(testy)])\n",
        "    out_np = out.cpu().detach().numpy()\n",
        "\n",
        "    ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "\n",
        "    conf_mat = confusion_matrix(testy, ids_pred_label)\n",
        "    print(conf_mat)\n",
        "\n",
        "    torch.save(ids_model.state_dict(), 'model/IDS.pth')\n",
        "    plt.plot(train_losses, label=\"train\")\n",
        "    plt.plot(test_losses, label=\"test\")\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsE2XfEVH-6R"
      },
      "source": [
        "main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wwe8EtoiIwpX"
      },
      "source": [
        "import os\n",
        "class BlackBoxWrapper():\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.model = Blackbox_IDS(input_dim, output_dim).to(device)\n",
        "        if os.path.exists('model/IDS.pth'):\n",
        "            param = torch.load('model/IDS.pth')\n",
        "            self.model.load_state_dict(param)\n",
        "        else:\n",
        "            print('BlackBox Model not saved yet')\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = torch.reshape(x, [x.shape[0],1,-1])\n",
        "        x = self.model(x)\n",
        "        return x\n",
        "\n",
        "    def eval(self):\n",
        "        self.model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2XL3mHILuOk"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD3QZZqrXScZ"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE8kaO1eKH5p"
      },
      "source": [
        "#Training Fuzzyness IDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlzE5lLPMroo"
      },
      "source": [
        "percent = 0.5\n",
        "normal_index = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rjc14eNYMV9o"
      },
      "source": [
        "##Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJmQ7-BXMMUg"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pprint\n",
        "from pickle import dump\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "debug = False\n",
        "\n",
        "NSLKDD_ATTACK_DICT = {\n",
        "    'DoS': ['apache2', 'back', 'land', 'neptune', 'mailbomb', 'pod', 'processtable', 'smurf', 'teardrop', 'udpstorm'],\n",
        "    'Probe': ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan'],\n",
        "    'Privilege': ['buffer_overflow', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm','httptunnel'],\n",
        "    'Access': ['ftp_write', 'guess_passwd', 'imap', 'multihop', 'named', 'phf', 'sendmail',\n",
        "               'snmpgetattack', 'snmpguess', 'spy', 'warezclient', 'warezmaster', 'xlock', 'xsnoop','worm'],\n",
        "    'Normal': ['normal']\n",
        "}\n",
        "\n",
        "\n",
        "NSLKDD_COL_NAMES = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
        "                    \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "                    \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n",
        "                    \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "                    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
        "                    \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "                    \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
        "                    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "                    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
        "                    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"class\", \"difficulty_level\"]\n",
        "\n",
        "binary = False\n",
        "\n",
        "NSLKDD_ATTACK_MAP = dict()\n",
        "for k, v in NSLKDD_ATTACK_DICT.items():\n",
        "    for att in v:\n",
        "        if not binary:\n",
        "            NSLKDD_ATTACK_MAP[att] = k\n",
        "        else:\n",
        "            if k == 'Normal':\n",
        "                NSLKDD_ATTACK_MAP[att] = k\n",
        "            else:\n",
        "                NSLKDD_ATTACK_MAP[att] = 'Attack'\n",
        "\n",
        "\n",
        "FEATURE_CATEGORY = dict()\n",
        "\n",
        "for i in range(9):\n",
        "    FEATURE_CATEGORY.setdefault('intrinsic', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(9, 22):\n",
        "    FEATURE_CATEGORY.setdefault('content', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(22, 31):\n",
        "    FEATURE_CATEGORY.setdefault('time_based', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(32, 41):\n",
        "    FEATURE_CATEGORY.setdefault('host_based', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "\n",
        "FEATURE_CATEGORY_REV = dict()\n",
        "\n",
        "for k,v in FEATURE_CATEGORY.items():\n",
        "    for cat in list(v):\n",
        "        FEATURE_CATEGORY_REV[cat] = k\n",
        "\n",
        "\n",
        "FEATURE_TYPE = dict()\n",
        "\n",
        "for i in [2, 3, 4]:\n",
        "    FEATURE_TYPE.setdefault('categorical', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [7, 12, 14, 20, 21, 22]:\n",
        "    FEATURE_TYPE.setdefault('binary', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [8, 9, 15, 43] + list(range(23, 42)):\n",
        "    FEATURE_TYPE.setdefault('discrete', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [1, 5, 6, 10, 11, 13, 16, 17, 18, 19]:\n",
        "    FEATURE_TYPE.setdefault('continuous', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "\n",
        "def create_batch1(x, y, batch_size):\n",
        "    a = list(range(len(x)))\n",
        "    np.random.shuffle(a)\n",
        "    x = x[a]\n",
        "    y = y[a]\n",
        "\n",
        "    batch_x = [x[batch_size * i: (i + 1) * batch_size, :].tolist() for i in range(len(x) // batch_size)]\n",
        "    batch_y = [y[batch_size * i: (i + 1) * batch_size].tolist() for i in range(len(x) // batch_size)]\n",
        "    return batch_x, batch_y\n",
        "\n",
        "\n",
        "def create_batch2(x, batch_size):\n",
        "    a = list(range(len(x)))\n",
        "    np.random.shuffle(a)\n",
        "    x = x[a]\n",
        "    batch_x = [x[batch_size * i: (i + 1) * batch_size, :] for i in range(len(x) // batch_size)]\n",
        "    return np.array(batch_x).astype(float)\n",
        "\n",
        "\n",
        "def preprocess4(train, functional_categories=None):\n",
        "    if functional_categories is None:\n",
        "        functional_categories = ['intrinsic', 'time_based']\n",
        "    train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "\n",
        "    raw_attack = np.array(train[train[\"class\"] == 1])[:, :-1]\n",
        "    normal = np.array(train[train[\"class\"] == 0])[:, :-1]\n",
        "    true_label = train[\"class\"]\n",
        "\n",
        "    del train[\"class\"]\n",
        "\n",
        "    train_columns = list(train.columns)\n",
        "\n",
        "    modification_mask = np.ones((len(train.columns)))\n",
        "    if functional_categories is None:\n",
        "        functional_categories = ['intrinsic', 'time_based']\n",
        "\n",
        "    for cat_ in functional_categories:\n",
        "        cat_cols = list(FEATURE_CATEGORY[cat_])\n",
        "        for cat_col in cat_cols:\n",
        "            for idx in range(len(train_columns)):\n",
        "                if str(train_columns[idx]).startswith(cat_col):\n",
        "                    modification_mask[idx] = 0\n",
        "\n",
        "    return train, raw_attack, normal, true_label, modification_mask\n",
        "\n",
        "\n",
        "# all\n",
        "def preprocess2(train, test, data_generation=False, attack_map=None):\n",
        "    train.drop(['difficulty_level'], axis=1, inplace=True)\n",
        "    test.drop(['difficulty_level'], axis=1, inplace=True)\n",
        "\n",
        "    train.sample(frac=1)\n",
        "\n",
        "    obj_cols = train.select_dtypes(include=['object']).copy().columns\n",
        "    obj_cols = list(obj_cols)\n",
        "\n",
        "    for col in obj_cols:\n",
        "\n",
        "        if col != 'class':\n",
        "            onehot_cols_train = pd.get_dummies(train[col], prefix=col, dtype='float64')\n",
        "            onehot_cols_test = pd.get_dummies(test[col], prefix=col, dtype='float64')\n",
        "\n",
        "            idx = 0\n",
        "            for find_col_idx in range(len(list(train.columns))):\n",
        "                if list(train.columns)[find_col_idx] == col:\n",
        "                    idx = find_col_idx\n",
        "\n",
        "            itr = 0\n",
        "            for new_col in list(onehot_cols_train.columns):\n",
        "                train.insert(idx + itr + 1, new_col, onehot_cols_train[new_col].values, True)\n",
        "\n",
        "                if new_col not in list(onehot_cols_test.columns):\n",
        "                    zero_col = np.zeros(test.values.shape[0])\n",
        "                    test.insert(idx + itr + 1, new_col, zero_col, True)\n",
        "                else:\n",
        "                    test.insert(idx + itr + 1, new_col, onehot_cols_test[new_col].values, True)\n",
        "\n",
        "                itr += 1\n",
        "\n",
        "            del train[col]\n",
        "            del test[col]\n",
        "\n",
        "    if not data_generation:\n",
        "        train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "        test[\"class\"] = test[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "    else:\n",
        "        train[\"class\"] = train[\"class\"].map(attack_map)\n",
        "        test[\"class\"] = test[\"class\"].map(attack_map)\n",
        "\n",
        "    columns = train.columns\n",
        "    train[\"num_outbound_cmds\"] = train[\"num_outbound_cmds\"].map(lambda x: 0)\n",
        "    test[\"num_outbound_cmds\"] = test[\"num_outbound_cmds\"].map(lambda x: 0)\n",
        "\n",
        "    if not data_generation:\n",
        "        trainx, trainy = np.array(train[train.columns[train.columns != \"class\"]]), np.array(train[\"class\"])\n",
        "        testx, testy = np.array(test[train.columns[train.columns != \"class\"]]), np.array(test[\"class\"])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(trainx)\n",
        "\n",
        "        trainx = scaler.transform(trainx)\n",
        "        testx = scaler.transform(testx)\n",
        "\n",
        "        return trainx, trainy, testx, testy\n",
        "    else:\n",
        "        trainx, trainy = train[train.columns[train.columns != \"class\"]], train[\"class\"]\n",
        "        testx, testy = test[train.columns[train.columns != \"class\"]], test[\"class\"]\n",
        "\n",
        "        columns = trainx.columns\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(trainx)\n",
        "\n",
        "        trainx = scaler.transform(trainx)\n",
        "        testx = scaler.transform(testx)\n",
        "\n",
        "        train_processed = pd.DataFrame(trainx, columns=columns)\n",
        "        train_processed[\"class\"] = trainy\n",
        "\n",
        "        test_processed = pd.DataFrame(testx, columns=columns)\n",
        "        test_processed[\"class\"] = testy\n",
        "\n",
        "        return train_processed, test_processed\n",
        "\n",
        "\n",
        "def preprocess3(train, test):\n",
        "    train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "    test[\"class\"] = test[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "\n",
        "    train_np = train.values\n",
        "    test_np = test.values\n",
        "\n",
        "    trainx, trainy = train_np[:, :-1], train_np[:, -1]\n",
        "    testx, testy = test_np[:, :-1], test_np[:, -1]\n",
        "\n",
        "    testy = np.array(testy).astype(int)\n",
        "    trainy = np.array(trainy).astype(int)\n",
        "\n",
        "    return trainx, trainy, testx, testy\n",
        "\n",
        "\n",
        "\n",
        "def preprocess5(train):\n",
        "    #print(train[\"class\"])\n",
        "    #train[\"class\"] = train[\"class\"].map(NSLKDD_ATTACK_MAP)\n",
        "    train[\"class\"] = train[\"class\"].astype('category')\n",
        "    #print(train[\"class\"].cat.categories)\n",
        "    cat_dict_r = dict(enumerate(train[\"class\"].cat.categories))\n",
        "    cat_dict = dict()\n",
        "\n",
        "    #print(\"Cat dict R\")\n",
        "    #print(cat_dict_r)\n",
        "    for key, value in cat_dict_r.items():\n",
        "        cat_dict[value] = key\n",
        "\n",
        "    #print(\"Cat dict\")\n",
        "    #print(cat_dict)\n",
        "\n",
        "    train = train.replace({\"class\": cat_dict})\n",
        "    #print(train[\"class\"])\n",
        "    train[\"class\"] = train[\"class\"].astype('int64')\n",
        "\n",
        "    train_np = train.values\n",
        "\n",
        "    trainx, trainy = train_np[:, :-1], train_np[:, -1]\n",
        "    #print(trainy)\n",
        "    trainy = np.array(trainy).astype(int)\n",
        "\n",
        "    return trainx, trainy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ittZBxIvMiKW"
      },
      "source": [
        "##Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35uxDYjwJ-mo"
      },
      "source": [
        "from idsgan_preprocessor import preprocess3\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "import numpy as np\n",
        "from math import log\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from keras.utils.np_utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "train = pd.read_csv('dataset/KDDTrain+.csv')\n",
        "test = pd.read_csv('dataset/KDDTest+.csv')\n",
        "\n",
        "trainx, trainy = preprocess5(train)\n",
        "testx, testy = preprocess5(test)\n",
        "#print(np.unique(trainy, return_counts=True))\n",
        "#print(np.unique(testy, return_counts=True))\n",
        "\n",
        "trainx, valx, trainy, valy = train_test_split(trainx, trainy, test_size=percent, random_state=12345)\n",
        "\n",
        "# valx = np.array([])\n",
        "# valy = np.array([1,2,3,0,4])\n",
        "#print(trainy[0], testy[0])\n",
        "\n",
        "trainy = pd.DataFrame(to_categorical(trainy, 5)).values\n",
        "valy = pd.DataFrame(to_categorical(valy, 5)).values\n",
        "testy = pd.DataFrame(to_categorical(testy, 5)).values\n",
        "\n",
        "#print(trainy[0], testy[0])\n",
        "print(trainx.shape, trainy.shape)\n",
        "print(valx.shape, valy.shape)\n",
        "print(testx.shape, testy.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8AeDw9dLmXZ"
      },
      "source": [
        "def train_classifier(X, y):\n",
        "    # create model\n",
        "    model = Sequential()\n",
        "    #model.add(Dense(40, input_dim=122, kernel_initializer='random_normal',bias_initializer='random_normal', activation='relu'))\n",
        "    model.add(Dense(80, input_dim=122, activation='relu'))\n",
        "    #model.add(Dense(60, activation='relu'))\n",
        "    model.add(Dense(40, activation='relu'))\n",
        "    model.add(Dense(20, activation='relu'))\n",
        "    model.add(Dense(5, activation='softmax'))\n",
        "\n",
        "    # Compile model\n",
        "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(X, y, epochs=200, batch_size=256)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "model = train_classifier(trainx, trainy)\n",
        "\n",
        "score = model.evaluate(testx, testy)\n",
        "\n",
        "print(\"Accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW5heIY9VceT"
      },
      "source": [
        "membershipVectors = model.predict(valx) # Membership Matriloc\n",
        "binarizedLabels = (membershipVectors == membershipVectors.max(axis=1, keepdims=1)).astype(float)\n",
        "\n",
        "def F(V):\n",
        "    def inner(mu):\n",
        "        try:\n",
        "            result = (mu * log(mu, 2)) + ((1 - mu) * log(1 - mu, 2))\n",
        "        except:\n",
        "            result = 0\n",
        "        return result\n",
        "    return - np.mean(list(map(inner, V)))\n",
        "\n",
        "fuzziness = np.array(list(map(F, membershipVectors)))\n",
        "\n",
        "print(fuzziness[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dqttIel6Vvt8"
      },
      "source": [
        "#lowFuzzinessIndices = np.append(np.where( fuzziness >= 9/10), np.where( fuzziness <= 1/10 ) )\n",
        "lowFuzzinessIndices = np.where( fuzziness <= 1/10 )\n",
        "highFuzzinessIndices = np.logical_and(fuzziness >= 2/6, fuzziness <= 2/3)\n",
        "\n",
        "# Fuzziness values >= 5/6\n",
        "lowFuzzinessGroup = valx[ lowFuzzinessIndices ]\n",
        "lowFuzzinessLabels = binarizedLabels[ lowFuzzinessIndices ]\n",
        "\n",
        "# Fuzziness values 2/6 <= x <= 2/3\n",
        "highFuzzinessGroup = valx[ highFuzzinessIndices ]\n",
        "highFuzzinessLabels = binarizedLabels [ highFuzzinessIndices ]\n",
        "\n",
        "# Append new samples to training set\n",
        "#train_set_x = np.concatenate((train_set_x, lowFuzzinessGroup, highFuzzinessGroup), axis=0)\n",
        "\n",
        "#train_set_y = np.concatenate((train_set_y, lowFuzzinessLabels, highFuzzinessLabels), axis=0)\n",
        "trainx = np.concatenate((trainx, lowFuzzinessGroup), axis=0)\n",
        "\n",
        "trainy = np.concatenate((trainy, lowFuzzinessLabels), axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCp79Sf_WRVd"
      },
      "source": [
        "print(trainx.shape)\n",
        "print(trainy.shape)\n",
        "model = train_classifier(trainx, trainy)\n",
        "score = model.evaluate(testx, testy)\n",
        "\n",
        "print(\"Accuracy: \", score[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDRYB67ZY3vD"
      },
      "source": [
        "##BlackBox IDS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJhC02EoZU2K"
      },
      "source": [
        "def getPred(x):\n",
        "    y = model.predict(x)\n",
        "    y_c = y.argmax(axis=-1)\n",
        "    y_bin = []\n",
        "    for val in y_c:\n",
        "        if val == normal_index:\n",
        "            y_bin.append(0)\n",
        "        else:\n",
        "            y_bin.append(1)\n",
        "    return np.array(y_bin)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIt8yZSXY2l9"
      },
      "source": [
        "import os\n",
        "class BlackBoxWrapper():\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        self.input_dim = input_dim\n",
        "        self.output_dim = output_dim\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = x.cpu().detach().numpy()\n",
        "        x = getPred(x)\n",
        "        x = torch.FloatTensor(x).to(device)\n",
        "        return x\n",
        "\n",
        "    def eval(self):\n",
        "        x = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx4MP_miFIsp"
      },
      "source": [
        "# IDSGAN Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "253J7gIt-gxw"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pprint\n",
        "from pickle import dump\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "\n",
        "NSLKDD_COL_NAMES = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
        "                    \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "                    \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n",
        "                    \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "                    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
        "                    \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "                    \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
        "                    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "                    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
        "                    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"class\", \"difficulty_level\"]\n",
        "\n",
        "FEATURE_CATEGORY = dict()\n",
        "\n",
        "for i in range(9):\n",
        "    FEATURE_CATEGORY.setdefault('intrinsic', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(9, 22):\n",
        "    FEATURE_CATEGORY.setdefault('content', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(22, 31):\n",
        "    FEATURE_CATEGORY.setdefault('time_based', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "for i in range(32, 41):\n",
        "    FEATURE_CATEGORY.setdefault('host_based', []).append(NSLKDD_COL_NAMES[i])\n",
        "\n",
        "\n",
        "FEATURE_CATEGORY_REV = dict()\n",
        "\n",
        "for k,v in FEATURE_CATEGORY.items():\n",
        "    for cat in list(v):\n",
        "        FEATURE_CATEGORY_REV[cat] = k\n",
        "\n",
        "\n",
        "FEATURE_TYPE = dict()\n",
        "\n",
        "for i in [2, 3, 4]:\n",
        "    FEATURE_TYPE.setdefault('categorical', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [7, 12, 14, 20, 21, 22]:\n",
        "    FEATURE_TYPE.setdefault('binary', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [8, 9, 15, 43] + list(range(23, 42)):\n",
        "    FEATURE_TYPE.setdefault('discrete', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "for i in [1, 5, 6, 10, 11, 13, 16, 17, 18, 19]:\n",
        "    FEATURE_TYPE.setdefault('continuous', []).append(NSLKDD_COL_NAMES[i - 1])\n",
        "\n",
        "\n",
        "def create_batch1(x, y, batch_size):\n",
        "    a = list(range(len(x)))\n",
        "    np.random.shuffle(a)\n",
        "    x = x[a]\n",
        "    y = y[a]\n",
        "\n",
        "    batch_x = [x[batch_size * i: (i + 1) * batch_size, :].tolist() for i in range(len(x) // batch_size)]\n",
        "    batch_y = [y[batch_size * i: (i + 1) * batch_size].tolist() for i in range(len(x) // batch_size)]\n",
        "    return batch_x, batch_y\n",
        "\n",
        "\n",
        "def create_batch2(x, batch_size):\n",
        "    a = list(range(len(x)))\n",
        "    np.random.shuffle(a)\n",
        "    x = x[a]\n",
        "    batch_x = [x[batch_size * i: (i + 1) * batch_size, :] for i in range(len(x) // batch_size)]\n",
        "    return np.array(batch_x).astype(float)\n",
        "\n",
        "\n",
        "def preprocess4(train, functional_categories=None):\n",
        "    if functional_categories is None:\n",
        "        functional_categories = ['intrinsic', 'time_based']\n",
        "    train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "\n",
        "    raw_attack = np.array(train[train[\"class\"] == 1])[:, :-1]\n",
        "    normal = np.array(train[train[\"class\"] == 0])[:, :-1]\n",
        "    true_label = train[\"class\"]\n",
        "\n",
        "    del train[\"class\"]\n",
        "\n",
        "    train_columns = list(train.columns)\n",
        "\n",
        "    modification_mask = np.ones((len(train.columns)))\n",
        "    if functional_categories is None:\n",
        "        functional_categories = ['intrinsic', 'time_based']\n",
        "\n",
        "    for cat_ in functional_categories:\n",
        "        cat_cols = list(FEATURE_CATEGORY[cat_])\n",
        "        for cat_col in cat_cols:\n",
        "            for idx in range(len(train_columns)):\n",
        "                if str(train_columns[idx]).startswith(cat_col):\n",
        "                    modification_mask[idx] = 0\n",
        "\n",
        "    return train, raw_attack, normal, true_label, modification_mask\n",
        "\n",
        "\n",
        "# all\n",
        "def preprocess2(train, test, data_generation=False, attack_map=None):\n",
        "    train.drop(['difficulty_level'], axis=1, inplace=True)\n",
        "    test.drop(['difficulty_level'], axis=1, inplace=True)\n",
        "\n",
        "    train.sample(frac=1)\n",
        "\n",
        "    obj_cols = train.select_dtypes(include=['object']).copy().columns\n",
        "    obj_cols = list(obj_cols)\n",
        "\n",
        "    for col in obj_cols:\n",
        "\n",
        "        if col != 'class':\n",
        "            onehot_cols_train = pd.get_dummies(train[col], prefix=col, dtype='float64')\n",
        "            onehot_cols_test = pd.get_dummies(test[col], prefix=col, dtype='float64')\n",
        "\n",
        "            idx = 0\n",
        "            for find_col_idx in range(len(list(train.columns))):\n",
        "                if list(train.columns)[find_col_idx] == col:\n",
        "                    idx = find_col_idx\n",
        "\n",
        "            itr = 0\n",
        "            for new_col in list(onehot_cols_train.columns):\n",
        "                train.insert(idx + itr + 1, new_col, onehot_cols_train[new_col].values, True)\n",
        "\n",
        "                if new_col not in list(onehot_cols_test.columns):\n",
        "                    zero_col = np.zeros(test.values.shape[0])\n",
        "                    test.insert(idx + itr + 1, new_col, zero_col, True)\n",
        "                else:\n",
        "                    test.insert(idx + itr + 1, new_col, onehot_cols_test[new_col].values, True)\n",
        "\n",
        "                itr += 1\n",
        "\n",
        "            del train[col]\n",
        "            del test[col]\n",
        "\n",
        "    if not data_generation:\n",
        "        train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "        test[\"class\"] = test[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "    else:\n",
        "        train[\"class\"] = train[\"class\"].map(attack_map)\n",
        "        test[\"class\"] = test[\"class\"].map(attack_map)\n",
        "\n",
        "    columns = train.columns\n",
        "    train[\"num_outbound_cmds\"] = train[\"num_outbound_cmds\"].map(lambda x: 0)\n",
        "    test[\"num_outbound_cmds\"] = test[\"num_outbound_cmds\"].map(lambda x: 0)\n",
        "\n",
        "    if not data_generation:\n",
        "        trainx, trainy = np.array(train[train.columns[train.columns != \"class\"]]), np.array(train[\"class\"])\n",
        "        testx, testy = np.array(test[train.columns[train.columns != \"class\"]]), np.array(test[\"class\"])\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(trainx)\n",
        "\n",
        "        trainx = scaler.transform(trainx)\n",
        "        testx = scaler.transform(testx)\n",
        "\n",
        "        return trainx, trainy, testx, testy\n",
        "    else:\n",
        "        trainx, trainy = train[train.columns[train.columns != \"class\"]], train[\"class\"]\n",
        "        testx, testy = test[train.columns[train.columns != \"class\"]], test[\"class\"]\n",
        "\n",
        "        columns = trainx.columns\n",
        "        scaler = StandardScaler()\n",
        "        scaler.fit(trainx)\n",
        "\n",
        "        trainx = scaler.transform(trainx)\n",
        "        testx = scaler.transform(testx)\n",
        "\n",
        "        train_processed = pd.DataFrame(trainx, columns=columns)\n",
        "        train_processed[\"class\"] = trainy\n",
        "\n",
        "        test_processed = pd.DataFrame(testx, columns=columns)\n",
        "        test_processed[\"class\"] = testy\n",
        "\n",
        "        return train_processed, test_processed\n",
        "\n",
        "\n",
        "def preprocess3(train, test):\n",
        "    train[\"class\"] = train[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "    test[\"class\"] = test[\"class\"].map(lambda x: 1 if x != \"Normal\" else 0)\n",
        "\n",
        "    train_np = train.values\n",
        "    test_np = test.values\n",
        "\n",
        "    trainx, trainy = train_np[:, :-1], train_np[:, -1]\n",
        "    testx, testy = test_np[:, :-1], test_np[:, -1]\n",
        "\n",
        "    testy = np.array(testy).astype(int)\n",
        "    trainy = np.array(trainy).astype(int)\n",
        "\n",
        "    return trainx, trainy, testx, testy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glF2qPVMh70D"
      },
      "source": [
        "# IDSGAN Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gH35a9fAORS"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from idsgan_preprocessor import preprocess2\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "\n",
        "import random\n",
        "\n",
        "random.seed(12345)\n",
        "\n",
        "NSLKDD_ATTACK_DICT = {\n",
        "    'DoS': ['apache2', 'back', 'land', 'neptune', 'mailbomb', 'pod', 'processtable', 'smurf', 'teardrop', 'udpstorm'],\n",
        "    'Probe': ['ipsweep', 'mscan', 'nmap', 'portsweep', 'saint', 'satan'],\n",
        "    'U2R': ['buffer_overflow', 'loadmodule', 'perl', 'ps', 'rootkit', 'sqlattack', 'xterm', 'httptunnel'],\n",
        "    'R2L': ['ftp_write', 'guess_passwd', 'imap', 'multihop', 'named', 'phf', 'sendmail',\n",
        "            'snmpgetattack', 'snmpguess', 'spy', 'warezclient', 'warezmaster', 'xlock', 'xsnoop', 'worm'],\n",
        "    'Normal': ['normal']\n",
        "}\n",
        "\n",
        "NSLKDD_COL_NAMES = [\"duration\", \"protocol_type\", \"service\", \"flag\", \"src_bytes\",\n",
        "                    \"dst_bytes\", \"land\", \"wrong_fragment\", \"urgent\", \"hot\", \"num_failed_logins\",\n",
        "                    \"logged_in\", \"num_compromised\", \"root_shell\", \"su_attempted\", \"num_root\",\n",
        "                    \"num_file_creations\", \"num_shells\", \"num_access_files\", \"num_outbound_cmds\",\n",
        "                    \"is_host_login\", \"is_guest_login\", \"count\", \"srv_count\", \"serror_rate\",\n",
        "                    \"srv_serror_rate\", \"rerror_rate\", \"srv_rerror_rate\", \"same_srv_rate\",\n",
        "                    \"diff_srv_rate\", \"srv_diff_host_rate\", \"dst_host_count\", \"dst_host_srv_count\",\n",
        "                    \"dst_host_same_srv_rate\", \"dst_host_diff_srv_rate\", \"dst_host_same_src_port_rate\",\n",
        "                    \"dst_host_srv_diff_host_rate\", \"dst_host_serror_rate\", \"dst_host_srv_serror_rate\",\n",
        "                    \"dst_host_rerror_rate\", \"dst_host_srv_rerror_rate\", \"class\", \"difficulty_level\"]\n",
        "\n",
        "binary = False\n",
        "\n",
        "NSLKDD_ATTACK_MAP = dict()\n",
        "for k, v in NSLKDD_ATTACK_DICT.items():\n",
        "    for att in v:\n",
        "        if not binary:\n",
        "            NSLKDD_ATTACK_MAP[att] = k\n",
        "        else:\n",
        "            if k == 'Normal':\n",
        "                NSLKDD_ATTACK_MAP[att] = k\n",
        "            else:\n",
        "                NSLKDD_ATTACK_MAP[att] = 'Attack'\n",
        "\n",
        "max_weight = 100\n",
        "\n",
        "considered_attack_list = ['DoS']\n",
        "\n",
        "train = pd.read_csv('dataset/KDDTrain+.txt', delimiter=',', header=None, names=NSLKDD_COL_NAMES)\n",
        "test = pd.read_csv('dataset/KDDTest+.txt', delimiter=',', header=None, names=NSLKDD_COL_NAMES)\n",
        "\n",
        "train, test = preprocess2(train, test, data_generation=True, attack_map=NSLKDD_ATTACK_MAP)\n",
        "\n",
        "blackbox_train = pd.DataFrame(columns=NSLKDD_COL_NAMES)\n",
        "discriminator_train_normal = pd.DataFrame(columns=NSLKDD_COL_NAMES)\n",
        "generator_train = pd.DataFrame(columns=NSLKDD_COL_NAMES)\n",
        "\n",
        "GAN_train = pd.DataFrame(columns=NSLKDD_COL_NAMES)\n",
        "\n",
        "generator_test_attack = pd.DataFrame(columns=NSLKDD_COL_NAMES)\n",
        "generator_test_normal = pd.DataFrame(columns=NSLKDD_COL_NAMES)\n",
        "generator_test_combined = pd.DataFrame(columns=NSLKDD_COL_NAMES)\n",
        "\n",
        "train_id_dict = dict()\n",
        "\n",
        "test_id_dict = dict()\n",
        "\n",
        "for i in range(len(train)):\n",
        "    train_id_dict.setdefault(train[\"class\"][i], []).append(i)\n",
        "\n",
        "for i in range(len(test)):\n",
        "    test_id_dict.setdefault(test[\"class\"][i], []).append(i)\n",
        "\n",
        "blackbox_train_ids = []\n",
        "discriminator_train_normal_ids = []\n",
        "generator_train_ids = []\n",
        "\n",
        "GAN_train_ids = []\n",
        "\n",
        "generator_test_attack_ids = []\n",
        "generator_test_normal_ids = []\n",
        "generator_test_combined_ids = []\n",
        "\n",
        "\n",
        "\n",
        "for k, v in train_id_dict.items():\n",
        "    if k == \"Normal\":\n",
        "        np.random.shuffle(v)\n",
        "        blackbox_train_ids = blackbox_train_ids + v[:(len(v) // 2)]\n",
        "        discriminator_train_normal_ids = discriminator_train_normal_ids + v[(len(v) // 2):]\n",
        "    else:\n",
        "        np.random.shuffle(v)\n",
        "        if k in considered_attack_list:\n",
        "            generator_train_ids = generator_train_ids + v[(len(v) // 2):]\n",
        "        blackbox_train_ids = blackbox_train_ids + v[:(len(v) // 2)]\n",
        "\n",
        "for k, v in test_id_dict.items():\n",
        "    if k == \"Normal\":\n",
        "        generator_test_normal_ids = generator_test_normal_ids + v\n",
        "    else:\n",
        "        if k in considered_attack_list:\n",
        "            generator_test_attack_ids = generator_test_attack_ids + v\n",
        "\n",
        "generator_test_combined_ids = generator_test_combined_ids + generator_test_normal_ids\n",
        "generator_test_combined_ids = generator_test_combined_ids + generator_test_attack_ids\n",
        "\n",
        "GAN_train_ids = GAN_train_ids + discriminator_train_normal_ids\n",
        "GAN_train_ids = GAN_train_ids + generator_train_ids\n",
        "\n",
        "blackbox_train = train.iloc[blackbox_train_ids]\n",
        "pd.DataFrame.to_csv(blackbox_train, 'dataset/blackbox_train.csv', index=False)\n",
        "\n",
        "discriminator_train_normal = train.iloc[discriminator_train_normal_ids]\n",
        "pd.DataFrame.to_csv(discriminator_train_normal, 'dataset/discriminator_train_normal.csv', index=False)\n",
        "\n",
        "generator_train = train.iloc[generator_train_ids]\n",
        "pd.DataFrame.to_csv(generator_train, 'dataset/generator_train.csv', index=False)\n",
        "\n",
        "generator_test_attack = test.iloc[generator_test_attack_ids]\n",
        "pd.DataFrame.to_csv(generator_test_attack, 'dataset/generator_test_attack.csv', index=False)\n",
        "\n",
        "generator_test_normal = test.iloc[generator_test_normal_ids]\n",
        "pd.DataFrame.to_csv(generator_test_normal, 'dataset/generator_test_normal.csv', index=False)\n",
        "\n",
        "generator_test_combined = test.iloc[generator_test_combined_ids]\n",
        "pd.DataFrame.to_csv(generator_test_combined, 'dataset/generator_test_combined.csv', index=False)\n",
        "\n",
        "GAN_train = train.iloc[GAN_train_ids]\n",
        "pd.DataFrame.to_csv(GAN_train, 'dataset/GAN_train.csv', index=False)\n",
        "\n",
        "# pd.DataFrame.to_csv(test, 'dataset/KDDTest+.csv', index=False)\n",
        "# pd.DataFrame.to_csv(train, 'dataset/KDDTrain+.csv', index=False)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L431aZFTFQVa"
      },
      "source": [
        "# IDSGAN Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eXrH9uWHAVY5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5389b22-5b55-4a43-9a3c-2691e66e2b93"
      },
      "source": [
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# from blackbox_wrapper import BlackBoxWrapper\n",
        "from idsgan_preprocessor import preprocess4, create_batch2\n",
        "from model.model_class import Generator, Discriminator\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "random.seed(12345)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "functional_categories = ['intrinsic', 'time_based']\n",
        "#functional_categories = ['intrinsic', 'time_based', 'host_based']\n",
        "#functional_categories = ['intrinsic', 'content']\n",
        "\n",
        "\n",
        "data = pd.read_csv(\"dataset/GAN_train.csv\")\n",
        "train_data, raw_attack, normal, true_label, modification_mask \\\n",
        "    = preprocess4(data, functional_categories=functional_categories)\n",
        "\n",
        "test = pd.read_csv(\"dataset/generator_test_combined.csv\")\n",
        "test, test_raw_attack, _, _, modification_mask_test = preprocess4(test, functional_categories)\n",
        "test_raw_attack = np.array(test_raw_attack).astype(float)\n",
        "\n",
        "modification_mask_test = np.tile(modification_mask_test, [len(test_raw_attack), 1])\n",
        "modification_mask_test_t = torch.FloatTensor(modification_mask_test).to(device)\n",
        "\n",
        "BATCH_SIZE = 256  # Batch size\n",
        "GEN_ITERS = 20\n",
        "CRITIC_ITERS = 1  # For WGAN and WGAN-GP, number of critic iters per gen iter\n",
        "LAMBDA = 10  # Gradient penalty lambda hyperparameter\n",
        "MAX_EPOCH = 500  # How many generator iterations to train for\n",
        "NOISE_SIZE = 27\n",
        "generator_input_dim = train_data.shape[1] + NOISE_SIZE\n",
        "generator_output_dim = train_data.shape[1]\n",
        "discriminator_output_dim = 1\n",
        "CLAMP = 0.01\n",
        "\n",
        "# read parameters of IDS\n",
        "\n",
        "modification_mask = np.tile(modification_mask, [BATCH_SIZE, 1])\n",
        "modification_mask_t = torch.FloatTensor(modification_mask).to(device)\n",
        "\n",
        "ids_model = BlackBoxWrapper(generator_output_dim, 1)\n",
        "\n",
        "# read model\n",
        "\n",
        "\n",
        "def test_GAN(learned_g):\n",
        "    # g_param = torch.load('save_model/generator.pth')\n",
        "    # learned_g.load_state_dict(g_param)\n",
        "    noise = np.random.uniform(0., 1., (len(test_raw_attack), NOISE_SIZE))\n",
        "    gen_x = np.concatenate([test_raw_attack, noise], axis=1)\n",
        "\n",
        "    gen_x_t = torch.FloatTensor(gen_x).to(device)\n",
        "\n",
        "    adversarial_attack_modification_l = learned_g(gen_x_t, modification_mask_test_t)\n",
        "    adversarial_attack_l = torch.FloatTensor(test_raw_attack).to(device) + adversarial_attack_modification_l\n",
        "\n",
        "    out = ids_model(adversarial_attack_l)\n",
        "    out = torch.reshape(out, [len(adversarial_attack_l)])\n",
        "    out_np = out.cpu().detach().numpy()\n",
        "\n",
        "    ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "    corr = np.sum(ids_pred_label)\n",
        "\n",
        "    adv_det_rate_learned = corr / len(test_raw_attack)\n",
        "\n",
        "    return adv_det_rate_learned\n",
        "\n",
        "\n",
        "generator = Generator(generator_input_dim, generator_output_dim).to(device)\n",
        "discriminator = Discriminator(generator_output_dim, discriminator_output_dim).to(device)\n",
        "\n",
        "optimizer_G = optim.RMSprop(generator.parameters(), lr=0.0001)\n",
        "optimizer_D = optim.RMSprop(discriminator.parameters(), lr=0.0001)\n",
        "\n",
        "batch_attack = create_batch2(raw_attack, BATCH_SIZE)\n",
        "batch_normal = create_batch2(normal, BATCH_SIZE)\n",
        "\n",
        "d_losses, g_losses = [], []\n",
        "ids_model.eval()\n",
        "generator.train()\n",
        "discriminator.train()\n",
        "cnt = -5\n",
        "print(\"IDSGAN start training\")\n",
        "print(\"-\" * 100)\n",
        "\n",
        "gen_attack_batch_idx = 0\n",
        "dis_normal_batch_idx = 0\n",
        "adv_dr = 1.00\n",
        "\n",
        "for epoch in range(MAX_EPOCH):\n",
        "\n",
        "    #  Train Generator\n",
        "\n",
        "    gen_loss = 0.\n",
        "    for g in range(GEN_ITERS):\n",
        "\n",
        "        for p in generator.parameters():\n",
        "            p.requires_grad = True\n",
        "\n",
        "        for p in discriminator.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "        ba = batch_attack[gen_attack_batch_idx]\n",
        "        gen_attack_batch_idx = (gen_attack_batch_idx + 1) % len(batch_attack)\n",
        "\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        noise = np.random.uniform(0., 1., (BATCH_SIZE, NOISE_SIZE))\n",
        "        gen_x = np.concatenate([ba, noise], axis=1)\n",
        "\n",
        "        gen_x_t = torch.FloatTensor(gen_x).to(device)\n",
        "\n",
        "        adversarial_attack_modification = generator(gen_x_t, modification_mask_t)\n",
        "        adversarial_attack = torch.FloatTensor(ba).to(device) + adversarial_attack_modification\n",
        "\n",
        "        D_pred = discriminator(adversarial_attack)\n",
        "        g_loss = torch.mean(D_pred)\n",
        "        # print(\"G loss\")\n",
        "        # print(g_loss)\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        g_loss.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        for p in generator.parameters():\n",
        "            p = torch.clamp(p, -0.01, 0.01)\n",
        "\n",
        "        gen_loss += g_loss.item()\n",
        "\n",
        "    gen_loss = gen_loss / GEN_ITERS\n",
        "\n",
        "    for p in discriminator.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "    for p in generator.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "    dis_loss = 0.\n",
        "    for c in range(CRITIC_ITERS):\n",
        "        optimizer_D.zero_grad()\n",
        "\n",
        "        np.random.shuffle(batch_normal)\n",
        "\n",
        "        bn = batch_normal[dis_normal_batch_idx]\n",
        "        ba = batch_attack[dis_normal_batch_idx % len(batch_attack)]\n",
        "        dis_normal_batch_idx = (dis_normal_batch_idx + 1) % len(batch_normal)\n",
        "\n",
        "        noise = np.random.uniform(0., 1., (BATCH_SIZE, NOISE_SIZE))\n",
        "        gen_x = np.concatenate([ba, noise], axis=1)\n",
        "\n",
        "        gen_x_t = torch.FloatTensor(gen_x).to(device)\n",
        "\n",
        "        adversarial_attack_modification = generator(gen_x_t, modification_mask_t)\n",
        "        adversarial_attack_t = torch.FloatTensor(ba).to(device) + adversarial_attack_modification\n",
        "        adversarial_attack = adversarial_attack_t.cpu().detach().numpy()\n",
        "\n",
        "        ids_input = np.concatenate([bn, adversarial_attack], axis=0)\n",
        "\n",
        "        l = list(range(len(ids_input)))\n",
        "        np.random.shuffle(l)\n",
        "\n",
        "        ids_input = ids_input[l]\n",
        "\n",
        "        ids_input = torch.FloatTensor(ids_input).to(device)\n",
        "\n",
        "        out = ids_model(ids_input)\n",
        "        out = torch.reshape(out, [len(ids_input)])\n",
        "        out_np = out.cpu().detach().numpy()\n",
        "\n",
        "        ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "\n",
        "        pred_normal = ids_input.cpu().detach().numpy()[ids_pred_label == 0]\n",
        "        pred_attack = ids_input.cpu().detach().numpy()[ids_pred_label == 1]\n",
        "\n",
        "        # print(len(pred_normal))\n",
        "        # print(len(pred_attack))\n",
        "\n",
        "        if len(pred_attack) == 0:\n",
        "            cnt += 1\n",
        "            break\n",
        "\n",
        "        D_normal = discriminator(torch.FloatTensor(pred_normal).to(device))\n",
        "        D_attack = discriminator(torch.FloatTensor(pred_attack).to(device))\n",
        "        D_adv = discriminator(adversarial_attack_t)\n",
        "\n",
        "        loss_normal = torch.mean(D_normal)\n",
        "        loss_attack = torch.mean(D_attack)\n",
        "        loss_adv = torch.mean(D_adv)\n",
        "\n",
        "        # print(loss_normal)\n",
        "        # print(loss_attack)\n",
        "        # print(loss_adv)\n",
        "        # print(\"\\n\")\n",
        "\n",
        "        d_loss = (loss_normal - loss_attack)  # + LAMBDA * gradient_penalty\n",
        "        dis_loss += d_loss.item()\n",
        "\n",
        "        d_loss.backward()\n",
        "        optimizer_D.step()\n",
        "\n",
        "        for p in discriminator.parameters():\n",
        "            p = torch.clamp(p, -0.01, 0.01)\n",
        "\n",
        "    dis_loss = dis_loss / CRITIC_ITERS\n",
        "\n",
        "    d_losses.append(dis_loss)\n",
        "    g_losses.append(gen_loss)\n",
        "\n",
        "    adv_dr_curr = test_GAN(generator)\n",
        "    if adv_dr_curr < adv_dr or epoch == 0:\n",
        "        adv_dr = adv_dr_curr\n",
        "        torch.save(generator.state_dict(), 'save_model/generator.pth')\n",
        "        torch.save(discriminator.state_dict(), 'save_model/discreminator.pth')\n",
        "\n",
        "    print(\"Epoch: {} -- Curr: {}, Min {}\".format(epoch, adv_dr_curr, adv_dr))\n",
        "\n",
        "    if cnt >= 100:\n",
        "        print(\"Not exist predicted attack traffic\")\n",
        "        break\n",
        "\n",
        "print(\"IDSGAN finish training\")\n",
        "plt.plot(d_losses, label=\"D_loss\")\n",
        "plt.plot(g_losses, label=\"G_loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IDSGAN start training\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Epoch: 0 -- Curr: 0.8141592920353983, Min 0.8141592920353983\n",
            "Epoch: 1 -- Curr: 0.8171091445427728, Min 0.8141592920353983\n",
            "Epoch: 2 -- Curr: 0.8234111021721641, Min 0.8141592920353983\n",
            "Epoch: 3 -- Curr: 0.8805309734513275, Min 0.8141592920353983\n",
            "Epoch: 4 -- Curr: 0.8944757307589166, Min 0.8141592920353983\n",
            "Epoch: 5 -- Curr: 0.8791901314025208, Min 0.8141592920353983\n",
            "Epoch: 6 -- Curr: 0.8168409761330115, Min 0.8141592920353983\n",
            "Epoch: 7 -- Curr: 0.7889514615178332, Min 0.7889514615178332\n",
            "Epoch: 8 -- Curr: 0.7902923035666398, Min 0.7889514615178332\n",
            "Epoch: 9 -- Curr: 0.8272995441137034, Min 0.7889514615178332\n",
            "Epoch: 10 -- Curr: 0.881871815500134, Min 0.7889514615178332\n",
            "Epoch: 11 -- Curr: 0.9026548672566371, Min 0.7889514615178332\n",
            "Epoch: 12 -- Curr: 0.8731563421828908, Min 0.7889514615178332\n",
            "Epoch: 13 -- Curr: 0.644945025475999, Min 0.644945025475999\n",
            "Epoch: 14 -- Curr: 0.2998122821131671, Min 0.2998122821131671\n",
            "Epoch: 15 -- Curr: 0.28640386162510056, Min 0.28640386162510056\n",
            "Epoch: 16 -- Curr: 0.3668543845534996, Min 0.28640386162510056\n",
            "Epoch: 17 -- Curr: 0.3990345937248592, Min 0.28640386162510056\n",
            "Epoch: 18 -- Curr: 0.6064628586752481, Min 0.28640386162510056\n",
            "Epoch: 19 -- Curr: 0.7811745776347546, Min 0.28640386162510056\n",
            "Epoch: 20 -- Curr: 0.7666934835076428, Min 0.28640386162510056\n",
            "Epoch: 21 -- Curr: 0.7125234647358542, Min 0.28640386162510056\n",
            "Epoch: 22 -- Curr: 0.799812282113167, Min 0.28640386162510056\n",
            "Epoch: 23 -- Curr: 0.7892196299275945, Min 0.28640386162510056\n",
            "Epoch: 24 -- Curr: 0.8140252078305176, Min 0.28640386162510056\n",
            "Epoch: 25 -- Curr: 0.8164387235183695, Min 0.28640386162510056\n",
            "Epoch: 26 -- Curr: 0.7465808527755431, Min 0.28640386162510056\n",
            "Epoch: 27 -- Curr: 0.6779297398766425, Min 0.28640386162510056\n",
            "Epoch: 28 -- Curr: 0.5831322070260123, Min 0.28640386162510056\n",
            "Epoch: 29 -- Curr: 0.30799141861088764, Min 0.28640386162510056\n",
            "Epoch: 30 -- Curr: 0.24859211584875301, Min 0.24859211584875301\n",
            "Epoch: 31 -- Curr: 0.29793510324483774, Min 0.24859211584875301\n",
            "Epoch: 32 -- Curr: 0.6654599088227406, Min 0.24859211584875301\n",
            "Epoch: 33 -- Curr: 0.82461786001609, Min 0.24859211584875301\n",
            "Epoch: 34 -- Curr: 0.7835880933226066, Min 0.24859211584875301\n",
            "Epoch: 35 -- Curr: 0.45253419147224455, Min 0.24859211584875301\n",
            "Epoch: 36 -- Curr: 0.27956556717618664, Min 0.24859211584875301\n",
            "Epoch: 37 -- Curr: 0.2413515687851971, Min 0.2413515687851971\n",
            "Epoch: 38 -- Curr: 0.23719495843389649, Min 0.23719495843389649\n",
            "Epoch: 39 -- Curr: 0.2283454009117726, Min 0.2283454009117726\n",
            "Epoch: 40 -- Curr: 0.221373022257978, Min 0.221373022257978\n",
            "Epoch: 41 -- Curr: 0.22593188522392063, Min 0.221373022257978\n",
            "Epoch: 42 -- Curr: 0.29592384017162776, Min 0.221373022257978\n",
            "Epoch: 43 -- Curr: 0.45682488602842586, Min 0.221373022257978\n",
            "Epoch: 44 -- Curr: 0.442343791901314, Min 0.221373022257978\n",
            "Epoch: 45 -- Curr: 0.2571735049611156, Min 0.221373022257978\n",
            "Epoch: 46 -- Curr: 0.22807723250201126, Min 0.221373022257978\n",
            "Epoch: 47 -- Curr: 0.2390721373022258, Min 0.221373022257978\n",
            "Epoch: 48 -- Curr: 0.22901582193617592, Min 0.221373022257978\n",
            "Epoch: 49 -- Curr: 0.251407884151247, Min 0.221373022257978\n",
            "Epoch: 50 -- Curr: 0.24859211584875301, Min 0.221373022257978\n",
            "Epoch: 51 -- Curr: 0.2279431482971306, Min 0.221373022257978\n",
            "Epoch: 52 -- Curr: 0.21775274872620004, Min 0.21775274872620004\n",
            "Epoch: 53 -- Curr: 0.9249128452668276, Min 0.21775274872620004\n",
            "Epoch: 54 -- Curr: 0.9556181281844999, Min 0.21775274872620004\n",
            "Epoch: 55 -- Curr: 0.9458299812282113, Min 0.21775274872620004\n",
            "Epoch: 56 -- Curr: 0.9120407615982837, Min 0.21775274872620004\n",
            "Epoch: 57 -- Curr: 0.8995709305443819, Min 0.21775274872620004\n",
            "Epoch: 58 -- Curr: 0.8952802359882006, Min 0.21775274872620004\n",
            "Epoch: 59 -- Curr: 0.8939393939393939, Min 0.21775274872620004\n",
            "Epoch: 60 -- Curr: 0.9060069723786538, Min 0.21775274872620004\n",
            "Epoch: 61 -- Curr: 0.9035934566908018, Min 0.21775274872620004\n",
            "Epoch: 62 -- Curr: 0.8305175650308394, Min 0.21775274872620004\n",
            "Epoch: 63 -- Curr: 0.7684365781710915, Min 0.21775274872620004\n",
            "Epoch: 64 -- Curr: 0.7679002413515688, Min 0.21775274872620004\n",
            "Epoch: 65 -- Curr: 0.7683024939662108, Min 0.21775274872620004\n",
            "Epoch: 66 -- Curr: 0.7716545990882274, Min 0.21775274872620004\n",
            "Epoch: 67 -- Curr: 0.7719227674979887, Min 0.21775274872620004\n",
            "Epoch: 68 -- Curr: 0.7720568517028694, Min 0.21775274872620004\n",
            "Epoch: 69 -- Curr: 0.8277017967283454, Min 0.21775274872620004\n",
            "Epoch: 70 -- Curr: 0.9203539823008849, Min 0.21775274872620004\n",
            "Epoch: 71 -- Curr: 0.9125770984178064, Min 0.21775274872620004\n",
            "Epoch: 72 -- Curr: 0.9129793510324484, Min 0.21775274872620004\n",
            "Epoch: 73 -- Curr: 0.9068114776079378, Min 0.21775274872620004\n",
            "Epoch: 74 -- Curr: 0.8950120675784392, Min 0.21775274872620004\n",
            "Epoch: 75 -- Curr: 0.890319120407616, Min 0.21775274872620004\n",
            "Epoch: 76 -- Curr: 0.8927326360954679, Min 0.21775274872620004\n",
            "Epoch: 77 -- Curr: 0.8928667203003486, Min 0.21775274872620004\n",
            "Epoch: 78 -- Curr: 0.9009117725931886, Min 0.21775274872620004\n",
            "Epoch: 79 -- Curr: 0.7904263877715205, Min 0.21775274872620004\n",
            "Epoch: 80 -- Curr: 0.7873424510592653, Min 0.21775274872620004\n",
            "Epoch: 81 -- Curr: 0.5301689460981497, Min 0.21775274872620004\n",
            "Epoch: 82 -- Curr: 0.2382676320729418, Min 0.21775274872620004\n",
            "Epoch: 83 -- Curr: 0.24108340037543577, Min 0.21775274872620004\n",
            "Epoch: 84 -- Curr: 0.24121748458031644, Min 0.21775274872620004\n",
            "Epoch: 85 -- Curr: 0.24108340037543577, Min 0.21775274872620004\n",
            "Epoch: 86 -- Curr: 0.24148565299007776, Min 0.21775274872620004\n",
            "Epoch: 87 -- Curr: 0.24094931617055512, Min 0.21775274872620004\n",
            "Epoch: 88 -- Curr: 0.24148565299007776, Min 0.21775274872620004\n",
            "Epoch: 89 -- Curr: 0.24202198980960044, Min 0.21775274872620004\n",
            "Epoch: 90 -- Curr: 0.24081523196567445, Min 0.21775274872620004\n",
            "Epoch: 91 -- Curr: 0.24068114776079377, Min 0.21775274872620004\n",
            "Epoch: 92 -- Curr: 0.305846071332797, Min 0.21775274872620004\n",
            "Epoch: 93 -- Curr: 0.7869401984446233, Min 0.21775274872620004\n",
            "Epoch: 94 -- Curr: 0.8079914186108876, Min 0.21775274872620004\n",
            "Epoch: 95 -- Curr: 0.8191204076159828, Min 0.21775274872620004\n",
            "Epoch: 96 -- Curr: 0.8183159023866988, Min 0.21775274872620004\n",
            "Epoch: 97 -- Curr: 0.4631268436578171, Min 0.21775274872620004\n",
            "Epoch: 98 -- Curr: 0.34566908018235454, Min 0.21775274872620004\n",
            "Epoch: 99 -- Curr: 0.24617860016090104, Min 0.21775274872620004\n",
            "Epoch: 100 -- Curr: 0.24121748458031644, Min 0.21775274872620004\n",
            "Epoch: 101 -- Curr: 0.24215607401448108, Min 0.21775274872620004\n",
            "Epoch: 102 -- Curr: 0.24121748458031644, Min 0.21775274872620004\n",
            "Epoch: 103 -- Curr: 0.24202198980960044, Min 0.21775274872620004\n",
            "Epoch: 104 -- Curr: 0.24229015821936176, Min 0.21775274872620004\n",
            "Epoch: 105 -- Curr: 0.24094931617055512, Min 0.21775274872620004\n",
            "Epoch: 106 -- Curr: 0.24161973719495844, Min 0.21775274872620004\n",
            "Epoch: 107 -- Curr: 0.2475194422097077, Min 0.21775274872620004\n",
            "Epoch: 108 -- Curr: 0.805846071332797, Min 0.21775274872620004\n",
            "Epoch: 109 -- Curr: 0.8069187449718423, Min 0.21775274872620004\n",
            "Epoch: 110 -- Curr: 0.8069187449718423, Min 0.21775274872620004\n",
            "Epoch: 111 -- Curr: 0.8057119871279164, Min 0.21775274872620004\n",
            "Epoch: 112 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 113 -- Curr: 0.8192544918208635, Min 0.21775274872620004\n",
            "Epoch: 114 -- Curr: 0.8201930812550282, Min 0.21775274872620004\n",
            "Epoch: 115 -- Curr: 0.8199249128452668, Min 0.21775274872620004\n",
            "Epoch: 116 -- Curr: 0.8200589970501475, Min 0.21775274872620004\n",
            "Epoch: 117 -- Curr: 0.8199249128452668, Min 0.21775274872620004\n",
            "Epoch: 118 -- Curr: 0.8197908286403862, Min 0.21775274872620004\n",
            "Epoch: 119 -- Curr: 0.8051756503083937, Min 0.21775274872620004\n",
            "Epoch: 120 -- Curr: 0.7963260927862698, Min 0.21775274872620004\n",
            "Epoch: 121 -- Curr: 0.7815768302493966, Min 0.21775274872620004\n",
            "Epoch: 122 -- Curr: 0.8230088495575221, Min 0.21775274872620004\n",
            "Epoch: 123 -- Curr: 0.8232770179672835, Min 0.21775274872620004\n",
            "Epoch: 124 -- Curr: 0.8235451863770448, Min 0.21775274872620004\n",
            "Epoch: 125 -- Curr: 0.8235451863770448, Min 0.21775274872620004\n",
            "Epoch: 126 -- Curr: 0.8235451863770448, Min 0.21775274872620004\n",
            "Epoch: 127 -- Curr: 0.8235451863770448, Min 0.21775274872620004\n",
            "Epoch: 128 -- Curr: 0.8235451863770448, Min 0.21775274872620004\n",
            "Epoch: 129 -- Curr: 0.8235451863770448, Min 0.21775274872620004\n",
            "Epoch: 130 -- Curr: 0.8235451863770448, Min 0.21775274872620004\n",
            "Epoch: 131 -- Curr: 0.8226065969428801, Min 0.21775274872620004\n",
            "Epoch: 132 -- Curr: 0.7418879056047197, Min 0.21775274872620004\n",
            "Epoch: 133 -- Curr: 0.7173504961115581, Min 0.21775274872620004\n",
            "Epoch: 134 -- Curr: 0.7157414856529901, Min 0.21775274872620004\n",
            "Epoch: 135 -- Curr: 0.7149369804237061, Min 0.21775274872620004\n",
            "Epoch: 136 -- Curr: 0.7150710646285867, Min 0.21775274872620004\n",
            "Epoch: 137 -- Curr: 0.7152051488334674, Min 0.21775274872620004\n",
            "Epoch: 138 -- Curr: 0.7148028962188254, Min 0.21775274872620004\n",
            "Epoch: 139 -- Curr: 0.7149369804237061, Min 0.21775274872620004\n",
            "Epoch: 140 -- Curr: 0.7148028962188254, Min 0.21775274872620004\n",
            "Epoch: 141 -- Curr: 0.7262000536336819, Min 0.21775274872620004\n",
            "Epoch: 142 -- Curr: 0.7723250201126307, Min 0.21775274872620004\n",
            "Epoch: 143 -- Curr: 0.7736658621614374, Min 0.21775274872620004\n",
            "Epoch: 144 -- Curr: 0.8263609546795387, Min 0.21775274872620004\n",
            "Epoch: 145 -- Curr: 0.9018503620273531, Min 0.21775274872620004\n",
            "Epoch: 146 -- Curr: 0.9097613301153125, Min 0.21775274872620004\n",
            "Epoch: 147 -- Curr: 0.915929203539823, Min 0.21775274872620004\n",
            "Epoch: 148 -- Curr: 0.9298739608474121, Min 0.21775274872620004\n",
            "Epoch: 149 -- Curr: 0.9297398766425315, Min 0.21775274872620004\n",
            "Epoch: 150 -- Curr: 0.9235720032180209, Min 0.21775274872620004\n",
            "Epoch: 151 -- Curr: 0.8848216680075087, Min 0.21775274872620004\n",
            "Epoch: 152 -- Curr: 0.7818449986591579, Min 0.21775274872620004\n",
            "Epoch: 153 -- Curr: 0.7817109144542773, Min 0.21775274872620004\n",
            "Epoch: 154 -- Curr: 0.7815768302493966, Min 0.21775274872620004\n",
            "Epoch: 155 -- Curr: 0.7881469562885492, Min 0.21775274872620004\n",
            "Epoch: 156 -- Curr: 0.7914990614105658, Min 0.21775274872620004\n",
            "Epoch: 157 -- Curr: 0.7940466613032985, Min 0.21775274872620004\n",
            "Epoch: 158 -- Curr: 0.8203271654599088, Min 0.21775274872620004\n",
            "Epoch: 159 -- Curr: 0.47438991686779297, Min 0.21775274872620004\n",
            "Epoch: 160 -- Curr: 0.47519442209707696, Min 0.21775274872620004\n",
            "Epoch: 161 -- Curr: 0.674979887369268, Min 0.21775274872620004\n",
            "Epoch: 162 -- Curr: 0.8224725127379995, Min 0.21775274872620004\n",
            "Epoch: 163 -- Curr: 0.8091981764548136, Min 0.21775274872620004\n",
            "Epoch: 164 -- Curr: 0.8101367658889783, Min 0.21775274872620004\n",
            "Epoch: 165 -- Curr: 0.8102708500938589, Min 0.21775274872620004\n",
            "Epoch: 166 -- Curr: 0.8101367658889783, Min 0.21775274872620004\n",
            "Epoch: 167 -- Curr: 0.8094663448645749, Min 0.21775274872620004\n",
            "Epoch: 168 -- Curr: 0.8085277554304103, Min 0.21775274872620004\n",
            "Epoch: 169 -- Curr: 0.8085277554304103, Min 0.21775274872620004\n",
            "Epoch: 170 -- Curr: 0.8081255028157683, Min 0.21775274872620004\n",
            "Epoch: 171 -- Curr: 0.8081255028157683, Min 0.21775274872620004\n",
            "Epoch: 172 -- Curr: 0.8085277554304103, Min 0.21775274872620004\n",
            "Epoch: 173 -- Curr: 0.8101367658889783, Min 0.21775274872620004\n",
            "Epoch: 174 -- Curr: 0.8101367658889783, Min 0.21775274872620004\n",
            "Epoch: 175 -- Curr: 0.8104049342987396, Min 0.21775274872620004\n",
            "Epoch: 176 -- Curr: 0.8101367658889783, Min 0.21775274872620004\n",
            "Epoch: 177 -- Curr: 0.8087959238401716, Min 0.21775274872620004\n",
            "Epoch: 178 -- Curr: 0.48028962188254226, Min 0.21775274872620004\n",
            "Epoch: 179 -- Curr: 0.47760793778492894, Min 0.21775274872620004\n",
            "Epoch: 180 -- Curr: 0.47801019039957093, Min 0.21775274872620004\n",
            "Epoch: 181 -- Curr: 0.3061142397425583, Min 0.21775274872620004\n",
            "Epoch: 182 -- Curr: 0.2912308930008045, Min 0.21775274872620004\n",
            "Epoch: 183 -- Curr: 0.3437919013140252, Min 0.21775274872620004\n",
            "Epoch: 184 -- Curr: 0.39165996245642265, Min 0.21775274872620004\n",
            "Epoch: 185 -- Curr: 0.31187986055242695, Min 0.21775274872620004\n",
            "Epoch: 186 -- Curr: 0.33480825958702065, Min 0.21775274872620004\n",
            "Epoch: 187 -- Curr: 0.4528023598820059, Min 0.21775274872620004\n",
            "Epoch: 188 -- Curr: 0.45333869670152854, Min 0.21775274872620004\n",
            "Epoch: 189 -- Curr: 0.4540091177259319, Min 0.21775274872620004\n",
            "Epoch: 190 -- Curr: 0.45374094931617054, Min 0.21775274872620004\n",
            "Epoch: 191 -- Curr: 0.45347278090640925, Min 0.21775274872620004\n",
            "Epoch: 192 -- Curr: 0.45374094931617054, Min 0.21775274872620004\n",
            "Epoch: 193 -- Curr: 0.45374094931617054, Min 0.21775274872620004\n",
            "Epoch: 194 -- Curr: 0.45387503352105124, Min 0.21775274872620004\n",
            "Epoch: 195 -- Curr: 0.44140520246714937, Min 0.21775274872620004\n",
            "Epoch: 196 -- Curr: 0.3149637972646822, Min 0.21775274872620004\n",
            "Epoch: 197 -- Curr: 0.31482971305980156, Min 0.21775274872620004\n",
            "Epoch: 198 -- Curr: 0.3943416465540359, Min 0.21775274872620004\n",
            "Epoch: 199 -- Curr: 0.31603647090372755, Min 0.21775274872620004\n",
            "Epoch: 200 -- Curr: 0.31603647090372755, Min 0.21775274872620004\n",
            "Epoch: 201 -- Curr: 0.3157683024939662, Min 0.21775274872620004\n",
            "Epoch: 202 -- Curr: 0.31603647090372755, Min 0.21775274872620004\n",
            "Epoch: 203 -- Curr: 0.31603647090372755, Min 0.21775274872620004\n",
            "Epoch: 204 -- Curr: 0.31603647090372755, Min 0.21775274872620004\n",
            "Epoch: 205 -- Curr: 0.31590238669884685, Min 0.21775274872620004\n",
            "Epoch: 206 -- Curr: 0.3157683024939662, Min 0.21775274872620004\n",
            "Epoch: 207 -- Curr: 0.31590238669884685, Min 0.21775274872620004\n",
            "Epoch: 208 -- Curr: 0.31590238669884685, Min 0.21775274872620004\n",
            "Epoch: 209 -- Curr: 0.31590238669884685, Min 0.21775274872620004\n",
            "Epoch: 210 -- Curr: 0.31590238669884685, Min 0.21775274872620004\n",
            "Epoch: 211 -- Curr: 0.31590238669884685, Min 0.21775274872620004\n",
            "Epoch: 212 -- Curr: 0.31590238669884685, Min 0.21775274872620004\n",
            "Epoch: 213 -- Curr: 0.31590238669884685, Min 0.21775274872620004\n",
            "Epoch: 214 -- Curr: 0.31590238669884685, Min 0.21775274872620004\n",
            "Epoch: 215 -- Curr: 0.31603647090372755, Min 0.21775274872620004\n",
            "Epoch: 216 -- Curr: 0.8112094395280236, Min 0.21775274872620004\n",
            "Epoch: 217 -- Curr: 0.8110753553231429, Min 0.21775274872620004\n",
            "Epoch: 218 -- Curr: 0.8113435237329043, Min 0.21775274872620004\n",
            "Epoch: 219 -- Curr: 0.8112094395280236, Min 0.21775274872620004\n",
            "Epoch: 220 -- Curr: 0.8110753553231429, Min 0.21775274872620004\n",
            "Epoch: 221 -- Curr: 0.8012872083668544, Min 0.21775274872620004\n",
            "Epoch: 222 -- Curr: 0.7923035666398498, Min 0.21775274872620004\n",
            "Epoch: 223 -- Curr: 0.7925717350496112, Min 0.21775274872620004\n",
            "Epoch: 224 -- Curr: 0.7579780101903996, Min 0.21775274872620004\n",
            "Epoch: 225 -- Curr: 0.7139983909895414, Min 0.21775274872620004\n",
            "Epoch: 226 -- Curr: 0.7170823277017967, Min 0.21775274872620004\n",
            "Epoch: 227 -- Curr: 0.7182890855457227, Min 0.21775274872620004\n",
            "Epoch: 228 -- Curr: 0.7180209171359614, Min 0.21775274872620004\n",
            "Epoch: 229 -- Curr: 0.7193617591847681, Min 0.21775274872620004\n",
            "Epoch: 230 -- Curr: 0.7201662644140521, Min 0.21775274872620004\n",
            "Epoch: 231 -- Curr: 0.7208366854384554, Min 0.21775274872620004\n",
            "Epoch: 232 -- Curr: 0.8053097345132744, Min 0.21775274872620004\n",
            "Epoch: 233 -- Curr: 0.3299812282113167, Min 0.21775274872620004\n",
            "Epoch: 234 -- Curr: 0.32434969160632876, Min 0.21775274872620004\n",
            "Epoch: 235 -- Curr: 0.2578439259855189, Min 0.21775274872620004\n",
            "Epoch: 236 -- Curr: 0.24658085277554304, Min 0.21775274872620004\n",
            "Epoch: 237 -- Curr: 0.2479216948243497, Min 0.21775274872620004\n",
            "Epoch: 238 -- Curr: 0.24684902118530438, Min 0.21775274872620004\n",
            "Epoch: 239 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 240 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 241 -- Curr: 0.24658085277554304, Min 0.21775274872620004\n",
            "Epoch: 242 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 243 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 244 -- Curr: 0.24617860016090104, Min 0.21775274872620004\n",
            "Epoch: 245 -- Curr: 0.2471171895950657, Min 0.21775274872620004\n",
            "Epoch: 246 -- Curr: 0.24725127379994635, Min 0.21775274872620004\n",
            "Epoch: 247 -- Curr: 0.24617860016090104, Min 0.21775274872620004\n",
            "Epoch: 248 -- Curr: 0.24698310539018503, Min 0.21775274872620004\n",
            "Epoch: 249 -- Curr: 0.24591043175113972, Min 0.21775274872620004\n",
            "Epoch: 250 -- Curr: 0.24577634754625904, Min 0.21775274872620004\n",
            "Epoch: 251 -- Curr: 0.24684902118530438, Min 0.21775274872620004\n",
            "Epoch: 252 -- Curr: 0.24765352641458835, Min 0.21775274872620004\n",
            "Epoch: 253 -- Curr: 0.24765352641458835, Min 0.21775274872620004\n",
            "Epoch: 254 -- Curr: 0.24631268436578171, Min 0.21775274872620004\n",
            "Epoch: 255 -- Curr: 0.24765352641458835, Min 0.21775274872620004\n",
            "Epoch: 256 -- Curr: 0.24510592652185573, Min 0.21775274872620004\n",
            "Epoch: 257 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 258 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 259 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 260 -- Curr: 0.24617860016090104, Min 0.21775274872620004\n",
            "Epoch: 261 -- Curr: 0.2471171895950657, Min 0.21775274872620004\n",
            "Epoch: 262 -- Curr: 0.24658085277554304, Min 0.21775274872620004\n",
            "Epoch: 263 -- Curr: 0.24698310539018503, Min 0.21775274872620004\n",
            "Epoch: 264 -- Curr: 0.24725127379994635, Min 0.21775274872620004\n",
            "Epoch: 265 -- Curr: 0.2475194422097077, Min 0.21775274872620004\n",
            "Epoch: 266 -- Curr: 0.24765352641458835, Min 0.21775274872620004\n",
            "Epoch: 267 -- Curr: 0.24510592652185573, Min 0.21775274872620004\n",
            "Epoch: 268 -- Curr: 0.2475194422097077, Min 0.21775274872620004\n",
            "Epoch: 269 -- Curr: 0.2471171895950657, Min 0.21775274872620004\n",
            "Epoch: 270 -- Curr: 0.2456422633413784, Min 0.21775274872620004\n",
            "Epoch: 271 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 272 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 273 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 274 -- Curr: 0.2475194422097077, Min 0.21775274872620004\n",
            "Epoch: 275 -- Curr: 0.2464467685706624, Min 0.21775274872620004\n",
            "Epoch: 276 -- Curr: 0.2464467685706624, Min 0.21775274872620004\n",
            "Epoch: 277 -- Curr: 0.24765352641458835, Min 0.21775274872620004\n",
            "Epoch: 278 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 279 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 280 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 281 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 282 -- Curr: 0.2471171895950657, Min 0.21775274872620004\n",
            "Epoch: 283 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 284 -- Curr: 0.24926253687315633, Min 0.21775274872620004\n",
            "Epoch: 285 -- Curr: 0.24684902118530438, Min 0.21775274872620004\n",
            "Epoch: 286 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 287 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 288 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 289 -- Curr: 0.24684902118530438, Min 0.21775274872620004\n",
            "Epoch: 290 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 291 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 292 -- Curr: 0.24765352641458835, Min 0.21775274872620004\n",
            "Epoch: 293 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 294 -- Curr: 0.2471171895950657, Min 0.21775274872620004\n",
            "Epoch: 295 -- Curr: 0.2464467685706624, Min 0.21775274872620004\n",
            "Epoch: 296 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 297 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 298 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 299 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 300 -- Curr: 0.24765352641458835, Min 0.21775274872620004\n",
            "Epoch: 301 -- Curr: 0.2479216948243497, Min 0.21775274872620004\n",
            "Epoch: 302 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 303 -- Curr: 0.24912845266827569, Min 0.21775274872620004\n",
            "Epoch: 304 -- Curr: 0.24886028425851434, Min 0.21775274872620004\n",
            "Epoch: 305 -- Curr: 0.24886028425851434, Min 0.21775274872620004\n",
            "Epoch: 306 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 307 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 308 -- Curr: 0.24617860016090104, Min 0.21775274872620004\n",
            "Epoch: 309 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 310 -- Curr: 0.24725127379994635, Min 0.21775274872620004\n",
            "Epoch: 311 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 312 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 313 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 314 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 315 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 316 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 317 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 318 -- Curr: 0.2479216948243497, Min 0.21775274872620004\n",
            "Epoch: 319 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 320 -- Curr: 0.2479216948243497, Min 0.21775274872620004\n",
            "Epoch: 321 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 322 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 323 -- Curr: 0.25033521051220164, Min 0.21775274872620004\n",
            "Epoch: 324 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 325 -- Curr: 0.24926253687315633, Min 0.21775274872620004\n",
            "Epoch: 326 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 327 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 328 -- Curr: 0.24725127379994635, Min 0.21775274872620004\n",
            "Epoch: 329 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 330 -- Curr: 0.24886028425851434, Min 0.21775274872620004\n",
            "Epoch: 331 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 332 -- Curr: 0.24926253687315633, Min 0.21775274872620004\n",
            "Epoch: 333 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 334 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 335 -- Curr: 0.2475194422097077, Min 0.21775274872620004\n",
            "Epoch: 336 -- Curr: 0.2479216948243497, Min 0.21775274872620004\n",
            "Epoch: 337 -- Curr: 0.24886028425851434, Min 0.21775274872620004\n",
            "Epoch: 338 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 339 -- Curr: 0.249798873692679, Min 0.21775274872620004\n",
            "Epoch: 340 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 341 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 342 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 343 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 344 -- Curr: 0.24912845266827569, Min 0.21775274872620004\n",
            "Epoch: 345 -- Curr: 0.24886028425851434, Min 0.21775274872620004\n",
            "Epoch: 346 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 347 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 348 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 349 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 350 -- Curr: 0.24725127379994635, Min 0.21775274872620004\n",
            "Epoch: 351 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 352 -- Curr: 0.24966478948779833, Min 0.21775274872620004\n",
            "Epoch: 353 -- Curr: 0.24912845266827569, Min 0.21775274872620004\n",
            "Epoch: 354 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 355 -- Curr: 0.2479216948243497, Min 0.21775274872620004\n",
            "Epoch: 356 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 357 -- Curr: 0.24953070528291768, Min 0.21775274872620004\n",
            "Epoch: 358 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 359 -- Curr: 0.24886028425851434, Min 0.21775274872620004\n",
            "Epoch: 360 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 361 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 362 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 363 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 364 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 365 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 366 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 367 -- Curr: 0.24725127379994635, Min 0.21775274872620004\n",
            "Epoch: 368 -- Curr: 0.24953070528291768, Min 0.21775274872620004\n",
            "Epoch: 369 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 370 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 371 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 372 -- Curr: 0.24953070528291768, Min 0.21775274872620004\n",
            "Epoch: 373 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 374 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 375 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 376 -- Curr: 0.2471171895950657, Min 0.21775274872620004\n",
            "Epoch: 377 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 378 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 379 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 380 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 381 -- Curr: 0.24926253687315633, Min 0.21775274872620004\n",
            "Epoch: 382 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 383 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 384 -- Curr: 0.2479216948243497, Min 0.21775274872620004\n",
            "Epoch: 385 -- Curr: 0.24966478948779833, Min 0.21775274872620004\n",
            "Epoch: 386 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 387 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 388 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 389 -- Curr: 0.249798873692679, Min 0.21775274872620004\n",
            "Epoch: 390 -- Curr: 0.2479216948243497, Min 0.21775274872620004\n",
            "Epoch: 391 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 392 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 393 -- Curr: 0.25006704210244035, Min 0.21775274872620004\n",
            "Epoch: 394 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 395 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 396 -- Curr: 0.24886028425851434, Min 0.21775274872620004\n",
            "Epoch: 397 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 398 -- Curr: 0.249798873692679, Min 0.21775274872620004\n",
            "Epoch: 399 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 400 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 401 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 402 -- Curr: 0.24966478948779833, Min 0.21775274872620004\n",
            "Epoch: 403 -- Curr: 0.24765352641458835, Min 0.21775274872620004\n",
            "Epoch: 404 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 405 -- Curr: 0.25033521051220164, Min 0.21775274872620004\n",
            "Epoch: 406 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 407 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 408 -- Curr: 0.24953070528291768, Min 0.21775274872620004\n",
            "Epoch: 409 -- Curr: 0.2479216948243497, Min 0.21775274872620004\n",
            "Epoch: 410 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 411 -- Curr: 0.25006704210244035, Min 0.21775274872620004\n",
            "Epoch: 412 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 413 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 414 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 415 -- Curr: 0.24912845266827569, Min 0.21775274872620004\n",
            "Epoch: 416 -- Curr: 0.24926253687315633, Min 0.21775274872620004\n",
            "Epoch: 417 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 418 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 419 -- Curr: 0.24738535800482703, Min 0.21775274872620004\n",
            "Epoch: 420 -- Curr: 0.24765352641458835, Min 0.21775274872620004\n",
            "Epoch: 421 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 422 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 423 -- Curr: 0.24953070528291768, Min 0.21775274872620004\n",
            "Epoch: 424 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 425 -- Curr: 0.249798873692679, Min 0.21775274872620004\n",
            "Epoch: 426 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 427 -- Curr: 0.24738535800482703, Min 0.21775274872620004\n",
            "Epoch: 428 -- Curr: 0.24886028425851434, Min 0.21775274872620004\n",
            "Epoch: 429 -- Curr: 0.24765352641458835, Min 0.21775274872620004\n",
            "Epoch: 430 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 431 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 432 -- Curr: 0.24953070528291768, Min 0.21775274872620004\n",
            "Epoch: 433 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 434 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 435 -- Curr: 0.24886028425851434, Min 0.21775274872620004\n",
            "Epoch: 436 -- Curr: 0.24993295789755968, Min 0.21775274872620004\n",
            "Epoch: 437 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 438 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 439 -- Curr: 0.24953070528291768, Min 0.21775274872620004\n",
            "Epoch: 440 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 441 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 442 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 443 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 444 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 445 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 446 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 447 -- Curr: 0.24966478948779833, Min 0.21775274872620004\n",
            "Epoch: 448 -- Curr: 0.24738535800482703, Min 0.21775274872620004\n",
            "Epoch: 449 -- Curr: 0.24886028425851434, Min 0.21775274872620004\n",
            "Epoch: 450 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 451 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 452 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 453 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 454 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 455 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 456 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 457 -- Curr: 0.24966478948779833, Min 0.21775274872620004\n",
            "Epoch: 458 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 459 -- Curr: 0.24926253687315633, Min 0.21775274872620004\n",
            "Epoch: 460 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 461 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 462 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "Epoch: 463 -- Curr: 0.24778761061946902, Min 0.21775274872620004\n",
            "Epoch: 464 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 465 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 466 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 467 -- Curr: 0.2475194422097077, Min 0.21775274872620004\n",
            "Epoch: 468 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 469 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 470 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 471 -- Curr: 0.24953070528291768, Min 0.21775274872620004\n",
            "Epoch: 472 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 473 -- Curr: 0.24912845266827569, Min 0.21775274872620004\n",
            "Epoch: 474 -- Curr: 0.24912845266827569, Min 0.21775274872620004\n",
            "Epoch: 475 -- Curr: 0.24805577902923034, Min 0.21775274872620004\n",
            "Epoch: 476 -- Curr: 0.2483239474389917, Min 0.21775274872620004\n",
            "Epoch: 477 -- Curr: 0.2487262000536337, Min 0.21775274872620004\n",
            "Epoch: 478 -- Curr: 0.24966478948779833, Min 0.21775274872620004\n",
            "Epoch: 479 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 480 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 481 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 482 -- Curr: 0.2479216948243497, Min 0.21775274872620004\n",
            "Epoch: 483 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 484 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 485 -- Curr: 0.2479216948243497, Min 0.21775274872620004\n",
            "Epoch: 486 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 487 -- Curr: 0.24926253687315633, Min 0.21775274872620004\n",
            "Epoch: 488 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 489 -- Curr: 0.24859211584875301, Min 0.21775274872620004\n",
            "Epoch: 490 -- Curr: 0.24765352641458835, Min 0.21775274872620004\n",
            "Epoch: 491 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 492 -- Curr: 0.250201126307321, Min 0.21775274872620004\n",
            "Epoch: 493 -- Curr: 0.248994368463395, Min 0.21775274872620004\n",
            "Epoch: 494 -- Curr: 0.24818986323411102, Min 0.21775274872620004\n",
            "Epoch: 495 -- Curr: 0.24912845266827569, Min 0.21775274872620004\n",
            "Epoch: 496 -- Curr: 0.24912845266827569, Min 0.21775274872620004\n",
            "Epoch: 497 -- Curr: 0.249396621078037, Min 0.21775274872620004\n",
            "Epoch: 498 -- Curr: 0.24912845266827569, Min 0.21775274872620004\n",
            "Epoch: 499 -- Curr: 0.24845803164387234, Min 0.21775274872620004\n",
            "IDSGAN finish training\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqTm5m0GFVHA"
      },
      "source": [
        "# IDSGAN Result Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdG4r_u5sV3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6018dc2-56f2-41e9-b38c-b70435f89d74"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# from blackbox_wrapper import BlackBoxWrapper\n",
        "from model.model_class import Generator\n",
        "from idsgan_preprocessor import preprocess4\n",
        "import random\n",
        "\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(12345)\n",
        "random.seed(12345)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "test = pd.read_csv(\"dataset/generator_test_combined.csv\")\n",
        "\n",
        "functional_categories = ['intrinsic', 'time_based']\n",
        "# functional_categories = ['intrinsic', 'time_based', 'host_based']\n",
        "# functional_categories = ['intrinsic', 'content']\n",
        "\n",
        "\n",
        "test, raw_attack, _, _, modification_mask = preprocess4(test, functional_categories)\n",
        "raw_attack = np.array(raw_attack).astype(float)\n",
        "\n",
        "modification_mask = np.tile(modification_mask, [len(raw_attack), 1])\n",
        "modification_mask_t = torch.FloatTensor(modification_mask).to(device)\n",
        "\n",
        "NOISE_SIZE = 27\n",
        "generator_input_dim = test.shape[1] + NOISE_SIZE\n",
        "generator_output_dim = test.shape[1]\n",
        "discriminator_output_dim = 1\n",
        "\n",
        "random_g = Generator(generator_input_dim, generator_output_dim)\n",
        "learned_g = Generator(generator_input_dim, generator_output_dim).to(device)\n",
        "\n",
        "ids_model = BlackBoxWrapper(generator_output_dim, 1)\n",
        "\n",
        "x_t = torch.FloatTensor(raw_attack).to(device)\n",
        "\n",
        "out = ids_model(x_t)\n",
        "out = torch.reshape(out, [len(raw_attack)])\n",
        "out_np = out.cpu().detach().numpy()\n",
        "\n",
        "ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "corr = np.sum(ids_pred_label)\n",
        "\n",
        "original_det_rate = corr / len(raw_attack)\n",
        "\n",
        "print(\"original detection rate : {}\".format(original_det_rate))\n",
        "\n",
        "\n",
        "def test_GAN():\n",
        "    g_param = torch.load('save_model/generator.pth')\n",
        "    learned_g.load_state_dict(g_param)\n",
        "\n",
        "    noise = np.random.uniform(0., 1., (len(raw_attack), NOISE_SIZE))\n",
        "    gen_x = np.concatenate([raw_attack, noise], axis=1)\n",
        "\n",
        "    gen_x_t = torch.FloatTensor(gen_x).to(device)\n",
        "\n",
        "    adversarial_attack_modification_l = learned_g(gen_x_t, modification_mask_t)\n",
        "    adversarial_attack_l = torch.FloatTensor(raw_attack).to(device) + adversarial_attack_modification_l\n",
        "\n",
        "    # print(np.min(adversarial_attack_modification_l.cpu().detach().numpy(), axis=0))\n",
        "    # print(np.max(adversarial_attack_modification_l.cpu().detach().numpy(), axis=0))\n",
        "\n",
        "    out = ids_model(adversarial_attack_l)\n",
        "    out = torch.reshape(out, [len(adversarial_attack_l)])\n",
        "    out_np = out.cpu().detach().numpy()\n",
        "\n",
        "    ids_pred_label = np.array(out_np > 0.5).astype(int)\n",
        "    corr = np.sum(ids_pred_label)\n",
        "\n",
        "    adv_det_rate_learned = corr / len(raw_attack)\n",
        "\n",
        "    return adv_det_rate_learned\n",
        "\n",
        "\n",
        "print(\"adversarial detection rate: {}\".format(test_GAN()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original detection rate : 0.8211316706891928\n",
            "adversarial detection rate: 0.21735049611155804\n"
          ]
        }
      ]
    }
  ]
}